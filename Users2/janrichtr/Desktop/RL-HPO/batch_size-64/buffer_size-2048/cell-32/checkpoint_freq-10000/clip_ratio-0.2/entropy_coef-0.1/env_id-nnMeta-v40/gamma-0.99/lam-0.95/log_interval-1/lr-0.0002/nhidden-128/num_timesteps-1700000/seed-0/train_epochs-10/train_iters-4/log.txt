Logging to /Users/janrichtr/Desktop/RL-HPO/batch_size-64/buffer_size-2048/cell-32/checkpoint_freq-10000/clip_ratio-0.2/entropy_coef-0.1/env_id-nnMeta-v40/gamma-0.99/lam-0.95/log_interval-1/lr-0.0002/nhidden-128/num_timesteps-1700000/seed-0/train_epochs-10/train_iters-4/
--------------------------------------
| approx_kl               | 0        |
| entropy                 | 0        |
| episodes                | 100      |
| lives                   | 100      |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | -0.417   |
| most_used_dataset       | 0        |
| number_of_used          | 20       |
| policy_loss             | 0        |
| steps                   | 740      |
| value_loss              | 0        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.318   |
| entropy                 | 3.44     |
| episodes                | 200      |
| lives                   | 200      |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 0.775    |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0645   |
| steps                   | 1454     |
| value_loss              | 6.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.107   |
| entropy                 | 3.33     |
| episodes                | 300      |
| lives                   | 300      |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 2.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0468   |
| steps                   | 2108     |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0655  |
| entropy                 | 3.35     |
| episodes                | 400      |
| lives                   | 400      |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 2.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0253   |
| steps                   | 2812     |
| value_loss              | 3.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0603  |
| entropy                 | 3.29     |
| episodes                | 500      |
| lives                   | 500      |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 2.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0323   |
| steps                   | 3495     |
| value_loss              | 3.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.13    |
| entropy                 | 3.28     |
| episodes                | 600      |
| lives                   | 600      |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 3.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0672   |
| steps                   | 4192     |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.128   |
| entropy                 | 3.32     |
| episodes                | 700      |
| lives                   | 700      |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 3.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0438   |
| steps                   | 4919     |
| value_loss              | 4.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.139   |
| entropy                 | 3.35     |
| episodes                | 800      |
| lives                   | 800      |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 2.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0709   |
| steps                   | 5593     |
| value_loss              | 3.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 3.28     |
| episodes                | 900      |
| lives                   | 900      |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 3.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0573   |
| steps                   | 6236     |
| value_loss              | 4.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.115   |
| entropy                 | 3.26     |
| episodes                | 1000     |
| lives                   | 1000     |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 3.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0571   |
| steps                   | 6872     |
| value_loss              | 4.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.14    |
| entropy                 | 3.15     |
| episodes                | 1100     |
| lives                   | 1100     |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 2.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0657   |
| steps                   | 7464     |
| value_loss              | 4.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.122   |
| entropy                 | 3.16     |
| episodes                | 1200     |
| lives                   | 1200     |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 3.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.057    |
| steps                   | 8097     |
| value_loss              | 3.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 3.17     |
| episodes                | 1300     |
| lives                   | 1300     |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 3.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0524   |
| steps                   | 8716     |
| value_loss              | 4.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0963  |
| entropy                 | 3.14     |
| episodes                | 1400     |
| lives                   | 1400     |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0485   |
| steps                   | 9370     |
| value_loss              | 4.35     |
--------------------------------------
Saving model due to mean reward increase: None -> 3.7344
--------------------------------------
| approx_kl               | -0.159   |
| entropy                 | 3.14     |
| episodes                | 1500     |
| lives                   | 1500     |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 3.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0565   |
| steps                   | 10001    |
| value_loss              | 4.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.199   |
| entropy                 | 3.16     |
| episodes                | 1600     |
| lives                   | 1600     |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 3.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0855   |
| steps                   | 10654    |
| value_loss              | 4.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.213   |
| entropy                 | 3.13     |
| episodes                | 1700     |
| lives                   | 1700     |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 3.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0872   |
| steps                   | 11271    |
| value_loss              | 5.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.116   |
| entropy                 | 2.95     |
| episodes                | 1800     |
| lives                   | 1800     |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 3.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0516   |
| steps                   | 11840    |
| value_loss              | 4.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.73     |
| episodes                | 1900     |
| lives                   | 1900     |
| mean 100 episode length | 5.72     |
| mean 100 episode reward | 2.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0094   |
| steps                   | 12312    |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0575  |
| entropy                 | 2.69     |
| episodes                | 2000     |
| lives                   | 2000     |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 3.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0163   |
| steps                   | 12883    |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0924  |
| entropy                 | 2.53     |
| episodes                | 2100     |
| lives                   | 2100     |
| mean 100 episode length | 5.4      |
| mean 100 episode reward | 2.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 13323    |
| value_loss              | 6.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0839  |
| entropy                 | 2.72     |
| episodes                | 2200     |
| lives                   | 2200     |
| mean 100 episode length | 6.35     |
| mean 100 episode reward | 2.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 13858    |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0716  |
| entropy                 | 2.6      |
| episodes                | 2300     |
| lives                   | 2300     |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 3.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 14401    |
| value_loss              | 6.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0551  |
| entropy                 | 2.62     |
| episodes                | 2400     |
| lives                   | 2400     |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 3.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 14959    |
| value_loss              | 5.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0748  |
| entropy                 | 2.66     |
| episodes                | 2500     |
| lives                   | 2500     |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 3.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 15563    |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0886  |
| entropy                 | 2.68     |
| episodes                | 2600     |
| lives                   | 2600     |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 3.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 16128    |
| value_loss              | 6.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0606  |
| entropy                 | 2.67     |
| episodes                | 2700     |
| lives                   | 2700     |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 3.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 16711    |
| value_loss              | 6.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.108   |
| entropy                 | 2.57     |
| episodes                | 2800     |
| lives                   | 2800     |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 3.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0266  |
| steps                   | 17290    |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0766  |
| entropy                 | 2.57     |
| episodes                | 2900     |
| lives                   | 2900     |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0215  |
| steps                   | 17863    |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0584  |
| entropy                 | 2.67     |
| episodes                | 3000     |
| lives                   | 3000     |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 3.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 18450    |
| value_loss              | 6.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0899  |
| entropy                 | 2.57     |
| episodes                | 3100     |
| lives                   | 3100     |
| mean 100 episode length | 6.17     |
| mean 100 episode reward | 3.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0236  |
| steps                   | 18967    |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0897  |
| entropy                 | 2.49     |
| episodes                | 3200     |
| lives                   | 3200     |
| mean 100 episode length | 5.95     |
| mean 100 episode reward | 3.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0249  |
| steps                   | 19462    |
| value_loss              | 6.17     |
--------------------------------------
Saving model due to running mean reward increase: 3.1734 -> 3.6666
--------------------------------------
| approx_kl               | -0.0709  |
| entropy                 | 2.48     |
| episodes                | 3300     |
| lives                   | 3300     |
| mean 100 episode length | 6.45     |
| mean 100 episode reward | 3.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 20007    |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0482  |
| entropy                 | 2.62     |
| episodes                | 3400     |
| lives                   | 3400     |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 3.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 20596    |
| value_loss              | 7.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0588  |
| entropy                 | 2.61     |
| episodes                | 3500     |
| lives                   | 3500     |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 3.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0274  |
| steps                   | 21178    |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0763  |
| entropy                 | 2.59     |
| episodes                | 3600     |
| lives                   | 3600     |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 4.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 21783    |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0573  |
| entropy                 | 2.58     |
| episodes                | 3700     |
| lives                   | 3700     |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 22392    |
| value_loss              | 8.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0546  |
| entropy                 | 2.62     |
| episodes                | 3800     |
| lives                   | 3800     |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 23058    |
| value_loss              | 7.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0514  |
| entropy                 | 2.58     |
| episodes                | 3900     |
| lives                   | 3900     |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 23660    |
| value_loss              | 8.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.114   |
| entropy                 | 2.56     |
| episodes                | 4000     |
| lives                   | 4000     |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0187  |
| steps                   | 24187    |
| value_loss              | 8.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0522  |
| entropy                 | 2.62     |
| episodes                | 4100     |
| lives                   | 4100     |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 24797    |
| value_loss              | 7.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0705  |
| entropy                 | 2.63     |
| episodes                | 4200     |
| lives                   | 4200     |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0208  |
| steps                   | 25432    |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0451  |
| entropy                 | 2.64     |
| episodes                | 4300     |
| lives                   | 4300     |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 26081    |
| value_loss              | 7.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0403  |
| entropy                 | 2.72     |
| episodes                | 4400     |
| lives                   | 4400     |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 4.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 26736    |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0517  |
| entropy                 | 2.68     |
| episodes                | 4500     |
| lives                   | 4500     |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 4.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 27371    |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0792  |
| entropy                 | 2.6      |
| episodes                | 4600     |
| lives                   | 4600     |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 4.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0212  |
| steps                   | 27961    |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.065   |
| entropy                 | 2.52     |
| episodes                | 4700     |
| lives                   | 4700     |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 4.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0206  |
| steps                   | 28521    |
| value_loss              | 7.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0786  |
| entropy                 | 2.57     |
| episodes                | 4800     |
| lives                   | 4800     |
| mean 100 episode length | 6.45     |
| mean 100 episode reward | 4.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0212  |
| steps                   | 29066    |
| value_loss              | 8.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0599  |
| entropy                 | 2.6      |
| episodes                | 4900     |
| lives                   | 4900     |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 29650    |
| value_loss              | 8.35     |
--------------------------------------
Saving model due to mean reward increase: 3.7344 -> 4.8486
Saving model due to running mean reward increase: 4.6018 -> 4.8486
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.58     |
| episodes                | 5000     |
| lives                   | 5000     |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 4.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0209  |
| steps                   | 30263    |
| value_loss              | 8.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0881  |
| entropy                 | 2.39     |
| episodes                | 5100     |
| lives                   | 5100     |
| mean 100 episode length | 5.65     |
| mean 100 episode reward | 3.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0254  |
| steps                   | 30728    |
| value_loss              | 9.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0867  |
| entropy                 | 2.48     |
| episodes                | 5200     |
| lives                   | 5200     |
| mean 100 episode length | 5.91     |
| mean 100 episode reward | 3.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 31219    |
| value_loss              | 8.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0746  |
| entropy                 | 2.63     |
| episodes                | 5300     |
| lives                   | 5300     |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.02    |
| steps                   | 31827    |
| value_loss              | 8.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0796  |
| entropy                 | 2.63     |
| episodes                | 5400     |
| lives                   | 5400     |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 4.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 32437    |
| value_loss              | 8.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0533  |
| entropy                 | 2.72     |
| episodes                | 5500     |
| lives                   | 5500     |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0196  |
| steps                   | 33085    |
| value_loss              | 9.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0953  |
| entropy                 | 2.73     |
| episodes                | 5600     |
| lives                   | 5600     |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 33742    |
| value_loss              | 7.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.07    |
| entropy                 | 2.7      |
| episodes                | 5700     |
| lives                   | 5700     |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 34434    |
| value_loss              | 8.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0822  |
| entropy                 | 2.65     |
| episodes                | 5800     |
| lives                   | 5800     |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 3.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 35031    |
| value_loss              | 7.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0669  |
| entropy                 | 2.61     |
| episodes                | 5900     |
| lives                   | 5900     |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 35658    |
| value_loss              | 8.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0763  |
| entropy                 | 2.62     |
| episodes                | 6000     |
| lives                   | 6000     |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 4.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 36282    |
| value_loss              | 7.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0847  |
| entropy                 | 2.48     |
| episodes                | 6100     |
| lives                   | 6100     |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 36844    |
| value_loss              | 9.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.114   |
| entropy                 | 2.43     |
| episodes                | 6200     |
| lives                   | 6200     |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 4.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0233  |
| steps                   | 37378    |
| value_loss              | 8.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0753  |
| entropy                 | 2.6      |
| episodes                | 6300     |
| lives                   | 6300     |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0183  |
| steps                   | 37964    |
| value_loss              | 9.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.58     |
| episodes                | 6400     |
| lives                   | 6400     |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 4.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0204  |
| steps                   | 38539    |
| value_loss              | 9.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0503  |
| entropy                 | 2.74     |
| episodes                | 6500     |
| lives                   | 6500     |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 39217    |
| value_loss              | 8.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0741  |
| entropy                 | 2.63     |
| episodes                | 6600     |
| lives                   | 6600     |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0236  |
| steps                   | 39825    |
| value_loss              | 9.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.076   |
| entropy                 | 2.72     |
| episodes                | 6700     |
| lives                   | 6700     |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 40438    |
| value_loss              | 8.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.084   |
| entropy                 | 2.63     |
| episodes                | 6800     |
| lives                   | 6800     |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 3.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0257  |
| steps                   | 41002    |
| value_loss              | 8.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0729  |
| entropy                 | 2.64     |
| episodes                | 6900     |
| lives                   | 6900     |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 41580    |
| value_loss              | 9.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0663  |
| entropy                 | 2.54     |
| episodes                | 7000     |
| lives                   | 7000     |
| mean 100 episode length | 6.11     |
| mean 100 episode reward | 3.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0262  |
| steps                   | 42091    |
| value_loss              | 9.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0662  |
| entropy                 | 2.51     |
| episodes                | 7100     |
| lives                   | 7100     |
| mean 100 episode length | 5.95     |
| mean 100 episode reward | 3.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.027   |
| steps                   | 42586    |
| value_loss              | 9.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0719  |
| entropy                 | 2.57     |
| episodes                | 7200     |
| lives                   | 7200     |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 4.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0207  |
| steps                   | 43145    |
| value_loss              | 8.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0591  |
| entropy                 | 2.62     |
| episodes                | 7300     |
| lives                   | 7300     |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 43719    |
| value_loss              | 9.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0854  |
| entropy                 | 2.62     |
| episodes                | 7400     |
| lives                   | 7400     |
| mean 100 episode length | 6.52     |
| mean 100 episode reward | 3.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 44271    |
| value_loss              | 8.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0813  |
| entropy                 | 2.55     |
| episodes                | 7500     |
| lives                   | 7500     |
| mean 100 episode length | 6.11     |
| mean 100 episode reward | 3.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.022   |
| steps                   | 44782    |
| value_loss              | 8.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0421  |
| entropy                 | 2.69     |
| episodes                | 7600     |
| lives                   | 7600     |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 3.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 45392    |
| value_loss              | 8.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0563  |
| entropy                 | 2.59     |
| episodes                | 7700     |
| lives                   | 7700     |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 4.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 45946    |
| value_loss              | 9.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0604  |
| entropy                 | 2.52     |
| episodes                | 7800     |
| lives                   | 7800     |
| mean 100 episode length | 6.07     |
| mean 100 episode reward | 3.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0279  |
| steps                   | 46453    |
| value_loss              | 9.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0934  |
| entropy                 | 2.48     |
| episodes                | 7900     |
| lives                   | 7900     |
| mean 100 episode length | 5.93     |
| mean 100 episode reward | 3.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0264  |
| steps                   | 46946    |
| value_loss              | 8.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0732  |
| entropy                 | 2.58     |
| episodes                | 8000     |
| lives                   | 8000     |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 4.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 47522    |
| value_loss              | 9.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0894  |
| entropy                 | 2.51     |
| episodes                | 8100     |
| lives                   | 8100     |
| mean 100 episode length | 6.37     |
| mean 100 episode reward | 3.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0238  |
| steps                   | 48059    |
| value_loss              | 9.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0759  |
| entropy                 | 2.6      |
| episodes                | 8200     |
| lives                   | 8200     |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 3.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 48631    |
| value_loss              | 9.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0912  |
| entropy                 | 2.56     |
| episodes                | 8300     |
| lives                   | 8300     |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 3.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0225  |
| steps                   | 49170    |
| value_loss              | 9.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0867  |
| entropy                 | 2.59     |
| episodes                | 8400     |
| lives                   | 8400     |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 3.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 49743    |
| value_loss              | 9.8      |
--------------------------------------
Saving model due to running mean reward increase: 3.3877 -> 4.227
--------------------------------------
| approx_kl               | -0.073   |
| entropy                 | 2.62     |
| episodes                | 8500     |
| lives                   | 8500     |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 4.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 50310    |
| value_loss              | 9.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.065   |
| entropy                 | 2.62     |
| episodes                | 8600     |
| lives                   | 8600     |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0192  |
| steps                   | 50933    |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0922  |
| entropy                 | 2.55     |
| episodes                | 8700     |
| lives                   | 8700     |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 4.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0205  |
| steps                   | 51493    |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0606  |
| entropy                 | 2.53     |
| episodes                | 8800     |
| lives                   | 8800     |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 3.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0205  |
| steps                   | 52014    |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0816  |
| entropy                 | 2.53     |
| episodes                | 8900     |
| lives                   | 8900     |
| mean 100 episode length | 6.23     |
| mean 100 episode reward | 3.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0247  |
| steps                   | 52537    |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0928  |
| entropy                 | 2.59     |
| episodes                | 9000     |
| lives                   | 9000     |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 3.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 53114    |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0818  |
| entropy                 | 2.59     |
| episodes                | 9100     |
| lives                   | 9100     |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 3.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0215  |
| steps                   | 53691    |
| value_loss              | 9.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0761  |
| entropy                 | 2.4      |
| episodes                | 9200     |
| lives                   | 9200     |
| mean 100 episode length | 5.63     |
| mean 100 episode reward | 3.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0275  |
| steps                   | 54154    |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0738  |
| entropy                 | 2.55     |
| episodes                | 9300     |
| lives                   | 9300     |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 4.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 54726    |
| value_loss              | 9.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0743  |
| entropy                 | 2.55     |
| episodes                | 9400     |
| lives                   | 9400     |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 3.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0222  |
| steps                   | 55282    |
| value_loss              | 9.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0867  |
| entropy                 | 2.48     |
| episodes                | 9500     |
| lives                   | 9500     |
| mean 100 episode length | 6.09     |
| mean 100 episode reward | 3.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0257  |
| steps                   | 55791    |
| value_loss              | 9.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0858  |
| entropy                 | 2.56     |
| episodes                | 9600     |
| lives                   | 9600     |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 3.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 56395    |
| value_loss              | 8.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0766  |
| entropy                 | 2.55     |
| episodes                | 9700     |
| lives                   | 9700     |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 4.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 57004    |
| value_loss              | 9.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0942  |
| entropy                 | 2.33     |
| episodes                | 9800     |
| lives                   | 9800     |
| mean 100 episode length | 5.82     |
| mean 100 episode reward | 3.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0183  |
| steps                   | 57486    |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.077   |
| entropy                 | 2.39     |
| episodes                | 9900     |
| lives                   | 9900     |
| mean 100 episode length | 6.18     |
| mean 100 episode reward | 3.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 58004    |
| value_loss              | 11.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0619  |
| entropy                 | 2.39     |
| episodes                | 10000    |
| lives                   | 10000    |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 4.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 58566    |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0579  |
| entropy                 | 2.3      |
| episodes                | 10100    |
| lives                   | 10100    |
| mean 100 episode length | 6.18     |
| mean 100 episode reward | 3.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 59084    |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0774  |
| entropy                 | 2.43     |
| episodes                | 10200    |
| lives                   | 10200    |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 4.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 59637    |
| value_loss              | 10.5     |
--------------------------------------
Saving model due to running mean reward increase: 3.9864 -> 4.282
--------------------------------------
| approx_kl               | -0.0837  |
| entropy                 | 2.37     |
| episodes                | 10300    |
| lives                   | 10300    |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 4.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 60193    |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0804  |
| entropy                 | 2.32     |
| episodes                | 10400    |
| lives                   | 10400    |
| mean 100 episode length | 6.23     |
| mean 100 episode reward | 3.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 60716    |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.045   |
| entropy                 | 2.51     |
| episodes                | 10500    |
| lives                   | 10500    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 3.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 61294    |
| value_loss              | 9.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.52     |
| episodes                | 10600    |
| lives                   | 10600    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 61898    |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0388  |
| entropy                 | 2.54     |
| episodes                | 10700    |
| lives                   | 10700    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 4.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 62517    |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0727  |
| entropy                 | 2.43     |
| episodes                | 10800    |
| lives                   | 10800    |
| mean 100 episode length | 6.24     |
| mean 100 episode reward | 3.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 63041    |
| value_loss              | 11.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0474  |
| entropy                 | 2.51     |
| episodes                | 10900    |
| lives                   | 10900    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 3.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0189  |
| steps                   | 63640    |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0616  |
| entropy                 | 2.41     |
| episodes                | 11000    |
| lives                   | 11000    |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 64201    |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.064   |
| entropy                 | 2.45     |
| episodes                | 11100    |
| lives                   | 11100    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 64789    |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0668  |
| entropy                 | 2.49     |
| episodes                | 11200    |
| lives                   | 11200    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 4.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 65396    |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0646  |
| entropy                 | 2.5      |
| episodes                | 11300    |
| lives                   | 11300    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 3.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 65988    |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0811  |
| entropy                 | 2.56     |
| episodes                | 11400    |
| lives                   | 11400    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0238  |
| steps                   | 66566    |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0671  |
| entropy                 | 2.48     |
| episodes                | 11500    |
| lives                   | 11500    |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 3.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0192  |
| steps                   | 67100    |
| value_loss              | 9.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0876  |
| entropy                 | 2.61     |
| episodes                | 11600    |
| lives                   | 11600    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 4.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 67720    |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0432  |
| entropy                 | 2.52     |
| episodes                | 11700    |
| lives                   | 11700    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 4.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0214  |
| steps                   | 68307    |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0586  |
| entropy                 | 2.56     |
| episodes                | 11800    |
| lives                   | 11800    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 4.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0185  |
| steps                   | 68930    |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0354  |
| entropy                 | 2.64     |
| episodes                | 11900    |
| lives                   | 11900    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 4.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0186  |
| steps                   | 69598    |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0678  |
| entropy                 | 2.59     |
| episodes                | 12000    |
| lives                   | 12000    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 3.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0187  |
| steps                   | 70190    |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0776  |
| entropy                 | 2.47     |
| episodes                | 12100    |
| lives                   | 12100    |
| mean 100 episode length | 6.36     |
| mean 100 episode reward | 3.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0233  |
| steps                   | 70726    |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.115   |
| entropy                 | 2.37     |
| episodes                | 12200    |
| lives                   | 12200    |
| mean 100 episode length | 5.89     |
| mean 100 episode reward | 3.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 71215    |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0713  |
| entropy                 | 2.53     |
| episodes                | 12300    |
| lives                   | 12300    |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 3.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 71776    |
| value_loss              | 9.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0968  |
| entropy                 | 2.47     |
| episodes                | 12400    |
| lives                   | 12400    |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 4.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.021   |
| steps                   | 72330    |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.44     |
| episodes                | 12500    |
| lives                   | 12500    |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 4.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 72891    |
| value_loss              | 11.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0981  |
| entropy                 | 2.42     |
| episodes                | 12600    |
| lives                   | 12600    |
| mean 100 episode length | 6.29     |
| mean 100 episode reward | 3.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 73420    |
| value_loss              | 12.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.45     |
| episodes                | 12700    |
| lives                   | 12700    |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 3.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 73992    |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0761  |
| entropy                 | 2.57     |
| episodes                | 12800    |
| lives                   | 12800    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 74661    |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0724  |
| entropy                 | 2.5      |
| episodes                | 12900    |
| lives                   | 12900    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 4.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 75237    |
| value_loss              | 11.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0691  |
| entropy                 | 2.52     |
| episodes                | 13000    |
| lives                   | 13000    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 4.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.025   |
| steps                   | 75858    |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0628  |
| entropy                 | 2.4      |
| episodes                | 13100    |
| lives                   | 13100    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 76466    |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.39     |
| episodes                | 13200    |
| lives                   | 13200    |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 4.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 77024    |
| value_loss              | 11.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0616  |
| entropy                 | 2.35     |
| episodes                | 13300    |
| lives                   | 13300    |
| mean 100 episode length | 6.09     |
| mean 100 episode reward | 3.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 77533    |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0642  |
| entropy                 | 2.53     |
| episodes                | 13400    |
| lives                   | 13400    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 78123    |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0586  |
| entropy                 | 2.58     |
| episodes                | 13500    |
| lives                   | 13500    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 78795    |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0597  |
| entropy                 | 2.6      |
| episodes                | 13600    |
| lives                   | 13600    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 79453    |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0603  |
| entropy                 | 2.52     |
| episodes                | 13700    |
| lives                   | 13700    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 4.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 80055    |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.069   |
| entropy                 | 2.53     |
| episodes                | 13800    |
| lives                   | 13800    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 80687    |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.39     |
| episodes                | 13900    |
| lives                   | 13900    |
| mean 100 episode length | 6.41     |
| mean 100 episode reward | 4.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 81228    |
| value_loss              | 12.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0708  |
| entropy                 | 2.5      |
| episodes                | 14000    |
| lives                   | 14000    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 81856    |
| value_loss              | 12       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.107   |
| entropy                 | 2.52     |
| episodes                | 14100    |
| lives                   | 14100    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 4.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 82483    |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0829  |
| entropy                 | 2.58     |
| episodes                | 14200    |
| lives                   | 14200    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 83126    |
| value_loss              | 11.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0918  |
| entropy                 | 2.67     |
| episodes                | 14300    |
| lives                   | 14300    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 4.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 83799    |
| value_loss              | 9.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0636  |
| entropy                 | 2.57     |
| episodes                | 14400    |
| lives                   | 14400    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0186  |
| steps                   | 84397    |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0848  |
| entropy                 | 2.56     |
| episodes                | 14500    |
| lives                   | 14500    |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 3.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.03    |
| steps                   | 84957    |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0982  |
| entropy                 | 2.61     |
| episodes                | 14600    |
| lives                   | 14600    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 4.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 85627    |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0639  |
| entropy                 | 2.52     |
| episodes                | 14700    |
| lives                   | 14700    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 86244    |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0659  |
| entropy                 | 2.48     |
| episodes                | 14800    |
| lives                   | 14800    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 4.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 86841    |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0538  |
| entropy                 | 2.42     |
| episodes                | 14900    |
| lives                   | 14900    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 4.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 87420    |
| value_loss              | 11.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0784  |
| entropy                 | 2.39     |
| episodes                | 15000    |
| lives                   | 15000    |
| mean 100 episode length | 6.08     |
| mean 100 episode reward | 3.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 87928    |
| value_loss              | 12.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.48     |
| episodes                | 15100    |
| lives                   | 15100    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 4.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 88505    |
| value_loss              | 12.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.5      |
| episodes                | 15200    |
| lives                   | 15200    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 89096    |
| value_loss              | 11.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0304  |
| entropy                 | 2.46     |
| episodes                | 15300    |
| lives                   | 15300    |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 3.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 89630    |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0607  |
| entropy                 | 2.52     |
| episodes                | 15400    |
| lives                   | 15400    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 4.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 90236    |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0657  |
| entropy                 | 2.62     |
| episodes                | 15500    |
| lives                   | 15500    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 4.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 90877    |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0611  |
| entropy                 | 2.69     |
| episodes                | 15600    |
| lives                   | 15600    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 91531    |
| value_loss              | 9.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0514  |
| entropy                 | 2.65     |
| episodes                | 15700    |
| lives                   | 15700    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 92203    |
| value_loss              | 8.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0659  |
| entropy                 | 2.56     |
| episodes                | 15800    |
| lives                   | 15800    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 92799    |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0551  |
| entropy                 | 2.65     |
| episodes                | 15900    |
| lives                   | 15900    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 93477    |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0579  |
| entropy                 | 2.66     |
| episodes                | 16000    |
| lives                   | 16000    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0181  |
| steps                   | 94113    |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.64     |
| episodes                | 16100    |
| lives                   | 16100    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 4.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 94734    |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0571  |
| entropy                 | 2.66     |
| episodes                | 16200    |
| lives                   | 16200    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 95409    |
| value_loss              | 9.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0669  |
| entropy                 | 2.67     |
| episodes                | 16300    |
| lives                   | 16300    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 4.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 96059    |
| value_loss              | 9.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0738  |
| entropy                 | 2.51     |
| episodes                | 16400    |
| lives                   | 16400    |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 3.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0234  |
| steps                   | 96631    |
| value_loss              | 9.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0789  |
| entropy                 | 2.56     |
| episodes                | 16500    |
| lives                   | 16500    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 4.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 97207    |
| value_loss              | 9.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0636  |
| entropy                 | 2.54     |
| episodes                | 16600    |
| lives                   | 16600    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 4.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 97806    |
| value_loss              | 9.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0575  |
| entropy                 | 2.61     |
| episodes                | 16700    |
| lives                   | 16700    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 4.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 98418    |
| value_loss              | 9.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0641  |
| entropy                 | 2.47     |
| episodes                | 16800    |
| lives                   | 16800    |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 3.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 98957    |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0631  |
| entropy                 | 2.58     |
| episodes                | 16900    |
| lives                   | 16900    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 4.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 99596    |
| value_loss              | 10.2     |
--------------------------------------
Saving model due to mean reward increase: 4.8486 -> 5.1874
Saving model due to running mean reward increase: 4.4379 -> 5.1874
--------------------------------------
| approx_kl               | -0.0593  |
| entropy                 | 2.56     |
| episodes                | 17000    |
| lives                   | 17000    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 100262   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0657  |
| entropy                 | 2.6      |
| episodes                | 17100    |
| lives                   | 17100    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 100907   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0547  |
| entropy                 | 2.6      |
| episodes                | 17200    |
| lives                   | 17200    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 4.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 101513   |
| value_loss              | 11.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0713  |
| entropy                 | 2.65     |
| episodes                | 17300    |
| lives                   | 17300    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 3.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 102143   |
| value_loss              | 9.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0724  |
| entropy                 | 2.55     |
| episodes                | 17400    |
| lives                   | 17400    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0216  |
| steps                   | 102728   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0777  |
| entropy                 | 2.52     |
| episodes                | 17500    |
| lives                   | 17500    |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 103296   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0665  |
| entropy                 | 2.59     |
| episodes                | 17600    |
| lives                   | 17600    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 103881   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0448  |
| entropy                 | 2.66     |
| episodes                | 17700    |
| lives                   | 17700    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 4.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 104507   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0625  |
| entropy                 | 2.66     |
| episodes                | 17800    |
| lives                   | 17800    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 105144   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0386  |
| entropy                 | 2.73     |
| episodes                | 17900    |
| lives                   | 17900    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 4.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 105833   |
| value_loss              | 9        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0562  |
| entropy                 | 2.65     |
| episodes                | 18000    |
| lives                   | 18000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 4.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0183  |
| steps                   | 106472   |
| value_loss              | 9.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0628  |
| entropy                 | 2.61     |
| episodes                | 18100    |
| lives                   | 18100    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 4.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 107139   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0706  |
| entropy                 | 2.52     |
| episodes                | 18200    |
| lives                   | 18200    |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 107706   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0743  |
| entropy                 | 2.63     |
| episodes                | 18300    |
| lives                   | 18300    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 4.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 108332   |
| value_loss              | 9.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.61     |
| episodes                | 18400    |
| lives                   | 18400    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 4.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0172  |
| steps                   | 108937   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0387  |
| entropy                 | 2.63     |
| episodes                | 18500    |
| lives                   | 18500    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0208  |
| steps                   | 109559   |
| value_loss              | 11.1     |
--------------------------------------
Saving model due to running mean reward increase: 4.2887 -> 4.39
--------------------------------------
| approx_kl               | -0.0499  |
| entropy                 | 2.63     |
| episodes                | 18600    |
| lives                   | 18600    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0204  |
| steps                   | 110176   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0584  |
| entropy                 | 2.53     |
| episodes                | 18700    |
| lives                   | 18700    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 110784   |
| value_loss              | 11.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.065   |
| entropy                 | 2.54     |
| episodes                | 18800    |
| lives                   | 18800    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 4.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 111374   |
| value_loss              | 11.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0615  |
| entropy                 | 2.65     |
| episodes                | 18900    |
| lives                   | 18900    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 112049   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0642  |
| entropy                 | 2.67     |
| episodes                | 19000    |
| lives                   | 19000    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 4.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 112713   |
| value_loss              | 9.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0711  |
| entropy                 | 2.62     |
| episodes                | 19100    |
| lives                   | 19100    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 4.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 113357   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.078   |
| entropy                 | 2.6      |
| episodes                | 19200    |
| lives                   | 19200    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 114000   |
| value_loss              | 9.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.114   |
| entropy                 | 2.54     |
| episodes                | 19300    |
| lives                   | 19300    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 114591   |
| value_loss              | 11.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0676  |
| entropy                 | 2.64     |
| episodes                | 19400    |
| lives                   | 19400    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 4.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.024   |
| steps                   | 115222   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.064   |
| entropy                 | 2.65     |
| episodes                | 19500    |
| lives                   | 19500    |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 3.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 115773   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0506  |
| entropy                 | 2.7      |
| episodes                | 19600    |
| lives                   | 19600    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 116444   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0533  |
| entropy                 | 2.69     |
| episodes                | 19700    |
| lives                   | 19700    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0166  |
| steps                   | 117101   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0365  |
| entropy                 | 2.76     |
| episodes                | 19800    |
| lives                   | 19800    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 4.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 117797   |
| value_loss              | 9.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0726  |
| entropy                 | 2.69     |
| episodes                | 19900    |
| lives                   | 19900    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 118492   |
| value_loss              | 9.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.069   |
| entropy                 | 2.7      |
| episodes                | 20000    |
| lives                   | 20000    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 119184   |
| value_loss              | 9.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.089   |
| entropy                 | 2.76     |
| episodes                | 20100    |
| lives                   | 20100    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 4.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 119875   |
| value_loss              | 9.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0589  |
| entropy                 | 2.7      |
| episodes                | 20200    |
| lives                   | 20200    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 4.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 120534   |
| value_loss              | 9.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0369  |
| entropy                 | 2.76     |
| episodes                | 20300    |
| lives                   | 20300    |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 121310   |
| value_loss              | 9.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.63     |
| episodes                | 20400    |
| lives                   | 20400    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 121983   |
| value_loss              | 9.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0269  |
| entropy                 | 2.64     |
| episodes                | 20500    |
| lives                   | 20500    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 122630   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0671  |
| entropy                 | 2.61     |
| episodes                | 20600    |
| lives                   | 20600    |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 3.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 123213   |
| value_loss              | 9.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0452  |
| entropy                 | 2.65     |
| episodes                | 20700    |
| lives                   | 20700    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 123857   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0561  |
| entropy                 | 2.71     |
| episodes                | 20800    |
| lives                   | 20800    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 124526   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0673  |
| entropy                 | 2.65     |
| episodes                | 20900    |
| lives                   | 20900    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0185  |
| steps                   | 125188   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0553  |
| entropy                 | 2.66     |
| episodes                | 21000    |
| lives                   | 21000    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 125895   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0506  |
| entropy                 | 2.64     |
| episodes                | 21100    |
| lives                   | 21100    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 126540   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.077   |
| entropy                 | 2.57     |
| episodes                | 21200    |
| lives                   | 21200    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 3.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0233  |
| steps                   | 127106   |
| value_loss              | 9.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.109   |
| entropy                 | 2.34     |
| episodes                | 21300    |
| lives                   | 21300    |
| mean 100 episode length | 5.73     |
| mean 100 episode reward | 3.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0235  |
| steps                   | 127579   |
| value_loss              | 11.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.49     |
| episodes                | 21400    |
| lives                   | 21400    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 4.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0239  |
| steps                   | 128158   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0578  |
| entropy                 | 2.63     |
| episodes                | 21500    |
| lives                   | 21500    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 4.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 128781   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0742  |
| entropy                 | 2.59     |
| episodes                | 21600    |
| lives                   | 21600    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 4.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 129395   |
| value_loss              | 9.59     |
--------------------------------------
Saving model due to running mean reward increase: 4.2358 -> 4.3116
--------------------------------------
| approx_kl               | -0.0752  |
| entropy                 | 2.61     |
| episodes                | 21700    |
| lives                   | 21700    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0187  |
| steps                   | 130003   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0626  |
| entropy                 | 2.49     |
| episodes                | 21800    |
| lives                   | 21800    |
| mean 100 episode length | 6.49     |
| mean 100 episode reward | 3.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0259  |
| steps                   | 130552   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0602  |
| entropy                 | 2.46     |
| episodes                | 21900    |
| lives                   | 21900    |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 4.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0212  |
| steps                   | 131117   |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0968  |
| entropy                 | 2.56     |
| episodes                | 22000    |
| lives                   | 22000    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 4.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0207  |
| steps                   | 131723   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0668  |
| entropy                 | 2.57     |
| episodes                | 22100    |
| lives                   | 22100    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 4.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 132337   |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.078   |
| entropy                 | 2.56     |
| episodes                | 22200    |
| lives                   | 22200    |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 3.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 132917   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0624  |
| entropy                 | 2.53     |
| episodes                | 22300    |
| lives                   | 22300    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.02    |
| steps                   | 133490   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0636  |
| entropy                 | 2.51     |
| episodes                | 22400    |
| lives                   | 22400    |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 3.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 134033   |
| value_loss              | 11.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0936  |
| entropy                 | 2.51     |
| episodes                | 22500    |
| lives                   | 22500    |
| mean 100 episode length | 6.35     |
| mean 100 episode reward | 3.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.024   |
| steps                   | 134568   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.053   |
| entropy                 | 2.6      |
| episodes                | 22600    |
| lives                   | 22600    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 4.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 135202   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0882  |
| entropy                 | 2.57     |
| episodes                | 22700    |
| lives                   | 22700    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 4.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 135838   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0736  |
| entropy                 | 2.61     |
| episodes                | 22800    |
| lives                   | 22800    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 136499   |
| value_loss              | 9.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0583  |
| entropy                 | 2.54     |
| episodes                | 22900    |
| lives                   | 22900    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 4.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0213  |
| steps                   | 137127   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.116   |
| entropy                 | 2.58     |
| episodes                | 23000    |
| lives                   | 23000    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 4.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0094   |
| steps                   | 137738   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0558  |
| entropy                 | 2.62     |
| episodes                | 23100    |
| lives                   | 23100    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 138324   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0673  |
| entropy                 | 2.5      |
| episodes                | 23200    |
| lives                   | 23200    |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 4.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 138899   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0435  |
| entropy                 | 2.64     |
| episodes                | 23300    |
| lives                   | 23300    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 4.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 139539   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0581  |
| entropy                 | 2.69     |
| episodes                | 23400    |
| lives                   | 23400    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 3.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0212  |
| steps                   | 140160   |
| value_loss              | 9.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0629  |
| entropy                 | 2.63     |
| episodes                | 23500    |
| lives                   | 23500    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 3.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 140765   |
| value_loss              | 8.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0776  |
| entropy                 | 2.6      |
| episodes                | 23600    |
| lives                   | 23600    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 4.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0216  |
| steps                   | 141391   |
| value_loss              | 9.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0358  |
| entropy                 | 2.66     |
| episodes                | 23700    |
| lives                   | 23700    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 4.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 142080   |
| value_loss              | 9.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.055   |
| entropy                 | 2.56     |
| episodes                | 23800    |
| lives                   | 23800    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 4.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 142673   |
| value_loss              | 9.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0886  |
| entropy                 | 2.51     |
| episodes                | 23900    |
| lives                   | 23900    |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 3.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0248  |
| steps                   | 143244   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0495  |
| entropy                 | 2.55     |
| episodes                | 24000    |
| lives                   | 24000    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 3.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 143855   |
| value_loss              | 9.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0947  |
| entropy                 | 2.44     |
| episodes                | 24100    |
| lives                   | 24100    |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 3.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 144406   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0477  |
| entropy                 | 2.54     |
| episodes                | 24200    |
| lives                   | 24200    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0195  |
| steps                   | 145012   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0748  |
| entropy                 | 2.51     |
| episodes                | 24300    |
| lives                   | 24300    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 145608   |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0635  |
| entropy                 | 2.45     |
| episodes                | 24400    |
| lives                   | 24400    |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 3.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0197  |
| steps                   | 146161   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0633  |
| entropy                 | 2.56     |
| episodes                | 24500    |
| lives                   | 24500    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 4.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0234  |
| steps                   | 146775   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0293  |
| entropy                 | 2.6      |
| episodes                | 24600    |
| lives                   | 24600    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 4.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0217  |
| steps                   | 147421   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0902  |
| entropy                 | 2.54     |
| episodes                | 24700    |
| lives                   | 24700    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 4.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 148018   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0901  |
| entropy                 | 2.49     |
| episodes                | 24800    |
| lives                   | 24800    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0163  |
| steps                   | 148591   |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.136   |
| entropy                 | 2.56     |
| episodes                | 24900    |
| lives                   | 24900    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0031   |
| steps                   | 149187   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.052   |
| entropy                 | 2.51     |
| episodes                | 25000    |
| lives                   | 25000    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 149768   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.12    |
| entropy                 | 2.54     |
| episodes                | 25100    |
| lives                   | 25100    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 150372   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.169   |
| entropy                 | 2.56     |
| episodes                | 25200    |
| lives                   | 25200    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 4.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 150981   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.16    |
| entropy                 | 2.58     |
| episodes                | 25300    |
| lives                   | 25300    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0037   |
| steps                   | 151619   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0765  |
| entropy                 | 2.66     |
| episodes                | 25400    |
| lives                   | 25400    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 4.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0213  |
| steps                   | 152282   |
| value_loss              | 9.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0742  |
| entropy                 | 2.66     |
| episodes                | 25500    |
| lives                   | 25500    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 4.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 152979   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.088   |
| entropy                 | 2.65     |
| episodes                | 25600    |
| lives                   | 25600    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 153652   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0876  |
| entropy                 | 2.69     |
| episodes                | 25700    |
| lives                   | 25700    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 4.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.002    |
| steps                   | 154303   |
| value_loss              | 9.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0946  |
| entropy                 | 2.68     |
| episodes                | 25800    |
| lives                   | 25800    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 4.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 154972   |
| value_loss              | 9.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0888  |
| entropy                 | 2.71     |
| episodes                | 25900    |
| lives                   | 25900    |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0059   |
| steps                   | 155721   |
| value_loss              | 9.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.068   |
| entropy                 | 2.65     |
| episodes                | 26000    |
| lives                   | 26000    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 4.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 156372   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0968  |
| entropy                 | 2.69     |
| episodes                | 26100    |
| lives                   | 26100    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 3.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 157035   |
| value_loss              | 9.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.107   |
| entropy                 | 2.85     |
| episodes                | 26200    |
| lives                   | 26200    |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0451   |
| steps                   | 157782   |
| value_loss              | 9.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0771  |
| entropy                 | 3        |
| episodes                | 26300    |
| lives                   | 26300    |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 4.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0511   |
| steps                   | 158509   |
| value_loss              | 9.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0779  |
| entropy                 | 3.02     |
| episodes                | 26400    |
| lives                   | 26400    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 4.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0655   |
| steps                   | 159158   |
| value_loss              | 9.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0389  |
| entropy                 | 3.03     |
| episodes                | 26500    |
| lives                   | 26500    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0295   |
| steps                   | 159776   |
| value_loss              | 10.5     |
--------------------------------------
Saving model due to running mean reward increase: 4.8949 -> 5.0105
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.86     |
| episodes                | 26600    |
| lives                   | 26600    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0088   |
| steps                   | 160380   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.84     |
| episodes                | 26700    |
| lives                   | 26700    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0185   |
| steps                   | 161020   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0522  |
| entropy                 | 2.84     |
| episodes                | 26800    |
| lives                   | 26800    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0231   |
| steps                   | 161643   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0469  |
| entropy                 | 2.73     |
| episodes                | 26900    |
| lives                   | 26900    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 162209   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.8      |
| episodes                | 27000    |
| lives                   | 27000    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 162865   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0364  |
| entropy                 | 2.71     |
| episodes                | 27100    |
| lives                   | 27100    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 163533   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0384  |
| entropy                 | 2.61     |
| episodes                | 27200    |
| lives                   | 27200    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0197  |
| steps                   | 164123   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0478  |
| entropy                 | 2.62     |
| episodes                | 27300    |
| lives                   | 27300    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 164785   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0306  |
| entropy                 | 2.69     |
| episodes                | 27400    |
| lives                   | 27400    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 165436   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.042   |
| entropy                 | 2.76     |
| episodes                | 27500    |
| lives                   | 27500    |
| mean 100 episode length | 8        |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0001   |
| steps                   | 166136   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0396  |
| entropy                 | 2.73     |
| episodes                | 27600    |
| lives                   | 27600    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0045   |
| steps                   | 166786   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0275  |
| entropy                 | 2.73     |
| episodes                | 27700    |
| lives                   | 27700    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 5.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0069   |
| steps                   | 167468   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.74     |
| episodes                | 27800    |
| lives                   | 27800    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 168126   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0357  |
| entropy                 | 2.72     |
| episodes                | 27900    |
| lives                   | 27900    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 168820   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0219  |
| entropy                 | 2.7      |
| episodes                | 28000    |
| lives                   | 28000    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 169481   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0421  |
| entropy                 | 2.64     |
| episodes                | 28100    |
| lives                   | 28100    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 170122   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0449  |
| entropy                 | 2.63     |
| episodes                | 28200    |
| lives                   | 28200    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 170733   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0325  |
| entropy                 | 2.62     |
| episodes                | 28300    |
| lives                   | 28300    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 171350   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0355  |
| entropy                 | 2.65     |
| episodes                | 28400    |
| lives                   | 28400    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 171987   |
| value_loss              | 9.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.037   |
| entropy                 | 2.7      |
| episodes                | 28500    |
| lives                   | 28500    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0183  |
| steps                   | 172682   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.047   |
| entropy                 | 2.58     |
| episodes                | 28600    |
| lives                   | 28600    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 173335   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0305  |
| entropy                 | 2.62     |
| episodes                | 28700    |
| lives                   | 28700    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 174004   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.64     |
| episodes                | 28800    |
| lives                   | 28800    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 174711   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0415  |
| entropy                 | 2.64     |
| episodes                | 28900    |
| lives                   | 28900    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0166  |
| steps                   | 175396   |
| value_loss              | 11.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.032   |
| entropy                 | 2.62     |
| episodes                | 29000    |
| lives                   | 29000    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 176027   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0535  |
| entropy                 | 2.58     |
| episodes                | 29100    |
| lives                   | 29100    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 176665   |
| value_loss              | 11.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0382  |
| entropy                 | 2.6      |
| episodes                | 29200    |
| lives                   | 29200    |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 177399   |
| value_loss              | 11.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0501  |
| entropy                 | 2.53     |
| episodes                | 29300    |
| lives                   | 29300    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 178033   |
| value_loss              | 11.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0435  |
| entropy                 | 2.5      |
| episodes                | 29400    |
| lives                   | 29400    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 178667   |
| value_loss              | 11.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0397  |
| entropy                 | 2.5      |
| episodes                | 29500    |
| lives                   | 29500    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 179278   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0315  |
| entropy                 | 2.5      |
| episodes                | 29600    |
| lives                   | 29600    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 179927   |
| value_loss              | 11.8     |
--------------------------------------
Saving model due to mean reward increase: 5.1874 -> 6.3302
Saving model due to running mean reward increase: 5.4988 -> 6.3302
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.52     |
| episodes                | 29700    |
| lives                   | 29700    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 180585   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0316  |
| entropy                 | 2.61     |
| episodes                | 29800    |
| lives                   | 29800    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 181294   |
| value_loss              | 11.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0358  |
| entropy                 | 2.62     |
| episodes                | 29900    |
| lives                   | 29900    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 181964   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0197  |
| entropy                 | 2.63     |
| episodes                | 30000    |
| lives                   | 30000    |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 182694   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0436  |
| entropy                 | 2.56     |
| episodes                | 30100    |
| lives                   | 30100    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 183358   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.58     |
| episodes                | 30200    |
| lives                   | 30200    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 184008   |
| value_loss              | 11.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0243  |
| entropy                 | 2.61     |
| episodes                | 30300    |
| lives                   | 30300    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 184692   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0391  |
| entropy                 | 2.6      |
| episodes                | 30400    |
| lives                   | 30400    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 185357   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0443  |
| entropy                 | 2.59     |
| episodes                | 30500    |
| lives                   | 30500    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 186002   |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0448  |
| entropy                 | 2.75     |
| episodes                | 30600    |
| lives                   | 30600    |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 186758   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0584  |
| entropy                 | 2.66     |
| episodes                | 30700    |
| lives                   | 30700    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 187425   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.69     |
| episodes                | 30800    |
| lives                   | 30800    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 188107   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0373  |
| entropy                 | 2.72     |
| episodes                | 30900    |
| lives                   | 30900    |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 188848   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0472  |
| entropy                 | 2.69     |
| episodes                | 31000    |
| lives                   | 31000    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 189555   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0443  |
| entropy                 | 2.64     |
| episodes                | 31100    |
| lives                   | 31100    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 190235   |
| value_loss              | 9.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0365  |
| entropy                 | 2.62     |
| episodes                | 31200    |
| lives                   | 31200    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 190918   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.037   |
| entropy                 | 2.68     |
| episodes                | 31300    |
| lives                   | 31300    |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 191623   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0638  |
| entropy                 | 2.69     |
| episodes                | 31400    |
| lives                   | 31400    |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 5.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 192335   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0385  |
| entropy                 | 2.76     |
| episodes                | 31500    |
| lives                   | 31500    |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0038   |
| steps                   | 193076   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0992  |
| entropy                 | 2.75     |
| episodes                | 31600    |
| lives                   | 31600    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.034    |
| steps                   | 193765   |
| value_loss              | 9.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0427  |
| entropy                 | 2.74     |
| episodes                | 31700    |
| lives                   | 31700    |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0035   |
| steps                   | 194486   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0206  |
| entropy                 | 2.64     |
| episodes                | 31800    |
| lives                   | 31800    |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 195202   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0375  |
| entropy                 | 2.53     |
| episodes                | 31900    |
| lives                   | 31900    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 195872   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0496  |
| entropy                 | 2.52     |
| episodes                | 32000    |
| lives                   | 32000    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 196536   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0368  |
| entropy                 | 2.51     |
| episodes                | 32100    |
| lives                   | 32100    |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 197256   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0562  |
| entropy                 | 2.53     |
| episodes                | 32200    |
| lives                   | 32200    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0019   |
| steps                   | 197883   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.62     |
| episodes                | 32300    |
| lives                   | 32300    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 198571   |
| value_loss              | 9.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0652  |
| entropy                 | 2.64     |
| episodes                | 32400    |
| lives                   | 32400    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0106   |
| steps                   | 199230   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0502  |
| entropy                 | 2.69     |
| episodes                | 32500    |
| lives                   | 32500    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0243   |
| steps                   | 199885   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.58     |
| episodes                | 32600    |
| lives                   | 32600    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 200524   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0463  |
| entropy                 | 2.48     |
| episodes                | 32700    |
| lives                   | 32700    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 201119   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0377  |
| entropy                 | 2.64     |
| episodes                | 32800    |
| lives                   | 32800    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 201820   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0411  |
| entropy                 | 2.61     |
| episodes                | 32900    |
| lives                   | 32900    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0055   |
| steps                   | 202497   |
| value_loss              | 9.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0199  |
| entropy                 | 2.5      |
| episodes                | 33000    |
| lives                   | 33000    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 203137   |
| value_loss              | 9.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0149  |
| entropy                 | 2.47     |
| episodes                | 33100    |
| lives                   | 33100    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 203822   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0273  |
| entropy                 | 2.42     |
| episodes                | 33200    |
| lives                   | 33200    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 204466   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0266  |
| entropy                 | 2.48     |
| episodes                | 33300    |
| lives                   | 33300    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 205154   |
| value_loss              | 9.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0144  |
| entropy                 | 2.41     |
| episodes                | 33400    |
| lives                   | 33400    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 205833   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.4      |
| episodes                | 33500    |
| lives                   | 33500    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 206502   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0204  |
| entropy                 | 2.46     |
| episodes                | 33600    |
| lives                   | 33600    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 207211   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0296  |
| entropy                 | 2.48     |
| episodes                | 33700    |
| lives                   | 33700    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 207902   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0282  |
| entropy                 | 2.53     |
| episodes                | 33800    |
| lives                   | 33800    |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 208610   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0274  |
| entropy                 | 2.57     |
| episodes                | 33900    |
| lives                   | 33900    |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 209331   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.55     |
| episodes                | 34000    |
| lives                   | 34000    |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 210030   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0463  |
| entropy                 | 2.54     |
| episodes                | 34100    |
| lives                   | 34100    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 210713   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0161  |
| entropy                 | 2.62     |
| episodes                | 34200    |
| lives                   | 34200    |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 5.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0183  |
| steps                   | 211434   |
| value_loss              | 9.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0277  |
| entropy                 | 2.53     |
| episodes                | 34300    |
| lives                   | 34300    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 212034   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.6      |
| episodes                | 34400    |
| lives                   | 34400    |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 212720   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.049   |
| entropy                 | 2.64     |
| episodes                | 34500    |
| lives                   | 34500    |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 213453   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0544  |
| entropy                 | 2.62     |
| episodes                | 34600    |
| lives                   | 34600    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0013   |
| steps                   | 214155   |
| value_loss              | 9.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.012   |
| entropy                 | 2.51     |
| episodes                | 34700    |
| lives                   | 34700    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0225  |
| steps                   | 214806   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0364  |
| entropy                 | 2.56     |
| episodes                | 34800    |
| lives                   | 34800    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 215490   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0567  |
| entropy                 | 2.53     |
| episodes                | 34900    |
| lives                   | 34900    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0094   |
| steps                   | 216129   |
| value_loss              | 9.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0209  |
| entropy                 | 2.48     |
| episodes                | 35000    |
| lives                   | 35000    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 216722   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0287  |
| entropy                 | 2.49     |
| episodes                | 35100    |
| lives                   | 35100    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 217332   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0468  |
| entropy                 | 2.43     |
| episodes                | 35200    |
| lives                   | 35200    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 217918   |
| value_loss              | 9.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.039   |
| entropy                 | 2.45     |
| episodes                | 35300    |
| lives                   | 35300    |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 4.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 218483   |
| value_loss              | 9.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0372  |
| entropy                 | 2.57     |
| episodes                | 35400    |
| lives                   | 35400    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 219133   |
| value_loss              | 9.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.62     |
| episodes                | 35500    |
| lives                   | 35500    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 219804   |
| value_loss              | 10       |
--------------------------------------
Saving model due to running mean reward increase: 5.0983 -> 5.1969
--------------------------------------
| approx_kl               | -0.0475  |
| entropy                 | 2.69     |
| episodes                | 35600    |
| lives                   | 35600    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 220469   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0194  |
| entropy                 | 2.57     |
| episodes                | 35700    |
| lives                   | 35700    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 221112   |
| value_loss              | 11       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.6      |
| episodes                | 35800    |
| lives                   | 35800    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 221794   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.035   |
| entropy                 | 2.63     |
| episodes                | 35900    |
| lives                   | 35900    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 222491   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0554  |
| entropy                 | 2.59     |
| episodes                | 36000    |
| lives                   | 36000    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 223109   |
| value_loss              | 11.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0465  |
| entropy                 | 2.67     |
| episodes                | 36100    |
| lives                   | 36100    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 223766   |
| value_loss              | 10.4     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0676  |
| entropy                 | 2.7      |
| episodes                | 36200    |
| lives                   | 36200    |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0198   |
| steps                   | 224465   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.58     |
| episodes                | 36300    |
| lives                   | 36300    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0013   |
| steps                   | 225102   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0564  |
| entropy                 | 2.44     |
| episodes                | 36400    |
| lives                   | 36400    |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0036   |
| steps                   | 225685   |
| value_loss              | 9.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0452  |
| entropy                 | 2.47     |
| episodes                | 36500    |
| lives                   | 36500    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 226312   |
| value_loss              | 9.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0552  |
| entropy                 | 2.54     |
| episodes                | 36600    |
| lives                   | 36600    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 226946   |
| value_loss              | 9.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.065   |
| entropy                 | 2.48     |
| episodes                | 36700    |
| lives                   | 36700    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 227594   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0142  |
| entropy                 | 2.45     |
| episodes                | 36800    |
| lives                   | 36800    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 4.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 228195   |
| value_loss              | 9.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0554  |
| entropy                 | 2.39     |
| episodes                | 36900    |
| lives                   | 36900    |
| mean 100 episode length | 6.44     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0067   |
| steps                   | 228739   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0502  |
| entropy                 | 2.55     |
| episodes                | 37000    |
| lives                   | 37000    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0031   |
| steps                   | 229365   |
| value_loss              | 11.3     |
--------------------------------------
Saving model due to running mean reward increase: 5.7917 -> 5.9813
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.55     |
| episodes                | 37100    |
| lives                   | 37100    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 230001   |
| value_loss              | 11.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0372  |
| entropy                 | 2.49     |
| episodes                | 37200    |
| lives                   | 37200    |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 230585   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0291  |
| entropy                 | 2.54     |
| episodes                | 37300    |
| lives                   | 37300    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0245  |
| steps                   | 231219   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0531  |
| entropy                 | 2.54     |
| episodes                | 37400    |
| lives                   | 37400    |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 231799   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0389  |
| entropy                 | 2.64     |
| episodes                | 37500    |
| lives                   | 37500    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 232447   |
| value_loss              | 9.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0385  |
| entropy                 | 2.71     |
| episodes                | 37600    |
| lives                   | 37600    |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0098   |
| steps                   | 233170   |
| value_loss              | 8.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0333  |
| entropy                 | 2.6      |
| episodes                | 37700    |
| lives                   | 37700    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 233838   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0522  |
| entropy                 | 2.5      |
| episodes                | 37800    |
| lives                   | 37800    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 234427   |
| value_loss              | 11.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.056   |
| entropy                 | 2.54     |
| episodes                | 37900    |
| lives                   | 37900    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 235089   |
| value_loss              | 11.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0384  |
| entropy                 | 2.56     |
| episodes                | 38000    |
| lives                   | 38000    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 235754   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0524  |
| entropy                 | 2.62     |
| episodes                | 38100    |
| lives                   | 38100    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 236451   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.71     |
| episodes                | 38200    |
| lives                   | 38200    |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0041   |
| steps                   | 237192   |
| value_loss              | 9.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.65     |
| episodes                | 38300    |
| lives                   | 38300    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 237863   |
| value_loss              | 9.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0156  |
| entropy                 | 2.66     |
| episodes                | 38400    |
| lives                   | 38400    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 238578   |
| value_loss              | 9.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.58     |
| episodes                | 38500    |
| lives                   | 38500    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 239243   |
| value_loss              | 9.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0352  |
| entropy                 | 2.59     |
| episodes                | 38600    |
| lives                   | 38600    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 239861   |
| value_loss              | 10.6     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0208  |
| entropy                 | 2.59     |
| episodes                | 38700    |
| lives                   | 38700    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 240498   |
| value_loss              | 9.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.58     |
| episodes                | 38800    |
| lives                   | 38800    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 241132   |
| value_loss              | 9.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0299  |
| entropy                 | 2.64     |
| episodes                | 38900    |
| lives                   | 38900    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 241807   |
| value_loss              | 9.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.62     |
| episodes                | 39000    |
| lives                   | 39000    |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 242510   |
| value_loss              | 8.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0174  |
| entropy                 | 2.51     |
| episodes                | 39100    |
| lives                   | 39100    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 243107   |
| value_loss              | 9.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0564  |
| entropy                 | 2.42     |
| episodes                | 39200    |
| lives                   | 39200    |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0166  |
| steps                   | 243663   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0457  |
| entropy                 | 2.53     |
| episodes                | 39300    |
| lives                   | 39300    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 244319   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.033   |
| entropy                 | 2.55     |
| episodes                | 39400    |
| lives                   | 39400    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 244981   |
| value_loss              | 9.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0342  |
| entropy                 | 2.52     |
| episodes                | 39500    |
| lives                   | 39500    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 245628   |
| value_loss              | 9.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0281  |
| entropy                 | 2.58     |
| episodes                | 39600    |
| lives                   | 39600    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 246329   |
| value_loss              | 9.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0494  |
| entropy                 | 2.57     |
| episodes                | 39700    |
| lives                   | 39700    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 246996   |
| value_loss              | 8.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0442  |
| entropy                 | 2.46     |
| episodes                | 39800    |
| lives                   | 39800    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 247574   |
| value_loss              | 9.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.069   |
| entropy                 | 2.67     |
| episodes                | 39900    |
| lives                   | 39900    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.013    |
| steps                   | 248255   |
| value_loss              | 9.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0537  |
| entropy                 | 2.63     |
| episodes                | 40000    |
| lives                   | 40000    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 248912   |
| value_loss              | 9.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0327  |
| entropy                 | 2.57     |
| episodes                | 40100    |
| lives                   | 40100    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 249550   |
| value_loss              | 9.84     |
--------------------------------------
Saving model due to running mean reward increase: 5.6162 -> 5.7983
--------------------------------------
| approx_kl               | -0.0313  |
| entropy                 | 2.56     |
| episodes                | 40200    |
| lives                   | 40200    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 250234   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0267  |
| entropy                 | 2.55     |
| episodes                | 40300    |
| lives                   | 40300    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 250936   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0436  |
| entropy                 | 2.55     |
| episodes                | 40400    |
| lives                   | 40400    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 251569   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0364  |
| entropy                 | 2.62     |
| episodes                | 40500    |
| lives                   | 40500    |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 252283   |
| value_loss              | 9.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0408  |
| entropy                 | 2.62     |
| episodes                | 40600    |
| lives                   | 40600    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0093   |
| steps                   | 252956   |
| value_loss              | 9.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.63     |
| episodes                | 40700    |
| lives                   | 40700    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 253616   |
| value_loss              | 9.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.042   |
| entropy                 | 2.59     |
| episodes                | 40800    |
| lives                   | 40800    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0043   |
| steps                   | 254272   |
| value_loss              | 9.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0306  |
| entropy                 | 2.65     |
| episodes                | 40900    |
| lives                   | 40900    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 254921   |
| value_loss              | 8.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.69     |
| episodes                | 41000    |
| lives                   | 41000    |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0038   |
| steps                   | 255627   |
| value_loss              | 9.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.63     |
| episodes                | 41100    |
| lives                   | 41100    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 256275   |
| value_loss              | 9.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0266  |
| entropy                 | 2.65     |
| episodes                | 41200    |
| lives                   | 41200    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 256954   |
| value_loss              | 9.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0371  |
| entropy                 | 2.62     |
| episodes                | 41300    |
| lives                   | 41300    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 257558   |
| value_loss              | 9.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0615  |
| entropy                 | 2.64     |
| episodes                | 41400    |
| lives                   | 41400    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 4.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 258178   |
| value_loss              | 9.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0493  |
| entropy                 | 2.69     |
| episodes                | 41500    |
| lives                   | 41500    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 258854   |
| value_loss              | 8.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0547  |
| entropy                 | 2.67     |
| episodes                | 41600    |
| lives                   | 41600    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 259497   |
| value_loss              | 9.43     |
--------------------------------------
Saving model due to running mean reward increase: 5.1501 -> 5.207
--------------------------------------
| approx_kl               | -0.0411  |
| entropy                 | 2.67     |
| episodes                | 41700    |
| lives                   | 41700    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 260132   |
| value_loss              | 9.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.048   |
| entropy                 | 2.6      |
| episodes                | 41800    |
| lives                   | 41800    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 260761   |
| value_loss              | 8.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0354  |
| entropy                 | 2.7      |
| episodes                | 41900    |
| lives                   | 41900    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 261396   |
| value_loss              | 9.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.68     |
| episodes                | 42000    |
| lives                   | 42000    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 262048   |
| value_loss              | 9.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0355  |
| entropy                 | 2.7      |
| episodes                | 42100    |
| lives                   | 42100    |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 262761   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0377  |
| entropy                 | 2.71     |
| episodes                | 42200    |
| lives                   | 42200    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 263445   |
| value_loss              | 10       |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0458  |
| entropy                 | 2.65     |
| episodes                | 42300    |
| lives                   | 42300    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 264130   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0421  |
| entropy                 | 2.56     |
| episodes                | 42400    |
| lives                   | 42400    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 264787   |
| value_loss              | 10.9     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0583  |
| entropy                 | 2.5      |
| episodes                | 42500    |
| lives                   | 42500    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 265380   |
| value_loss              | 10.5     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0499  |
| entropy                 | 2.53     |
| episodes                | 42600    |
| lives                   | 42600    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 266017   |
| value_loss              | 10.7     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0501  |
| entropy                 | 2.53     |
| episodes                | 42700    |
| lives                   | 42700    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 266663   |
| value_loss              | 11.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0609  |
| entropy                 | 2.52     |
| episodes                | 42800    |
| lives                   | 42800    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 267308   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0524  |
| entropy                 | 2.51     |
| episodes                | 42900    |
| lives                   | 42900    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 267917   |
| value_loss              | 10.2     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0385  |
| entropy                 | 2.57     |
| episodes                | 43000    |
| lives                   | 43000    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 268568   |
| value_loss              | 9.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.64     |
| episodes                | 43100    |
| lives                   | 43100    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 269258   |
| value_loss              | 9.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0187  |
| entropy                 | 2.61     |
| episodes                | 43200    |
| lives                   | 43200    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 269887   |
| value_loss              | 8.78     |
--------------------------------------
Saving model due to running mean reward increase: 5.7202 -> 5.8308
--------------------------------------
| approx_kl               | -0.0254  |
| entropy                 | 2.71     |
| episodes                | 43300    |
| lives                   | 43300    |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 270618   |
| value_loss              | 8.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.67     |
| episodes                | 43400    |
| lives                   | 43400    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 271320   |
| value_loss              | 8.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.6      |
| episodes                | 43500    |
| lives                   | 43500    |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 272006   |
| value_loss              | 8.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0468  |
| entropy                 | 2.52     |
| episodes                | 43600    |
| lives                   | 43600    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 272682   |
| value_loss              | 9.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.58     |
| episodes                | 43700    |
| lives                   | 43700    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0013   |
| steps                   | 273337   |
| value_loss              | 9.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0182  |
| entropy                 | 2.69     |
| episodes                | 43800    |
| lives                   | 43800    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 274014   |
| value_loss              | 8.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.019   |
| entropy                 | 2.57     |
| episodes                | 43900    |
| lives                   | 43900    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 274703   |
| value_loss              | 9.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.52     |
| episodes                | 44000    |
| lives                   | 44000    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 275357   |
| value_loss              | 10.3     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.59     |
| episodes                | 44100    |
| lives                   | 44100    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 276023   |
| value_loss              | 8.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0231  |
| entropy                 | 2.65     |
| episodes                | 44200    |
| lives                   | 44200    |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 276769   |
| value_loss              | 8.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0411  |
| entropy                 | 2.56     |
| episodes                | 44300    |
| lives                   | 44300    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 277445   |
| value_loss              | 9.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0401  |
| entropy                 | 2.53     |
| episodes                | 44400    |
| lives                   | 44400    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 278076   |
| value_loss              | 8.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0564  |
| entropy                 | 2.52     |
| episodes                | 44500    |
| lives                   | 44500    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 278736   |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0569  |
| entropy                 | 2.5      |
| episodes                | 44600    |
| lives                   | 44600    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 279403   |
| value_loss              | 8.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0557  |
| entropy                 | 2.44     |
| episodes                | 44700    |
| lives                   | 44700    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 280004   |
| value_loss              | 8.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0431  |
| entropy                 | 2.48     |
| episodes                | 44800    |
| lives                   | 44800    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0204  |
| steps                   | 280642   |
| value_loss              | 9.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0547  |
| entropy                 | 2.56     |
| episodes                | 44900    |
| lives                   | 44900    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 281309   |
| value_loss              | 8.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0574  |
| entropy                 | 2.54     |
| episodes                | 45000    |
| lives                   | 45000    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 281951   |
| value_loss              | 8.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0426  |
| entropy                 | 2.62     |
| episodes                | 45100    |
| lives                   | 45100    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 282622   |
| value_loss              | 9.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0579  |
| entropy                 | 2.67     |
| episodes                | 45200    |
| lives                   | 45200    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0142   |
| steps                   | 283278   |
| value_loss              | 9.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0355  |
| entropy                 | 2.63     |
| episodes                | 45300    |
| lives                   | 45300    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 283932   |
| value_loss              | 9.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0541  |
| entropy                 | 2.55     |
| episodes                | 45400    |
| lives                   | 45400    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 284549   |
| value_loss              | 10.1     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0557  |
| entropy                 | 2.56     |
| episodes                | 45500    |
| lives                   | 45500    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 285185   |
| value_loss              | 10.8     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.063   |
| entropy                 | 2.49     |
| episodes                | 45600    |
| lives                   | 45600    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 285778   |
| value_loss              | 9.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0549  |
| entropy                 | 2.49     |
| episodes                | 45700    |
| lives                   | 45700    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 286388   |
| value_loss              | 9.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.5      |
| episodes                | 45800    |
| lives                   | 45800    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 286997   |
| value_loss              | 9.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.53     |
| episodes                | 45900    |
| lives                   | 45900    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 287623   |
| value_loss              | 9.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0379  |
| entropy                 | 2.54     |
| episodes                | 46000    |
| lives                   | 46000    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 288275   |
| value_loss              | 9.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.046   |
| entropy                 | 2.55     |
| episodes                | 46100    |
| lives                   | 46100    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 288919   |
| value_loss              | 9.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0446  |
| entropy                 | 2.56     |
| episodes                | 46200    |
| lives                   | 46200    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 289539   |
| value_loss              | 8.74     |
--------------------------------------
Saving model due to running mean reward increase: 5.347 -> 5.3664
--------------------------------------
| approx_kl               | -0.0587  |
| entropy                 | 2.51     |
| episodes                | 46300    |
| lives                   | 46300    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 290137   |
| value_loss              | 8.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0758  |
| entropy                 | 2.53     |
| episodes                | 46400    |
| lives                   | 46400    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0084   |
| steps                   | 290769   |
| value_loss              | 9.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0794  |
| entropy                 | 2.62     |
| episodes                | 46500    |
| lives                   | 46500    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.029    |
| steps                   | 291383   |
| value_loss              | 7.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0384  |
| entropy                 | 2.68     |
| episodes                | 46600    |
| lives                   | 46600    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 292049   |
| value_loss              | 8.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.06    |
| entropy                 | 2.67     |
| episodes                | 46700    |
| lives                   | 46700    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 292741   |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0377  |
| entropy                 | 2.59     |
| episodes                | 46800    |
| lives                   | 46800    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 293330   |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0529  |
| entropy                 | 2.62     |
| episodes                | 46900    |
| lives                   | 46900    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 293919   |
| value_loss              | 8.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0383  |
| entropy                 | 2.62     |
| episodes                | 47000    |
| lives                   | 47000    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0002   |
| steps                   | 294563   |
| value_loss              | 8.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.6      |
| episodes                | 47100    |
| lives                   | 47100    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 295198   |
| value_loss              | 9.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0616  |
| entropy                 | 2.62     |
| episodes                | 47200    |
| lives                   | 47200    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 295815   |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0643  |
| entropy                 | 2.61     |
| episodes                | 47300    |
| lives                   | 47300    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 296442   |
| value_loss              | 8.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0365  |
| entropy                 | 2.65     |
| episodes                | 47400    |
| lives                   | 47400    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 297126   |
| value_loss              | 8.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.052   |
| entropy                 | 2.61     |
| episodes                | 47500    |
| lives                   | 47500    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 297814   |
| value_loss              | 7.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0636  |
| entropy                 | 2.51     |
| episodes                | 47600    |
| lives                   | 47600    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 298451   |
| value_loss              | 8.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0413  |
| entropy                 | 2.52     |
| episodes                | 47700    |
| lives                   | 47700    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 299093   |
| value_loss              | 8.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0558  |
| entropy                 | 2.52     |
| episodes                | 47800    |
| lives                   | 47800    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 299731   |
| value_loss              | 9.14     |
--------------------------------------
Saving model due to running mean reward increase: 5.3798 -> 5.8266
--------------------------------------
| approx_kl               | -0.0454  |
| entropy                 | 2.53     |
| episodes                | 47900    |
| lives                   | 47900    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 300376   |
| value_loss              | 8.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.053   |
| entropy                 | 2.49     |
| episodes                | 48000    |
| lives                   | 48000    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 301005   |
| value_loss              | 9.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.56     |
| episodes                | 48100    |
| lives                   | 48100    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 301667   |
| value_loss              | 8.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.048   |
| entropy                 | 2.57     |
| episodes                | 48200    |
| lives                   | 48200    |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 302370   |
| value_loss              | 8.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.47     |
| episodes                | 48300    |
| lives                   | 48300    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 302977   |
| value_loss              | 9.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.51     |
| episodes                | 48400    |
| lives                   | 48400    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 303621   |
| value_loss              | 9.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0305  |
| entropy                 | 2.37     |
| episodes                | 48500    |
| lives                   | 48500    |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 304181   |
| value_loss              | 9.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.51     |
| episodes                | 48600    |
| lives                   | 48600    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 304794   |
| value_loss              | 9.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0643  |
| entropy                 | 2.49     |
| episodes                | 48700    |
| lives                   | 48700    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 305427   |
| value_loss              | 9.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0577  |
| entropy                 | 2.44     |
| episodes                | 48800    |
| lives                   | 48800    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 306025   |
| value_loss              | 9.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0668  |
| entropy                 | 2.48     |
| episodes                | 48900    |
| lives                   | 48900    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 306637   |
| value_loss              | 9.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0623  |
| entropy                 | 2.39     |
| episodes                | 49000    |
| lives                   | 49000    |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0026   |
| steps                   | 307201   |
| value_loss              | 9.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0508  |
| entropy                 | 2.4      |
| episodes                | 49100    |
| lives                   | 49100    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 307778   |
| value_loss              | 9.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.51     |
| episodes                | 49200    |
| lives                   | 49200    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 308404   |
| value_loss              | 9.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0416  |
| entropy                 | 2.57     |
| episodes                | 49300    |
| lives                   | 49300    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 309037   |
| value_loss              | 9.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0587  |
| entropy                 | 2.5      |
| episodes                | 49400    |
| lives                   | 49400    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 309629   |
| value_loss              | 9.11     |
--------------------------------------
Saving model due to running mean reward increase: 4.9199 -> 5.8716
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.58     |
| episodes                | 49500    |
| lives                   | 49500    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 310267   |
| value_loss              | 9.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.58     |
| episodes                | 49600    |
| lives                   | 49600    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 310898   |
| value_loss              | 8.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0527  |
| entropy                 | 2.5      |
| episodes                | 49700    |
| lives                   | 49700    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 311486   |
| value_loss              | 9.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0524  |
| entropy                 | 2.57     |
| episodes                | 49800    |
| lives                   | 49800    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 312117   |
| value_loss              | 9.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.049   |
| entropy                 | 2.61     |
| episodes                | 49900    |
| lives                   | 49900    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 312778   |
| value_loss              | 8.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0409  |
| entropy                 | 2.56     |
| episodes                | 50000    |
| lives                   | 50000    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 313402   |
| value_loss              | 9.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.043   |
| entropy                 | 2.59     |
| episodes                | 50100    |
| lives                   | 50100    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 314068   |
| value_loss              | 9.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0674  |
| entropy                 | 2.48     |
| episodes                | 50200    |
| lives                   | 50200    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 314686   |
| value_loss              | 8.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0484  |
| entropy                 | 2.49     |
| episodes                | 50300    |
| lives                   | 50300    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 315325   |
| value_loss              | 8.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0463  |
| entropy                 | 2.48     |
| episodes                | 50400    |
| lives                   | 50400    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0036   |
| steps                   | 315973   |
| value_loss              | 9.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.039   |
| entropy                 | 2.51     |
| episodes                | 50500    |
| lives                   | 50500    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 316560   |
| value_loss              | 9.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0509  |
| entropy                 | 2.61     |
| episodes                | 50600    |
| lives                   | 50600    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 317178   |
| value_loss              | 8.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.66     |
| episodes                | 50700    |
| lives                   | 50700    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0079   |
| steps                   | 317847   |
| value_loss              | 8.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0476  |
| entropy                 | 2.52     |
| episodes                | 50800    |
| lives                   | 50800    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 318444   |
| value_loss              | 9.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0373  |
| entropy                 | 2.51     |
| episodes                | 50900    |
| lives                   | 50900    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 319109   |
| value_loss              | 9.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0621  |
| entropy                 | 2.44     |
| episodes                | 51000    |
| lives                   | 51000    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 319712   |
| value_loss              | 8.97     |
--------------------------------------
Saving model due to mean reward increase: 6.3302 -> 6.676
Saving model due to running mean reward increase: 6.101 -> 6.676
--------------------------------------
| approx_kl               | -0.0458  |
| entropy                 | 2.56     |
| episodes                | 51100    |
| lives                   | 51100    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 320402   |
| value_loss              | 9.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.59     |
| episodes                | 51200    |
| lives                   | 51200    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 321078   |
| value_loss              | 8.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0199  |
| entropy                 | 2.6      |
| episodes                | 51300    |
| lives                   | 51300    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 321766   |
| value_loss              | 8.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0111  |
| entropy                 | 2.65     |
| episodes                | 51400    |
| lives                   | 51400    |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0217  |
| steps                   | 322513   |
| value_loss              | 7.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0524  |
| entropy                 | 2.63     |
| episodes                | 51500    |
| lives                   | 51500    |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 323225   |
| value_loss              | 7.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0516  |
| entropy                 | 2.53     |
| episodes                | 51600    |
| lives                   | 51600    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 323912   |
| value_loss              | 8.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0425  |
| entropy                 | 2.58     |
| episodes                | 51700    |
| lives                   | 51700    |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0097   |
| steps                   | 324644   |
| value_loss              | 8.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0367  |
| entropy                 | 2.52     |
| episodes                | 51800    |
| lives                   | 51800    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 325268   |
| value_loss              | 7.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0603  |
| entropy                 | 2.56     |
| episodes                | 51900    |
| lives                   | 51900    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 325906   |
| value_loss              | 7.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0709  |
| entropy                 | 2.49     |
| episodes                | 52000    |
| lives                   | 52000    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 326508   |
| value_loss              | 9.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.49     |
| episodes                | 52100    |
| lives                   | 52100    |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 327091   |
| value_loss              | 8.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0571  |
| entropy                 | 2.56     |
| episodes                | 52200    |
| lives                   | 52200    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 327723   |
| value_loss              | 7.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.05    |
| entropy                 | 2.56     |
| episodes                | 52300    |
| lives                   | 52300    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 328375   |
| value_loss              | 8.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0453  |
| entropy                 | 2.66     |
| episodes                | 52400    |
| lives                   | 52400    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 329046   |
| value_loss              | 7.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0371  |
| entropy                 | 2.73     |
| episodes                | 52500    |
| lives                   | 52500    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 329738   |
| value_loss              | 7.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0339  |
| entropy                 | 2.75     |
| episodes                | 52600    |
| lives                   | 52600    |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 330468   |
| value_loss              | 8.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.69     |
| episodes                | 52700    |
| lives                   | 52700    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 331146   |
| value_loss              | 8.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0723  |
| entropy                 | 2.63     |
| episodes                | 52800    |
| lives                   | 52800    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 331815   |
| value_loss              | 8.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0681  |
| entropy                 | 2.63     |
| episodes                | 52900    |
| lives                   | 52900    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 332494   |
| value_loss              | 8.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0897  |
| entropy                 | 2.55     |
| episodes                | 53000    |
| lives                   | 53000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.033    |
| steps                   | 333133   |
| value_loss              | 8.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0566  |
| entropy                 | 2.42     |
| episodes                | 53100    |
| lives                   | 53100    |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 333689   |
| value_loss              | 9.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0352  |
| entropy                 | 2.49     |
| episodes                | 53200    |
| lives                   | 53200    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 334325   |
| value_loss              | 8.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0567  |
| entropy                 | 2.47     |
| episodes                | 53300    |
| lives                   | 53300    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 334947   |
| value_loss              | 8.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0793  |
| entropy                 | 2.47     |
| episodes                | 53400    |
| lives                   | 53400    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 335548   |
| value_loss              | 9.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0332  |
| entropy                 | 2.58     |
| episodes                | 53500    |
| lives                   | 53500    |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 336259   |
| value_loss              | 8.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0307  |
| entropy                 | 2.51     |
| episodes                | 53600    |
| lives                   | 53600    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 336941   |
| value_loss              | 8.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.046   |
| entropy                 | 2.55     |
| episodes                | 53700    |
| lives                   | 53700    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 337623   |
| value_loss              | 7.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.44     |
| episodes                | 53800    |
| lives                   | 53800    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 338263   |
| value_loss              | 8.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0306  |
| entropy                 | 2.5      |
| episodes                | 53900    |
| lives                   | 53900    |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 338968   |
| value_loss              | 7.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0414  |
| entropy                 | 2.44     |
| episodes                | 54000    |
| lives                   | 54000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 339607   |
| value_loss              | 8.87     |
--------------------------------------
Saving model due to running mean reward increase: 6.006 -> 6.2821
--------------------------------------
| approx_kl               | -0.0278  |
| entropy                 | 2.41     |
| episodes                | 54100    |
| lives                   | 54100    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 340224   |
| value_loss              | 8.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0501  |
| entropy                 | 2.47     |
| episodes                | 54200    |
| lives                   | 54200    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 340892   |
| value_loss              | 9        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0357  |
| entropy                 | 2.51     |
| episodes                | 54300    |
| lives                   | 54300    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 341572   |
| value_loss              | 8.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0408  |
| entropy                 | 2.52     |
| episodes                | 54400    |
| lives                   | 54400    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 342260   |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.027   |
| entropy                 | 2.49     |
| episodes                | 54500    |
| lives                   | 54500    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 342910   |
| value_loss              | 5.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.032   |
| entropy                 | 2.52     |
| episodes                | 54600    |
| lives                   | 54600    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 343577   |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0366  |
| entropy                 | 2.47     |
| episodes                | 54700    |
| lives                   | 54700    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 344182   |
| value_loss              | 6.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0404  |
| entropy                 | 2.49     |
| episodes                | 54800    |
| lives                   | 54800    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 344879   |
| value_loss              | 6.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0408  |
| entropy                 | 2.57     |
| episodes                | 54900    |
| lives                   | 54900    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 345564   |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.016   |
| entropy                 | 2.58     |
| episodes                | 55000    |
| lives                   | 55000    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 346260   |
| value_loss              | 6.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0147  |
| entropy                 | 2.56     |
| episodes                | 55100    |
| lives                   | 55100    |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 346970   |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0353  |
| entropy                 | 2.55     |
| episodes                | 55200    |
| lives                   | 55200    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 347636   |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0369  |
| entropy                 | 2.49     |
| episodes                | 55300    |
| lives                   | 55300    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 348302   |
| value_loss              | 5.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.44     |
| episodes                | 55400    |
| lives                   | 55400    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 348982   |
| value_loss              | 6.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0156  |
| entropy                 | 2.42     |
| episodes                | 55500    |
| lives                   | 55500    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 349655   |
| value_loss              | 5.82     |
--------------------------------------
Saving model due to mean reward increase: 6.676 -> 6.7428
Saving model due to running mean reward increase: 6.6335 -> 6.7428
--------------------------------------
| approx_kl               | -0.0302  |
| entropy                 | 2.45     |
| episodes                | 55600    |
| lives                   | 55600    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0246  |
| steps                   | 350335   |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.053   |
| entropy                 | 2.49     |
| episodes                | 55700    |
| lives                   | 55700    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 350997   |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.52     |
| episodes                | 55800    |
| lives                   | 55800    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 351646   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0424  |
| entropy                 | 2.58     |
| episodes                | 55900    |
| lives                   | 55900    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 352339   |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.6      |
| episodes                | 56000    |
| lives                   | 56000    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 352991   |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0291  |
| entropy                 | 2.52     |
| episodes                | 56100    |
| lives                   | 56100    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0062   |
| steps                   | 353642   |
| value_loss              | 6.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.42     |
| episodes                | 56200    |
| lives                   | 56200    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 354241   |
| value_loss              | 6.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0383  |
| entropy                 | 2.52     |
| episodes                | 56300    |
| lives                   | 56300    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 354918   |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0512  |
| entropy                 | 2.51     |
| episodes                | 56400    |
| lives                   | 56400    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 355553   |
| value_loss              | 5.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0307  |
| entropy                 | 2.49     |
| episodes                | 56500    |
| lives                   | 56500    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 356210   |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0139  |
| entropy                 | 2.32     |
| episodes                | 56600    |
| lives                   | 56600    |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 356784   |
| value_loss              | 6.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0457  |
| entropy                 | 2.4      |
| episodes                | 56700    |
| lives                   | 56700    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0205  |
| steps                   | 357413   |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.49     |
| episodes                | 56800    |
| lives                   | 56800    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 358071   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0452  |
| entropy                 | 2.48     |
| episodes                | 56900    |
| lives                   | 56900    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 358719   |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.065   |
| entropy                 | 2.41     |
| episodes                | 57000    |
| lives                   | 57000    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 359341   |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0634  |
| entropy                 | 2.45     |
| episodes                | 57100    |
| lives                   | 57100    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0022   |
| steps                   | 359979   |
| value_loss              | 6.57     |
--------------------------------------
Saving model due to running mean reward increase: 5.5024 -> 6.0261
--------------------------------------
| approx_kl               | -0.0438  |
| entropy                 | 2.41     |
| episodes                | 57200    |
| lives                   | 57200    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 360621   |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0527  |
| entropy                 | 2.39     |
| episodes                | 57300    |
| lives                   | 57300    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 361243   |
| value_loss              | 7.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0267  |
| entropy                 | 2.4      |
| episodes                | 57400    |
| lives                   | 57400    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 361843   |
| value_loss              | 6.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0379  |
| entropy                 | 2.47     |
| episodes                | 57500    |
| lives                   | 57500    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 362461   |
| value_loss              | 6.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0375  |
| entropy                 | 2.5      |
| episodes                | 57600    |
| lives                   | 57600    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 363136   |
| value_loss              | 6.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.46     |
| episodes                | 57700    |
| lives                   | 57700    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 363749   |
| value_loss              | 6.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.51     |
| episodes                | 57800    |
| lives                   | 57800    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 364427   |
| value_loss              | 7.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0228  |
| entropy                 | 2.49     |
| episodes                | 57900    |
| lives                   | 57900    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 365094   |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.45     |
| episodes                | 58000    |
| lives                   | 58000    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0166  |
| steps                   | 365720   |
| value_loss              | 5.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0221  |
| entropy                 | 2.48     |
| episodes                | 58100    |
| lives                   | 58100    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 366373   |
| value_loss              | 5.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.53     |
| episodes                | 58200    |
| lives                   | 58200    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 367053   |
| value_loss              | 4.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.56     |
| episodes                | 58300    |
| lives                   | 58300    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 367711   |
| value_loss              | 4.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0455  |
| entropy                 | 2.52     |
| episodes                | 58400    |
| lives                   | 58400    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 368339   |
| value_loss              | 4.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0682  |
| entropy                 | 2.49     |
| episodes                | 58500    |
| lives                   | 58500    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 368993   |
| value_loss              | 5.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0472  |
| entropy                 | 2.49     |
| episodes                | 58600    |
| lives                   | 58600    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 369646   |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0496  |
| entropy                 | 2.54     |
| episodes                | 58700    |
| lives                   | 58700    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 370331   |
| value_loss              | 4.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0607  |
| entropy                 | 2.54     |
| episodes                | 58800    |
| lives                   | 58800    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 370957   |
| value_loss              | 4.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0488  |
| entropy                 | 2.55     |
| episodes                | 58900    |
| lives                   | 58900    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 371608   |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0183  |
| entropy                 | 2.59     |
| episodes                | 59000    |
| lives                   | 59000    |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 372321   |
| value_loss              | 5.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0306  |
| entropy                 | 2.49     |
| episodes                | 59100    |
| lives                   | 59100    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 372959   |
| value_loss              | 5.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.53     |
| episodes                | 59200    |
| lives                   | 59200    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0091   |
| steps                   | 373606   |
| value_loss              | 6.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0183  |
| entropy                 | 2.52     |
| episodes                | 59300    |
| lives                   | 59300    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 374278   |
| value_loss              | 5.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.46     |
| episodes                | 59400    |
| lives                   | 59400    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 374969   |
| value_loss              | 5.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0114  |
| entropy                 | 2.35     |
| episodes                | 59500    |
| lives                   | 59500    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 375595   |
| value_loss              | 5.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0247  |
| entropy                 | 2.37     |
| episodes                | 59600    |
| lives                   | 59600    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 376214   |
| value_loss              | 5.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0151  |
| entropy                 | 2.4      |
| episodes                | 59700    |
| lives                   | 59700    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 376851   |
| value_loss              | 5.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0102  |
| entropy                 | 2.43     |
| episodes                | 59800    |
| lives                   | 59800    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 377495   |
| value_loss              | 5.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0166  |
| entropy                 | 2.43     |
| episodes                | 59900    |
| lives                   | 59900    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 378153   |
| value_loss              | 5.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.41     |
| episodes                | 60000    |
| lives                   | 60000    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 378783   |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0191  |
| entropy                 | 2.52     |
| episodes                | 60100    |
| lives                   | 60100    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 379466   |
| value_loss              | 6.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0225  |
| entropy                 | 2.48     |
| episodes                | 60200    |
| lives                   | 60200    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 380083   |
| value_loss              | 6.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.5      |
| episodes                | 60300    |
| lives                   | 60300    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0192  |
| steps                   | 380742   |
| value_loss              | 6.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0172  |
| entropy                 | 2.51     |
| episodes                | 60400    |
| lives                   | 60400    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 381410   |
| value_loss              | 6.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.016   |
| entropy                 | 2.45     |
| episodes                | 60500    |
| lives                   | 60500    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 382072   |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.42     |
| episodes                | 60600    |
| lives                   | 60600    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 382680   |
| value_loss              | 6.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0396  |
| entropy                 | 2.47     |
| episodes                | 60700    |
| lives                   | 60700    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 383338   |
| value_loss              | 6.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.43     |
| episodes                | 60800    |
| lives                   | 60800    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 383997   |
| value_loss              | 6.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.48     |
| episodes                | 60900    |
| lives                   | 60900    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 384676   |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.48     |
| episodes                | 61000    |
| lives                   | 61000    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0209  |
| steps                   | 385342   |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0284  |
| entropy                 | 2.48     |
| episodes                | 61100    |
| lives                   | 61100    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0201  |
| steps                   | 386003   |
| value_loss              | 6.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.037   |
| entropy                 | 2.52     |
| episodes                | 61200    |
| lives                   | 61200    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 386665   |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0085  |
| entropy                 | 2.57     |
| episodes                | 61300    |
| lives                   | 61300    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 387361   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0287  |
| entropy                 | 2.53     |
| episodes                | 61400    |
| lives                   | 61400    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 388016   |
| value_loss              | 5.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.52     |
| episodes                | 61500    |
| lives                   | 61500    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 388663   |
| value_loss              | 5.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0233  |
| entropy                 | 2.49     |
| episodes                | 61600    |
| lives                   | 61600    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 389305   |
| value_loss              | 4.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0179  |
| entropy                 | 2.42     |
| episodes                | 61700    |
| lives                   | 61700    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 389979   |
| value_loss              | 5.57     |
--------------------------------------
Saving model due to running mean reward increase: 6.2968 -> 6.5696
--------------------------------------
| approx_kl               | -0.0199  |
| entropy                 | 2.42     |
| episodes                | 61800    |
| lives                   | 61800    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 390626   |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0419  |
| entropy                 | 2.44     |
| episodes                | 61900    |
| lives                   | 61900    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 391241   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0355  |
| entropy                 | 2.45     |
| episodes                | 62000    |
| lives                   | 62000    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 391857   |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0397  |
| entropy                 | 2.53     |
| episodes                | 62100    |
| lives                   | 62100    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 392501   |
| value_loss              | 7.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.58     |
| episodes                | 62200    |
| lives                   | 62200    |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 393205   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0167  |
| entropy                 | 2.63     |
| episodes                | 62300    |
| lives                   | 62300    |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 393974   |
| value_loss              | 6.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.62     |
| episodes                | 62400    |
| lives                   | 62400    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 394669   |
| value_loss              | 5.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.64     |
| episodes                | 62500    |
| lives                   | 62500    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 395384   |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0394  |
| entropy                 | 2.6      |
| episodes                | 62600    |
| lives                   | 62600    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 396049   |
| value_loss              | 6.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0532  |
| entropy                 | 2.61     |
| episodes                | 62700    |
| lives                   | 62700    |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 396784   |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0277  |
| entropy                 | 2.52     |
| episodes                | 62800    |
| lives                   | 62800    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0008   |
| steps                   | 397418   |
| value_loss              | 4.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0219  |
| entropy                 | 2.54     |
| episodes                | 62900    |
| lives                   | 62900    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 398064   |
| value_loss              | 5.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.45     |
| episodes                | 63000    |
| lives                   | 63000    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 398664   |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0476  |
| entropy                 | 2.58     |
| episodes                | 63100    |
| lives                   | 63100    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0221  |
| steps                   | 399335   |
| value_loss              | 5.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0626  |
| entropy                 | 2.5      |
| episodes                | 63200    |
| lives                   | 63200    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 399940   |
| value_loss              | 6.09     |
--------------------------------------
Saving model due to running mean reward increase: 5.4712 -> 5.5438
--------------------------------------
| approx_kl               | -0.0736  |
| entropy                 | 2.44     |
| episodes                | 63300    |
| lives                   | 63300    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 400562   |
| value_loss              | 6.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0478  |
| entropy                 | 2.5      |
| episodes                | 63400    |
| lives                   | 63400    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 401185   |
| value_loss              | 5.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0685  |
| entropy                 | 2.57     |
| episodes                | 63500    |
| lives                   | 63500    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0008   |
| steps                   | 401821   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0553  |
| entropy                 | 2.5      |
| episodes                | 63600    |
| lives                   | 63600    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 402435   |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0369  |
| entropy                 | 2.5      |
| episodes                | 63700    |
| lives                   | 63700    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 403095   |
| value_loss              | 6.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0497  |
| entropy                 | 2.45     |
| episodes                | 63800    |
| lives                   | 63800    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 403737   |
| value_loss              | 6.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0556  |
| entropy                 | 2.49     |
| episodes                | 63900    |
| lives                   | 63900    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 404400   |
| value_loss              | 6.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.47     |
| episodes                | 64000    |
| lives                   | 64000    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 5.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 405046   |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.42     |
| episodes                | 64100    |
| lives                   | 64100    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0222  |
| steps                   | 405704   |
| value_loss              | 5.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0439  |
| entropy                 | 2.39     |
| episodes                | 64200    |
| lives                   | 64200    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 406324   |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0414  |
| entropy                 | 2.48     |
| episodes                | 64300    |
| lives                   | 64300    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 406953   |
| value_loss              | 6.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0269  |
| entropy                 | 2.54     |
| episodes                | 64400    |
| lives                   | 64400    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0203  |
| steps                   | 407636   |
| value_loss              | 6.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0452  |
| entropy                 | 2.54     |
| episodes                | 64500    |
| lives                   | 64500    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 408326   |
| value_loss              | 5.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.53     |
| episodes                | 64600    |
| lives                   | 64600    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0049   |
| steps                   | 408979   |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0212  |
| entropy                 | 2.5      |
| episodes                | 64700    |
| lives                   | 64700    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 409608   |
| value_loss              | 6.62     |
--------------------------------------
Saving model due to running mean reward increase: 5.7227 -> 6.3495
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.46     |
| episodes                | 64800    |
| lives                   | 64800    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 410268   |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0246  |
| entropy                 | 2.47     |
| episodes                | 64900    |
| lives                   | 64900    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0182  |
| steps                   | 410942   |
| value_loss              | 6.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.029   |
| entropy                 | 2.47     |
| episodes                | 65000    |
| lives                   | 65000    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 411603   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.016   |
| entropy                 | 2.49     |
| episodes                | 65100    |
| lives                   | 65100    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 412290   |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.48     |
| episodes                | 65200    |
| lives                   | 65200    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 412935   |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.51     |
| episodes                | 65300    |
| lives                   | 65300    |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 413640   |
| value_loss              | 5.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.013   |
| entropy                 | 2.49     |
| episodes                | 65400    |
| lives                   | 65400    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 414323   |
| value_loss              | 5.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.44     |
| episodes                | 65500    |
| lives                   | 65500    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0093   |
| steps                   | 414927   |
| value_loss              | 5.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0313  |
| entropy                 | 2.41     |
| episodes                | 65600    |
| lives                   | 65600    |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0221  |
| steps                   | 415502   |
| value_loss              | 6.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0154  |
| entropy                 | 2.53     |
| episodes                | 65700    |
| lives                   | 65700    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 416195   |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0423  |
| entropy                 | 2.42     |
| episodes                | 65800    |
| lives                   | 65800    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 416817   |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.48     |
| episodes                | 65900    |
| lives                   | 65900    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 417450   |
| value_loss              | 5.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0514  |
| entropy                 | 2.43     |
| episodes                | 66000    |
| lives                   | 66000    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 418051   |
| value_loss              | 6.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0856  |
| entropy                 | 2.38     |
| episodes                | 66100    |
| lives                   | 66100    |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 418614   |
| value_loss              | 6.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0473  |
| entropy                 | 2.44     |
| episodes                | 66200    |
| lives                   | 66200    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 419238   |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0578  |
| entropy                 | 2.4      |
| episodes                | 66300    |
| lives                   | 66300    |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 419777   |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0546  |
| entropy                 | 2.48     |
| episodes                | 66400    |
| lives                   | 66400    |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 420371   |
| value_loss              | 6.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.059   |
| entropy                 | 2.54     |
| episodes                | 66500    |
| lives                   | 66500    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 421012   |
| value_loss              | 6.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0484  |
| entropy                 | 2.53     |
| episodes                | 66600    |
| lives                   | 66600    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 421653   |
| value_loss              | 6.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0676  |
| entropy                 | 2.43     |
| episodes                | 66700    |
| lives                   | 66700    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 422244   |
| value_loss              | 6.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0539  |
| entropy                 | 2.46     |
| episodes                | 66800    |
| lives                   | 66800    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 422876   |
| value_loss              | 6.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0808  |
| entropy                 | 2.42     |
| episodes                | 66900    |
| lives                   | 66900    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0023   |
| steps                   | 423494   |
| value_loss              | 7.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0576  |
| entropy                 | 2.46     |
| episodes                | 67000    |
| lives                   | 67000    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 424099   |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0497  |
| entropy                 | 2.5      |
| episodes                | 67100    |
| lives                   | 67100    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 424743   |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0372  |
| entropy                 | 2.44     |
| episodes                | 67200    |
| lives                   | 67200    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 425347   |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.53     |
| episodes                | 67300    |
| lives                   | 67300    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0072   |
| steps                   | 426016   |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0239  |
| entropy                 | 2.45     |
| episodes                | 67400    |
| lives                   | 67400    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 426625   |
| value_loss              | 5.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0279  |
| entropy                 | 2.52     |
| episodes                | 67500    |
| lives                   | 67500    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 427284   |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0393  |
| entropy                 | 2.45     |
| episodes                | 67600    |
| lives                   | 67600    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 427941   |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.039   |
| entropy                 | 2.52     |
| episodes                | 67700    |
| lives                   | 67700    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 428588   |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.64     |
| episodes                | 67800    |
| lives                   | 67800    |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 429350   |
| value_loss              | 5.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.51     |
| episodes                | 67900    |
| lives                   | 67900    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 429992   |
| value_loss              | 5.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0422  |
| entropy                 | 2.49     |
| episodes                | 68000    |
| lives                   | 68000    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 430599   |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0608  |
| entropy                 | 2.51     |
| episodes                | 68100    |
| lives                   | 68100    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 431204   |
| value_loss              | 6.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0456  |
| entropy                 | 2.53     |
| episodes                | 68200    |
| lives                   | 68200    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 431854   |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0339  |
| entropy                 | 2.56     |
| episodes                | 68300    |
| lives                   | 68300    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 432546   |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0586  |
| entropy                 | 2.58     |
| episodes                | 68400    |
| lives                   | 68400    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 433207   |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0525  |
| entropy                 | 2.6      |
| episodes                | 68500    |
| lives                   | 68500    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 433862   |
| value_loss              | 6.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.032   |
| entropy                 | 2.6      |
| episodes                | 68600    |
| lives                   | 68600    |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 434561   |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.57     |
| episodes                | 68700    |
| lives                   | 68700    |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 435271   |
| value_loss              | 6.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0537  |
| entropy                 | 2.52     |
| episodes                | 68800    |
| lives                   | 68800    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 435964   |
| value_loss              | 5.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.45     |
| episodes                | 68900    |
| lives                   | 68900    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 436614   |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0246  |
| entropy                 | 2.53     |
| episodes                | 69000    |
| lives                   | 69000    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 437311   |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0152  |
| entropy                 | 2.56     |
| episodes                | 69100    |
| lives                   | 69100    |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 438030   |
| value_loss              | 5.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0217  |
| entropy                 | 2.5      |
| episodes                | 69200    |
| lives                   | 69200    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 438711   |
| value_loss              | 5.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0145  |
| entropy                 | 2.51     |
| episodes                | 69300    |
| lives                   | 69300    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 439408   |
| value_loss              | 5.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 2.45     |
| episodes                | 69400    |
| lives                   | 69400    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 440102   |
| value_loss              | 4.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0132  |
| entropy                 | 2.5      |
| episodes                | 69500    |
| lives                   | 69500    |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 7.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 440848   |
| value_loss              | 5.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0202  |
| entropy                 | 2.5      |
| episodes                | 69600    |
| lives                   | 69600    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 441523   |
| value_loss              | 4.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0174  |
| entropy                 | 2.52     |
| episodes                | 69700    |
| lives                   | 69700    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 442194   |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0329  |
| entropy                 | 2.53     |
| episodes                | 69800    |
| lives                   | 69800    |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 442880   |
| value_loss              | 5.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0253  |
| entropy                 | 2.44     |
| episodes                | 69900    |
| lives                   | 69900    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 443561   |
| value_loss              | 5.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0427  |
| entropy                 | 2.42     |
| episodes                | 70000    |
| lives                   | 70000    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 444191   |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0184  |
| entropy                 | 2.59     |
| episodes                | 70100    |
| lives                   | 70100    |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 444921   |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0274  |
| entropy                 | 2.53     |
| episodes                | 70200    |
| lives                   | 70200    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 445567   |
| value_loss              | 5.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.56     |
| episodes                | 70300    |
| lives                   | 70300    |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 446285   |
| value_loss              | 5.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.46     |
| episodes                | 70400    |
| lives                   | 70400    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 446941   |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.46     |
| episodes                | 70500    |
| lives                   | 70500    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 447561   |
| value_loss              | 5.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.51     |
| episodes                | 70600    |
| lives                   | 70600    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 448219   |
| value_loss              | 5.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0296  |
| entropy                 | 2.47     |
| episodes                | 70700    |
| lives                   | 70700    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 448882   |
| value_loss              | 5.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0364  |
| entropy                 | 2.46     |
| episodes                | 70800    |
| lives                   | 70800    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 449552   |
| value_loss              | 5.79     |
--------------------------------------
Saving model due to running mean reward increase: 6.1399 -> 6.3206
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.42     |
| episodes                | 70900    |
| lives                   | 70900    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 450231   |
| value_loss              | 5.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0207  |
| entropy                 | 2.48     |
| episodes                | 71000    |
| lives                   | 71000    |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 450968   |
| value_loss              | 5.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0327  |
| entropy                 | 2.48     |
| episodes                | 71100    |
| lives                   | 71100    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 451641   |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0304  |
| entropy                 | 2.48     |
| episodes                | 71200    |
| lives                   | 71200    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 452331   |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.46     |
| episodes                | 71300    |
| lives                   | 71300    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 453008   |
| value_loss              | 5.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0316  |
| entropy                 | 2.48     |
| episodes                | 71400    |
| lives                   | 71400    |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 7.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 453711   |
| value_loss              | 7.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.51     |
| episodes                | 71500    |
| lives                   | 71500    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 454386   |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0202  |
| entropy                 | 2.46     |
| episodes                | 71600    |
| lives                   | 71600    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 455050   |
| value_loss              | 6.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0214  |
| entropy                 | 2.53     |
| episodes                | 71700    |
| lives                   | 71700    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 455743   |
| value_loss              | 6.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0209  |
| entropy                 | 2.51     |
| episodes                | 71800    |
| lives                   | 71800    |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 456456   |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0175  |
| entropy                 | 2.53     |
| episodes                | 71900    |
| lives                   | 71900    |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 457198   |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.48     |
| episodes                | 72000    |
| lives                   | 72000    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 457894   |
| value_loss              | 4.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0192  |
| entropy                 | 2.54     |
| episodes                | 72100    |
| lives                   | 72100    |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 458623   |
| value_loss              | 4.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.47     |
| episodes                | 72200    |
| lives                   | 72200    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 459283   |
| value_loss              | 4.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0159  |
| entropy                 | 2.45     |
| episodes                | 72300    |
| lives                   | 72300    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 459976   |
| value_loss              | 4.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0087  |
| entropy                 | 2.45     |
| episodes                | 72400    |
| lives                   | 72400    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 460657   |
| value_loss              | 5.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.46     |
| episodes                | 72500    |
| lives                   | 72500    |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0208  |
| steps                   | 461373   |
| value_loss              | 4.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0058  |
| entropy                 | 2.44     |
| episodes                | 72600    |
| lives                   | 72600    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 462037   |
| value_loss              | 4.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0133  |
| entropy                 | 2.49     |
| episodes                | 72700    |
| lives                   | 72700    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 462704   |
| value_loss              | 4.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0268  |
| entropy                 | 2.44     |
| episodes                | 72800    |
| lives                   | 72800    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 463337   |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0665  |
| entropy                 | 2.48     |
| episodes                | 72900    |
| lives                   | 72900    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0084   |
| steps                   | 464026   |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0382  |
| entropy                 | 2.51     |
| episodes                | 73000    |
| lives                   | 73000    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0104   |
| steps                   | 464706   |
| value_loss              | 6.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.4      |
| episodes                | 73100    |
| lives                   | 73100    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 465285   |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.39     |
| episodes                | 73200    |
| lives                   | 73200    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 465873   |
| value_loss              | 7.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0463  |
| entropy                 | 2.36     |
| episodes                | 73300    |
| lives                   | 73300    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 466450   |
| value_loss              | 8.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0668  |
| entropy                 | 2.42     |
| episodes                | 73400    |
| lives                   | 73400    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 467026   |
| value_loss              | 7.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0723  |
| entropy                 | 2.47     |
| episodes                | 73500    |
| lives                   | 73500    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 467681   |
| value_loss              | 8.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0625  |
| entropy                 | 2.48     |
| episodes                | 73600    |
| lives                   | 73600    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 468310   |
| value_loss              | 8.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0832  |
| entropy                 | 2.53     |
| episodes                | 73700    |
| lives                   | 73700    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 468934   |
| value_loss              | 7.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0471  |
| entropy                 | 2.53     |
| episodes                | 73800    |
| lives                   | 73800    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 469559   |
| value_loss              | 7.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0675  |
| entropy                 | 2.57     |
| episodes                | 73900    |
| lives                   | 73900    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 470200   |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.07    |
| entropy                 | 2.59     |
| episodes                | 74000    |
| lives                   | 74000    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 470841   |
| value_loss              | 7.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0954  |
| entropy                 | 2.6      |
| episodes                | 74100    |
| lives                   | 74100    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.035    |
| steps                   | 471484   |
| value_loss              | 6.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0122  |
| entropy                 | 2.58     |
| episodes                | 74200    |
| lives                   | 74200    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 472169   |
| value_loss              | 7.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0282  |
| entropy                 | 2.6      |
| episodes                | 74300    |
| lives                   | 74300    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 472851   |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0231  |
| entropy                 | 2.62     |
| episodes                | 74400    |
| lives                   | 74400    |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 473567   |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0282  |
| entropy                 | 2.67     |
| episodes                | 74500    |
| lives                   | 74500    |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 474322   |
| value_loss              | 7        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0342  |
| entropy                 | 2.65     |
| episodes                | 74600    |
| lives                   | 74600    |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 475077   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.05    |
| entropy                 | 2.52     |
| episodes                | 74700    |
| lives                   | 74700    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 475711   |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0311  |
| entropy                 | 2.64     |
| episodes                | 74800    |
| lives                   | 74800    |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 476463   |
| value_loss              | 6.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0245  |
| entropy                 | 2.64     |
| episodes                | 74900    |
| lives                   | 74900    |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 477199   |
| value_loss              | 7.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0389  |
| entropy                 | 2.73     |
| episodes                | 75000    |
| lives                   | 75000    |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0104   |
| steps                   | 477925   |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0402  |
| entropy                 | 2.62     |
| episodes                | 75100    |
| lives                   | 75100    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 478576   |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0217  |
| entropy                 | 2.59     |
| episodes                | 75200    |
| lives                   | 75200    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 479236   |
| value_loss              | 7.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0225  |
| entropy                 | 2.57     |
| episodes                | 75300    |
| lives                   | 75300    |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 479947   |
| value_loss              | 7.17     |
--------------------------------------
Saving model due to running mean reward increase: 5.8033 -> 6.0927
--------------------------------------
| approx_kl               | -0.0277  |
| entropy                 | 2.54     |
| episodes                | 75400    |
| lives                   | 75400    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 480622   |
| value_loss              | 7.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0483  |
| entropy                 | 2.55     |
| episodes                | 75500    |
| lives                   | 75500    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 481329   |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.52     |
| episodes                | 75600    |
| lives                   | 75600    |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 482079   |
| value_loss              | 6.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0634  |
| entropy                 | 2.51     |
| episodes                | 75700    |
| lives                   | 75700    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 482764   |
| value_loss              | 6.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0566  |
| entropy                 | 2.56     |
| episodes                | 75800    |
| lives                   | 75800    |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 483475   |
| value_loss              | 6.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0376  |
| entropy                 | 2.57     |
| episodes                | 75900    |
| lives                   | 75900    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0056   |
| steps                   | 484184   |
| value_loss              | 7.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0221  |
| entropy                 | 2.55     |
| episodes                | 76000    |
| lives                   | 76000    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 484849   |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0281  |
| entropy                 | 2.42     |
| episodes                | 76100    |
| lives                   | 76100    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 485430   |
| value_loss              | 7.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0647  |
| entropy                 | 2.39     |
| episodes                | 76200    |
| lives                   | 76200    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0181  |
| steps                   | 486021   |
| value_loss              | 9.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.45     |
| episodes                | 76300    |
| lives                   | 76300    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 486636   |
| value_loss              | 8.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0813  |
| entropy                 | 2.44     |
| episodes                | 76400    |
| lives                   | 76400    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 487236   |
| value_loss              | 8.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0711  |
| entropy                 | 2.52     |
| episodes                | 76500    |
| lives                   | 76500    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 487901   |
| value_loss              | 8.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0839  |
| entropy                 | 2.47     |
| episodes                | 76600    |
| lives                   | 76600    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 488509   |
| value_loss              | 8.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0591  |
| entropy                 | 2.44     |
| episodes                | 76700    |
| lives                   | 76700    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 489114   |
| value_loss              | 8.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0649  |
| entropy                 | 2.5      |
| episodes                | 76800    |
| lives                   | 76800    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 489734   |
| value_loss              | 8.6      |
--------------------------------------
Saving model due to running mean reward increase: 5.1568 -> 6.1612
--------------------------------------
| approx_kl               | -0.0612  |
| entropy                 | 2.5      |
| episodes                | 76900    |
| lives                   | 76900    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0238  |
| steps                   | 490342   |
| value_loss              | 8.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0567  |
| entropy                 | 2.62     |
| episodes                | 77000    |
| lives                   | 77000    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 491008   |
| value_loss              | 7.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0579  |
| entropy                 | 2.68     |
| episodes                | 77100    |
| lives                   | 77100    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 491715   |
| value_loss              | 7.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0192  |
| entropy                 | 2.69     |
| episodes                | 77200    |
| lives                   | 77200    |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 492456   |
| value_loss              | 7.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.68     |
| episodes                | 77300    |
| lives                   | 77300    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 5.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0043   |
| steps                   | 493149   |
| value_loss              | 7.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0129  |
| entropy                 | 2.59     |
| episodes                | 77400    |
| lives                   | 77400    |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0032   |
| steps                   | 493863   |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.53     |
| episodes                | 77500    |
| lives                   | 77500    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 494520   |
| value_loss              | 7.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.59     |
| episodes                | 77600    |
| lives                   | 77600    |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 7.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 495270   |
| value_loss              | 7.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.46     |
| episodes                | 77700    |
| lives                   | 77700    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 495906   |
| value_loss              | 8.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0217  |
| entropy                 | 2.53     |
| episodes                | 77800    |
| lives                   | 77800    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 496590   |
| value_loss              | 7.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0116  |
| entropy                 | 2.51     |
| episodes                | 77900    |
| lives                   | 77900    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 497305   |
| value_loss              | 7.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0111  |
| entropy                 | 2.51     |
| episodes                | 78000    |
| lives                   | 78000    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 7.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 498014   |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0387  |
| entropy                 | 2.52     |
| episodes                | 78100    |
| lives                   | 78100    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 498692   |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0246  |
| entropy                 | 2.52     |
| episodes                | 78200    |
| lives                   | 78200    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 499361   |
| value_loss              | 7.71     |
--------------------------------------
Saving model due to mean reward increase: 6.7428 -> 6.8561
Saving model due to running mean reward increase: 6.6548 -> 6.8561
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.53     |
| episodes                | 78300    |
| lives                   | 78300    |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 7.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 500095   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0115  |
| entropy                 | 2.61     |
| episodes                | 78400    |
| lives                   | 78400    |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 500851   |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.51     |
| episodes                | 78500    |
| lives                   | 78500    |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0217  |
| steps                   | 501562   |
| value_loss              | 5.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0226  |
| entropy                 | 2.5      |
| episodes                | 78600    |
| lives                   | 78600    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 502247   |
| value_loss              | 5.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0206  |
| entropy                 | 2.45     |
| episodes                | 78700    |
| lives                   | 78700    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 502923   |
| value_loss              | 5.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.48     |
| episodes                | 78800    |
| lives                   | 78800    |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 503627   |
| value_loss              | 5.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.015   |
| entropy                 | 2.54     |
| episodes                | 78900    |
| lives                   | 78900    |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 504354   |
| value_loss              | 5.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0168  |
| entropy                 | 2.55     |
| episodes                | 79000    |
| lives                   | 79000    |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 505073   |
| value_loss              | 5.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0177  |
| entropy                 | 2.47     |
| episodes                | 79100    |
| lives                   | 79100    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 505767   |
| value_loss              | 6.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0213  |
| entropy                 | 2.54     |
| episodes                | 79200    |
| lives                   | 79200    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 506468   |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0143  |
| entropy                 | 2.55     |
| episodes                | 79300    |
| lives                   | 79300    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 507134   |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0296  |
| entropy                 | 2.59     |
| episodes                | 79400    |
| lives                   | 79400    |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 507898   |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0248  |
| entropy                 | 2.5      |
| episodes                | 79500    |
| lives                   | 79500    |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 508601   |
| value_loss              | 7        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0169  |
| entropy                 | 2.52     |
| episodes                | 79600    |
| lives                   | 79600    |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 509307   |
| value_loss              | 7.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0179  |
| entropy                 | 2.52     |
| episodes                | 79700    |
| lives                   | 79700    |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 510047   |
| value_loss              | 7.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0161  |
| entropy                 | 2.53     |
| episodes                | 79800    |
| lives                   | 79800    |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 510795   |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0178  |
| entropy                 | 2.46     |
| episodes                | 79900    |
| lives                   | 79900    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 511461   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0224  |
| entropy                 | 2.52     |
| episodes                | 80000    |
| lives                   | 80000    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 512144   |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0091  |
| entropy                 | 2.58     |
| episodes                | 80100    |
| lives                   | 80100    |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 512871   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0107  |
| entropy                 | 2.56     |
| episodes                | 80200    |
| lives                   | 80200    |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 513592   |
| value_loss              | 7.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0098  |
| entropy                 | 2.53     |
| episodes                | 80300    |
| lives                   | 80300    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 514285   |
| value_loss              | 6.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0213  |
| entropy                 | 2.57     |
| episodes                | 80400    |
| lives                   | 80400    |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 515004   |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0195  |
| entropy                 | 2.6      |
| episodes                | 80500    |
| lives                   | 80500    |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 515736   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0381  |
| entropy                 | 2.52     |
| episodes                | 80600    |
| lives                   | 80600    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 516404   |
| value_loss              | 8.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0511  |
| entropy                 | 2.52     |
| episodes                | 80700    |
| lives                   | 80700    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 517056   |
| value_loss              | 7.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0452  |
| entropy                 | 2.56     |
| episodes                | 80800    |
| lives                   | 80800    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 517736   |
| value_loss              | 7.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0409  |
| entropy                 | 2.54     |
| episodes                | 80900    |
| lives                   | 80900    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 518401   |
| value_loss              | 8.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0389  |
| entropy                 | 2.5      |
| episodes                | 81000    |
| lives                   | 81000    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 519053   |
| value_loss              | 8.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0268  |
| entropy                 | 2.53     |
| episodes                | 81100    |
| lives                   | 81100    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 519744   |
| value_loss              | 7.27     |
--------------------------------------
Saving model due to running mean reward increase: 6.3529 -> 6.4095
--------------------------------------
| approx_kl               | -0.0399  |
| entropy                 | 2.45     |
| episodes                | 81200    |
| lives                   | 81200    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 520341   |
| value_loss              | 6.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0382  |
| entropy                 | 2.53     |
| episodes                | 81300    |
| lives                   | 81300    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 521001   |
| value_loss              | 8.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.52     |
| episodes                | 81400    |
| lives                   | 81400    |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0008   |
| steps                   | 521687   |
| value_loss              | 8.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.58     |
| episodes                | 81500    |
| lives                   | 81500    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 522396   |
| value_loss              | 8.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0253  |
| entropy                 | 2.51     |
| episodes                | 81600    |
| lives                   | 81600    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 523049   |
| value_loss              | 7.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0531  |
| entropy                 | 2.51     |
| episodes                | 81700    |
| lives                   | 81700    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 523684   |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.043   |
| entropy                 | 2.57     |
| episodes                | 81800    |
| lives                   | 81800    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 524348   |
| value_loss              | 7.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.025   |
| entropy                 | 2.61     |
| episodes                | 81900    |
| lives                   | 81900    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 525049   |
| value_loss              | 7.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0454  |
| entropy                 | 2.55     |
| episodes                | 82000    |
| lives                   | 82000    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 525717   |
| value_loss              | 8.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.029   |
| entropy                 | 2.52     |
| episodes                | 82100    |
| lives                   | 82100    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 526375   |
| value_loss              | 7.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0197  |
| entropy                 | 2.49     |
| episodes                | 82200    |
| lives                   | 82200    |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 527078   |
| value_loss              | 7.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.58     |
| episodes                | 82300    |
| lives                   | 82300    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 527755   |
| value_loss              | 7.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.5      |
| episodes                | 82400    |
| lives                   | 82400    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 528394   |
| value_loss              | 6.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.027   |
| entropy                 | 2.59     |
| episodes                | 82500    |
| lives                   | 82500    |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 529128   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0383  |
| entropy                 | 2.49     |
| episodes                | 82600    |
| lives                   | 82600    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 529804   |
| value_loss              | 7.65     |
--------------------------------------
Saving model due to running mean reward increase: 6.5086 -> 6.6976
--------------------------------------
| approx_kl               | -0.0224  |
| entropy                 | 2.49     |
| episodes                | 82700    |
| lives                   | 82700    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 530451   |
| value_loss              | 6.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.52     |
| episodes                | 82800    |
| lives                   | 82800    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 531142   |
| value_loss              | 7.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.014   |
| entropy                 | 2.5      |
| episodes                | 82900    |
| lives                   | 82900    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 531851   |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0206  |
| entropy                 | 2.51     |
| episodes                | 83000    |
| lives                   | 83000    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 532553   |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.51     |
| episodes                | 83100    |
| lives                   | 83100    |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 533266   |
| value_loss              | 6.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0273  |
| entropy                 | 2.48     |
| episodes                | 83200    |
| lives                   | 83200    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 533957   |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0262  |
| entropy                 | 2.48     |
| episodes                | 83300    |
| lives                   | 83300    |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 534679   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0343  |
| entropy                 | 2.48     |
| episodes                | 83400    |
| lives                   | 83400    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 535357   |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0432  |
| entropy                 | 2.47     |
| episodes                | 83500    |
| lives                   | 83500    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 535993   |
| value_loss              | 6.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.52     |
| episodes                | 83600    |
| lives                   | 83600    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 536655   |
| value_loss              | 7.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0443  |
| entropy                 | 2.5      |
| episodes                | 83700    |
| lives                   | 83700    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 537272   |
| value_loss              | 7.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0292  |
| entropy                 | 2.5      |
| episodes                | 83800    |
| lives                   | 83800    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 537928   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0371  |
| entropy                 | 2.51     |
| episodes                | 83900    |
| lives                   | 83900    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 538580   |
| value_loss              | 7.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.54     |
| episodes                | 84000    |
| lives                   | 84000    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 539253   |
| value_loss              | 7.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.51     |
| episodes                | 84100    |
| lives                   | 84100    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 539882   |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0561  |
| entropy                 | 2.48     |
| episodes                | 84200    |
| lives                   | 84200    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 540481   |
| value_loss              | 6.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0531  |
| entropy                 | 2.54     |
| episodes                | 84300    |
| lives                   | 84300    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 541104   |
| value_loss              | 6.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0429  |
| entropy                 | 2.52     |
| episodes                | 84400    |
| lives                   | 84400    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 541735   |
| value_loss              | 6.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0609  |
| entropy                 | 2.54     |
| episodes                | 84500    |
| lives                   | 84500    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 542389   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.5      |
| episodes                | 84600    |
| lives                   | 84600    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 543085   |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.49     |
| episodes                | 84700    |
| lives                   | 84700    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 543775   |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0268  |
| entropy                 | 2.5      |
| episodes                | 84800    |
| lives                   | 84800    |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 544479   |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0445  |
| entropy                 | 2.49     |
| episodes                | 84900    |
| lives                   | 84900    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 545152   |
| value_loss              | 6.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.52     |
| episodes                | 85000    |
| lives                   | 85000    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 545842   |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.073   |
| entropy                 | 2.47     |
| episodes                | 85100    |
| lives                   | 85100    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0086   |
| steps                   | 546497   |
| value_loss              | 6.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0425  |
| entropy                 | 2.46     |
| episodes                | 85200    |
| lives                   | 85200    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0205   |
| steps                   | 547123   |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0496  |
| entropy                 | 2.29     |
| episodes                | 85300    |
| lives                   | 85300    |
| mean 100 episode length | 6.44     |
| mean 100 episode reward | 4.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 547667   |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.43     |
| episodes                | 85400    |
| lives                   | 85400    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 548310   |
| value_loss              | 7.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.37     |
| episodes                | 85500    |
| lives                   | 85500    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 548907   |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0376  |
| entropy                 | 2.39     |
| episodes                | 85600    |
| lives                   | 85600    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 549529   |
| value_loss              | 6.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.37     |
| episodes                | 85700    |
| lives                   | 85700    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 550130   |
| value_loss              | 6.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0274  |
| entropy                 | 2.47     |
| episodes                | 85800    |
| lives                   | 85800    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 550785   |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0607  |
| entropy                 | 2.5      |
| episodes                | 85900    |
| lives                   | 85900    |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 551369   |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.044   |
| entropy                 | 2.44     |
| episodes                | 86000    |
| lives                   | 86000    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 551977   |
| value_loss              | 6.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0491  |
| entropy                 | 2.4      |
| episodes                | 86100    |
| lives                   | 86100    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 552591   |
| value_loss              | 7.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.46     |
| episodes                | 86200    |
| lives                   | 86200    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 553241   |
| value_loss              | 7.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0515  |
| entropy                 | 2.38     |
| episodes                | 86300    |
| lives                   | 86300    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 553893   |
| value_loss              | 7.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0455  |
| entropy                 | 2.45     |
| episodes                | 86400    |
| lives                   | 86400    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 554517   |
| value_loss              | 6.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.053   |
| entropy                 | 2.49     |
| episodes                | 86500    |
| lives                   | 86500    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 555126   |
| value_loss              | 7.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0516  |
| entropy                 | 2.45     |
| episodes                | 86600    |
| lives                   | 86600    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 555741   |
| value_loss              | 6.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0687  |
| entropy                 | 2.45     |
| episodes                | 86700    |
| lives                   | 86700    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 556337   |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.48     |
| episodes                | 86800    |
| lives                   | 86800    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 556982   |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.49     |
| episodes                | 86900    |
| lives                   | 86900    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 557664   |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0616  |
| entropy                 | 2.4      |
| episodes                | 87000    |
| lives                   | 87000    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 558278   |
| value_loss              | 6.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.59     |
| episodes                | 87100    |
| lives                   | 87100    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 558934   |
| value_loss              | 7.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.73     |
| episodes                | 87200    |
| lives                   | 87200    |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 559723   |
| value_loss              | 6.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0498  |
| entropy                 | 2.74     |
| episodes                | 87300    |
| lives                   | 87300    |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0072   |
| steps                   | 560453   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0315  |
| entropy                 | 2.64     |
| episodes                | 87400    |
| lives                   | 87400    |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 561204   |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0206  |
| entropy                 | 2.6      |
| episodes                | 87500    |
| lives                   | 87500    |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 561928   |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0487  |
| entropy                 | 2.59     |
| episodes                | 87600    |
| lives                   | 87600    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 562623   |
| value_loss              | 7.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0427  |
| entropy                 | 2.61     |
| episodes                | 87700    |
| lives                   | 87700    |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0072   |
| steps                   | 563344   |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.041   |
| entropy                 | 2.69     |
| episodes                | 87800    |
| lives                   | 87800    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0098   |
| steps                   | 563999   |
| value_loss              | 6.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0376  |
| entropy                 | 2.73     |
| episodes                | 87900    |
| lives                   | 87900    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0252   |
| steps                   | 564643   |
| value_loss              | 6.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0384  |
| entropy                 | 2.52     |
| episodes                | 88000    |
| lives                   | 88000    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 565276   |
| value_loss              | 6.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.54     |
| episodes                | 88100    |
| lives                   | 88100    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 565924   |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0226  |
| entropy                 | 2.5      |
| episodes                | 88200    |
| lives                   | 88200    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 566562   |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0122  |
| entropy                 | 2.6      |
| episodes                | 88300    |
| lives                   | 88300    |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 567293   |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0242  |
| entropy                 | 2.62     |
| episodes                | 88400    |
| lives                   | 88400    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 567980   |
| value_loss              | 6.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0182  |
| entropy                 | 2.59     |
| episodes                | 88500    |
| lives                   | 88500    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 568669   |
| value_loss              | 6.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.6      |
| episodes                | 88600    |
| lives                   | 88600    |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 569406   |
| value_loss              | 6.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.49     |
| episodes                | 88700    |
| lives                   | 88700    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 570071   |
| value_loss              | 6.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.55     |
| episodes                | 88800    |
| lives                   | 88800    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 570745   |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0248  |
| entropy                 | 2.58     |
| episodes                | 88900    |
| lives                   | 88900    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 571424   |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0172  |
| entropy                 | 2.62     |
| episodes                | 89000    |
| lives                   | 89000    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 572103   |
| value_loss              | 6.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0183  |
| entropy                 | 2.59     |
| episodes                | 89100    |
| lives                   | 89100    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 572782   |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0166  |
| entropy                 | 2.57     |
| episodes                | 89200    |
| lives                   | 89200    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 573446   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0329  |
| entropy                 | 2.57     |
| episodes                | 89300    |
| lives                   | 89300    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.02    |
| steps                   | 574120   |
| value_loss              | 7.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.61     |
| episodes                | 89400    |
| lives                   | 89400    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 574752   |
| value_loss              | 6.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0092  |
| entropy                 | 2.59     |
| episodes                | 89500    |
| lives                   | 89500    |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 575461   |
| value_loss              | 6.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.58     |
| episodes                | 89600    |
| lives                   | 89600    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 576155   |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0177  |
| entropy                 | 2.64     |
| episodes                | 89700    |
| lives                   | 89700    |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 576881   |
| value_loss              | 6.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0192  |
| entropy                 | 2.58     |
| episodes                | 89800    |
| lives                   | 89800    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 577548   |
| value_loss              | 6.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.55     |
| episodes                | 89900    |
| lives                   | 89900    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 578202   |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0182  |
| entropy                 | 2.55     |
| episodes                | 90000    |
| lives                   | 90000    |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 578937   |
| value_loss              | 6.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0389  |
| entropy                 | 2.45     |
| episodes                | 90100    |
| lives                   | 90100    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 579541   |
| value_loss              | 6.14     |
--------------------------------------
Saving model due to running mean reward increase: 5.8462 -> 6.135
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.6      |
| episodes                | 90200    |
| lives                   | 90200    |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 580246   |
| value_loss              | 6.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.024   |
| entropy                 | 2.65     |
| episodes                | 90300    |
| lives                   | 90300    |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 580996   |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.61     |
| episodes                | 90400    |
| lives                   | 90400    |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 581700   |
| value_loss              | 6.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.6      |
| episodes                | 90500    |
| lives                   | 90500    |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 582413   |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0203  |
| entropy                 | 2.5      |
| episodes                | 90600    |
| lives                   | 90600    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 583052   |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.013   |
| entropy                 | 2.51     |
| episodes                | 90700    |
| lives                   | 90700    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0241  |
| steps                   | 583715   |
| value_loss              | 6.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0414  |
| entropy                 | 2.51     |
| episodes                | 90800    |
| lives                   | 90800    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0002   |
| steps                   | 584376   |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.67     |
| episodes                | 90900    |
| lives                   | 90900    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 585066   |
| value_loss              | 6.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0105  |
| entropy                 | 2.58     |
| episodes                | 91000    |
| lives                   | 91000    |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 585770   |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0279  |
| entropy                 | 2.49     |
| episodes                | 91100    |
| lives                   | 91100    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 586401   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0772  |
| entropy                 | 2.62     |
| episodes                | 91200    |
| lives                   | 91200    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0044   |
| steps                   | 587108   |
| value_loss              | 6.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0419  |
| entropy                 | 2.53     |
| episodes                | 91300    |
| lives                   | 91300    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0223   |
| steps                   | 587714   |
| value_loss              | 6.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0211  |
| entropy                 | 2.37     |
| episodes                | 91400    |
| lives                   | 91400    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 4.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 588316   |
| value_loss              | 6.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.62     |
| episodes                | 91500    |
| lives                   | 91500    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 589005   |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.58     |
| episodes                | 91600    |
| lives                   | 91600    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 589684   |
| value_loss              | 6.29     |
--------------------------------------
Saving model due to running mean reward increase: 5.9133 -> 6.3225
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.53     |
| episodes                | 91700    |
| lives                   | 91700    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 590372   |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0155  |
| entropy                 | 2.56     |
| episodes                | 91800    |
| lives                   | 91800    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 591028   |
| value_loss              | 6.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0249  |
| entropy                 | 2.56     |
| episodes                | 91900    |
| lives                   | 91900    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 591694   |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0125  |
| entropy                 | 2.56     |
| episodes                | 92000    |
| lives                   | 92000    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 592389   |
| value_loss              | 6.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0214  |
| entropy                 | 2.55     |
| episodes                | 92100    |
| lives                   | 92100    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 593067   |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0381  |
| entropy                 | 2.58     |
| episodes                | 92200    |
| lives                   | 92200    |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 593779   |
| value_loss              | 6.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0717  |
| entropy                 | 2.57     |
| episodes                | 92300    |
| lives                   | 92300    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0       |
| steps                   | 594462   |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.57     |
| episodes                | 92400    |
| lives                   | 92400    |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 595160   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0402  |
| entropy                 | 2.56     |
| episodes                | 92500    |
| lives                   | 92500    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 595844   |
| value_loss              | 6.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0311  |
| entropy                 | 2.58     |
| episodes                | 92600    |
| lives                   | 92600    |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 596556   |
| value_loss              | 6.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.62     |
| episodes                | 92700    |
| lives                   | 92700    |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 597254   |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.172   |
| entropy                 | 2.45     |
| episodes                | 92800    |
| lives                   | 92800    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0672   |
| steps                   | 597868   |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.42     |
| episodes                | 92900    |
| lives                   | 92900    |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0155   |
| steps                   | 598437   |
| value_loss              | 6.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0257  |
| entropy                 | 2.45     |
| episodes                | 93000    |
| lives                   | 93000    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 599081   |
| value_loss              | 6.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0312  |
| entropy                 | 2.44     |
| episodes                | 93100    |
| lives                   | 93100    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 599689   |
| value_loss              | 6.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.49     |
| episodes                | 93200    |
| lives                   | 93200    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 600346   |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.038   |
| entropy                 | 2.54     |
| episodes                | 93300    |
| lives                   | 93300    |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 5.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 601044   |
| value_loss              | 5.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.024   |
| entropy                 | 2.53     |
| episodes                | 93400    |
| lives                   | 93400    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 601734   |
| value_loss              | 5.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.47     |
| episodes                | 93500    |
| lives                   | 93500    |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 602432   |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0117  |
| entropy                 | 2.46     |
| episodes                | 93600    |
| lives                   | 93600    |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 603149   |
| value_loss              | 5.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0145  |
| entropy                 | 2.46     |
| episodes                | 93700    |
| lives                   | 93700    |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 603849   |
| value_loss              | 6.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0212  |
| entropy                 | 2.46     |
| episodes                | 93800    |
| lives                   | 93800    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 604529   |
| value_loss              | 6.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0196  |
| entropy                 | 2.36     |
| episodes                | 93900    |
| lives                   | 93900    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 605179   |
| value_loss              | 5.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0197  |
| entropy                 | 2.43     |
| episodes                | 94000    |
| lives                   | 94000    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0189  |
| steps                   | 605846   |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0153  |
| entropy                 | 2.58     |
| episodes                | 94100    |
| lives                   | 94100    |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 606655   |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0321  |
| entropy                 | 2.43     |
| episodes                | 94200    |
| lives                   | 94200    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 607352   |
| value_loss              | 6.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0249  |
| entropy                 | 2.41     |
| episodes                | 94300    |
| lives                   | 94300    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 608045   |
| value_loss              | 5.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.49     |
| episodes                | 94400    |
| lives                   | 94400    |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 608756   |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0162  |
| entropy                 | 2.49     |
| episodes                | 94500    |
| lives                   | 94500    |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 609498   |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.46     |
| episodes                | 94600    |
| lives                   | 94600    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0049   |
| steps                   | 610146   |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0186  |
| entropy                 | 2.33     |
| episodes                | 94700    |
| lives                   | 94700    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 610737   |
| value_loss              | 6.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0388  |
| entropy                 | 2.42     |
| episodes                | 94800    |
| lives                   | 94800    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 611388   |
| value_loss              | 6.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0459  |
| entropy                 | 2.45     |
| episodes                | 94900    |
| lives                   | 94900    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 612033   |
| value_loss              | 5.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0559  |
| entropy                 | 2.42     |
| episodes                | 95000    |
| lives                   | 95000    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 612650   |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0267  |
| entropy                 | 2.46     |
| episodes                | 95100    |
| lives                   | 95100    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 613338   |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0481  |
| entropy                 | 2.42     |
| episodes                | 95200    |
| lives                   | 95200    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 613963   |
| value_loss              | 6.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0608  |
| entropy                 | 2.49     |
| episodes                | 95300    |
| lives                   | 95300    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0106   |
| steps                   | 614621   |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0235  |
| entropy                 | 2.56     |
| episodes                | 95400    |
| lives                   | 95400    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0031   |
| steps                   | 615300   |
| value_loss              | 6.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.52     |
| episodes                | 95500    |
| lives                   | 95500    |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 616006   |
| value_loss              | 6.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.5      |
| episodes                | 95600    |
| lives                   | 95600    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 616667   |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.59     |
| episodes                | 95700    |
| lives                   | 95700    |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 617394   |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0149  |
| entropy                 | 2.57     |
| episodes                | 95800    |
| lives                   | 95800    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 618095   |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0206  |
| entropy                 | 2.65     |
| episodes                | 95900    |
| lives                   | 95900    |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 618840   |
| value_loss              | 6.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.015   |
| entropy                 | 2.59     |
| episodes                | 96000    |
| lives                   | 96000    |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 619564   |
| value_loss              | 5.61     |
--------------------------------------
Saving model due to running mean reward increase: 5.9149 -> 6.1412
--------------------------------------
| approx_kl               | -0.0412  |
| entropy                 | 2.59     |
| episodes                | 96100    |
| lives                   | 96100    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 620266   |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0175  |
| entropy                 | 2.57     |
| episodes                | 96200    |
| lives                   | 96200    |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 621021   |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0207  |
| entropy                 | 2.55     |
| episodes                | 96300    |
| lives                   | 96300    |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 621748   |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0296  |
| entropy                 | 2.58     |
| episodes                | 96400    |
| lives                   | 96400    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 622463   |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0254  |
| entropy                 | 2.51     |
| episodes                | 96500    |
| lives                   | 96500    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 623154   |
| value_loss              | 5.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0231  |
| entropy                 | 2.59     |
| episodes                | 96600    |
| lives                   | 96600    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 623856   |
| value_loss              | 5.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.5      |
| episodes                | 96700    |
| lives                   | 96700    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 624475   |
| value_loss              | 5.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0187  |
| entropy                 | 2.38     |
| episodes                | 96800    |
| lives                   | 96800    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 625066   |
| value_loss              | 5.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0122  |
| entropy                 | 2.39     |
| episodes                | 96900    |
| lives                   | 96900    |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 625623   |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0371  |
| entropy                 | 2.5      |
| episodes                | 97000    |
| lives                   | 97000    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 626248   |
| value_loss              | 5.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.031   |
| entropy                 | 2.51     |
| episodes                | 97100    |
| lives                   | 97100    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 626885   |
| value_loss              | 5.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.57     |
| episodes                | 97200    |
| lives                   | 97200    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 627543   |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0553  |
| entropy                 | 2.55     |
| episodes                | 97300    |
| lives                   | 97300    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 628181   |
| value_loss              | 6.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.042   |
| entropy                 | 2.6      |
| episodes                | 97400    |
| lives                   | 97400    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 628848   |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.63     |
| episodes                | 97500    |
| lives                   | 97500    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 629518   |
| value_loss              | 5.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0405  |
| entropy                 | 2.57     |
| episodes                | 97600    |
| lives                   | 97600    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 630183   |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0309  |
| entropy                 | 2.46     |
| episodes                | 97700    |
| lives                   | 97700    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 630781   |
| value_loss              | 5.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.47     |
| episodes                | 97800    |
| lives                   | 97800    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 631398   |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.47     |
| episodes                | 97900    |
| lives                   | 97900    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 632004   |
| value_loss              | 4.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.57     |
| episodes                | 98000    |
| lives                   | 98000    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 632673   |
| value_loss              | 5.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0454  |
| entropy                 | 2.54     |
| episodes                | 98100    |
| lives                   | 98100    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0185   |
| steps                   | 633352   |
| value_loss              | 6.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0211  |
| entropy                 | 2.55     |
| episodes                | 98200    |
| lives                   | 98200    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 633949   |
| value_loss              | 5.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.65     |
| episodes                | 98300    |
| lives                   | 98300    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 634566   |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0462  |
| entropy                 | 2.63     |
| episodes                | 98400    |
| lives                   | 98400    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 635219   |
| value_loss              | 5.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0321  |
| entropy                 | 2.59     |
| episodes                | 98500    |
| lives                   | 98500    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0057   |
| steps                   | 635902   |
| value_loss              | 5.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.019   |
| entropy                 | 2.57     |
| episodes                | 98600    |
| lives                   | 98600    |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 636592   |
| value_loss              | 6.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.032   |
| entropy                 | 2.55     |
| episodes                | 98700    |
| lives                   | 98700    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0204  |
| steps                   | 637218   |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.63     |
| episodes                | 98800    |
| lives                   | 98800    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 637933   |
| value_loss              | 5.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0205  |
| entropy                 | 2.61     |
| episodes                | 98900    |
| lives                   | 98900    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 638597   |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0403  |
| entropy                 | 2.57     |
| episodes                | 99000    |
| lives                   | 99000    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 639269   |
| value_loss              | 5.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0254  |
| entropy                 | 2.57     |
| episodes                | 99100    |
| lives                   | 99100    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 639893   |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0121  |
| entropy                 | 2.58     |
| episodes                | 99200    |
| lives                   | 99200    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 640552   |
| value_loss              | 6.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0118  |
| entropy                 | 2.63     |
| episodes                | 99300    |
| lives                   | 99300    |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 641256   |
| value_loss              | 6.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0221  |
| entropy                 | 2.62     |
| episodes                | 99400    |
| lives                   | 99400    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 5.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 641943   |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.019   |
| entropy                 | 2.63     |
| episodes                | 99500    |
| lives                   | 99500    |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 642689   |
| value_loss              | 6.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.57     |
| episodes                | 99600    |
| lives                   | 99600    |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 643432   |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.52     |
| episodes                | 99700    |
| lives                   | 99700    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 644105   |
| value_loss              | 5.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0139  |
| entropy                 | 2.57     |
| episodes                | 99800    |
| lives                   | 99800    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 644812   |
| value_loss              | 5.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0169  |
| entropy                 | 2.56     |
| episodes                | 99900    |
| lives                   | 99900    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 645471   |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0144  |
| entropy                 | 2.54     |
| episodes                | 100000   |
| lives                   | 100000   |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 646115   |
| value_loss              | 5.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0253  |
| entropy                 | 2.58     |
| episodes                | 100100   |
| lives                   | 100100   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 646754   |
| value_loss              | 5.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0317  |
| entropy                 | 2.57     |
| episodes                | 100200   |
| lives                   | 100200   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 647433   |
| value_loss              | 6.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.61     |
| episodes                | 100300   |
| lives                   | 100300   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 648145   |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.53     |
| episodes                | 100400   |
| lives                   | 100400   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 648835   |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.018   |
| entropy                 | 2.59     |
| episodes                | 100500   |
| lives                   | 100500   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 649526   |
| value_loss              | 6.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0446  |
| entropy                 | 2.62     |
| episodes                | 100600   |
| lives                   | 100600   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.002    |
| steps                   | 650240   |
| value_loss              | 6.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.029   |
| entropy                 | 2.52     |
| episodes                | 100700   |
| lives                   | 100700   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 650945   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0365  |
| entropy                 | 2.6      |
| episodes                | 100800   |
| lives                   | 100800   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0062   |
| steps                   | 651686   |
| value_loss              | 6.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0165  |
| entropy                 | 2.59     |
| episodes                | 100900   |
| lives                   | 100900   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0163  |
| steps                   | 652410   |
| value_loss              | 5.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.018   |
| entropy                 | 2.59     |
| episodes                | 101000   |
| lives                   | 101000   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 653142   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0453  |
| entropy                 | 2.55     |
| episodes                | 101100   |
| lives                   | 101100   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0052   |
| steps                   | 653848   |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.31     |
| episodes                | 101200   |
| lives                   | 101200   |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0047   |
| steps                   | 654375   |
| value_loss              | 6.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.45     |
| episodes                | 101300   |
| lives                   | 101300   |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 654967   |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0414  |
| entropy                 | 2.5      |
| episodes                | 101400   |
| lives                   | 101400   |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 4.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 655560   |
| value_loss              | 6.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0659  |
| entropy                 | 2.6      |
| episodes                | 101500   |
| lives                   | 101500   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 656165   |
| value_loss              | 6.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0529  |
| entropy                 | 2.55     |
| episodes                | 101600   |
| lives                   | 101600   |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 656808   |
| value_loss              | 7.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0613  |
| entropy                 | 2.64     |
| episodes                | 101700   |
| lives                   | 101700   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 657476   |
| value_loss              | 7        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0478  |
| entropy                 | 2.64     |
| episodes                | 101800   |
| lives                   | 101800   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 658147   |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0798  |
| entropy                 | 2.65     |
| episodes                | 101900   |
| lives                   | 101900   |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 658797   |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0629  |
| entropy                 | 2.64     |
| episodes                | 102000   |
| lives                   | 102000   |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 659447   |
| value_loss              | 7.11     |
--------------------------------------
Saving model due to running mean reward increase: 5.6421 -> 6.1004
--------------------------------------
| approx_kl               | -0.0502  |
| entropy                 | 2.72     |
| episodes                | 102100   |
| lives                   | 102100   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0245   |
| steps                   | 660148   |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0354  |
| entropy                 | 2.6      |
| episodes                | 102200   |
| lives                   | 102200   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0019   |
| steps                   | 660836   |
| value_loss              | 7.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0243  |
| entropy                 | 2.58     |
| episodes                | 102300   |
| lives                   | 102300   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 661549   |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.54     |
| episodes                | 102400   |
| lives                   | 102400   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 662238   |
| value_loss              | 7.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.6      |
| episodes                | 102500   |
| lives                   | 102500   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 663009   |
| value_loss              | 7.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0372  |
| entropy                 | 2.58     |
| episodes                | 102600   |
| lives                   | 102600   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 663698   |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.6      |
| episodes                | 102700   |
| lives                   | 102700   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 664395   |
| value_loss              | 6.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0149  |
| entropy                 | 2.48     |
| episodes                | 102800   |
| lives                   | 102800   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 665062   |
| value_loss              | 7.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.5      |
| episodes                | 102900   |
| lives                   | 102900   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0181  |
| steps                   | 665733   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.51     |
| episodes                | 103000   |
| lives                   | 103000   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 666428   |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0159  |
| entropy                 | 2.54     |
| episodes                | 103100   |
| lives                   | 103100   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 7.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 667190   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.49     |
| episodes                | 103200   |
| lives                   | 103200   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 667912   |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.49     |
| episodes                | 103300   |
| lives                   | 103300   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 668634   |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.5      |
| episodes                | 103400   |
| lives                   | 103400   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 669357   |
| value_loss              | 6.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0361  |
| entropy                 | 2.53     |
| episodes                | 103500   |
| lives                   | 103500   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 670101   |
| value_loss              | 7.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0253  |
| entropy                 | 2.55     |
| episodes                | 103600   |
| lives                   | 103600   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 670819   |
| value_loss              | 6.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.53     |
| episodes                | 103700   |
| lives                   | 103700   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 671528   |
| value_loss              | 6.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.61     |
| episodes                | 103800   |
| lives                   | 103800   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 672269   |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0285  |
| entropy                 | 2.57     |
| episodes                | 103900   |
| lives                   | 103900   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 672957   |
| value_loss              | 7.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0348  |
| entropy                 | 2.66     |
| episodes                | 104000   |
| lives                   | 104000   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 673710   |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0389  |
| entropy                 | 2.62     |
| episodes                | 104100   |
| lives                   | 104100   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 674409   |
| value_loss              | 7.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0541  |
| entropy                 | 2.63     |
| episodes                | 104200   |
| lives                   | 104200   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 675148   |
| value_loss              | 6.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.038   |
| entropy                 | 2.54     |
| episodes                | 104300   |
| lives                   | 104300   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 675833   |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0232  |
| entropy                 | 2.47     |
| episodes                | 104400   |
| lives                   | 104400   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 676518   |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.5      |
| episodes                | 104500   |
| lives                   | 104500   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 677235   |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.025   |
| entropy                 | 2.51     |
| episodes                | 104600   |
| lives                   | 104600   |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 677909   |
| value_loss              | 6.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0286  |
| entropy                 | 2.58     |
| episodes                | 104700   |
| lives                   | 104700   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 678635   |
| value_loss              | 6.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.59     |
| episodes                | 104800   |
| lives                   | 104800   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 679355   |
| value_loss              | 6.44     |
--------------------------------------
Saving model due to running mean reward increase: 5.8532 -> 6.2958
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.57     |
| episodes                | 104900   |
| lives                   | 104900   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 680042   |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0404  |
| entropy                 | 2.61     |
| episodes                | 105000   |
| lives                   | 105000   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 680771   |
| value_loss              | 6.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0356  |
| entropy                 | 2.53     |
| episodes                | 105100   |
| lives                   | 105100   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 681469   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0356  |
| entropy                 | 2.63     |
| episodes                | 105200   |
| lives                   | 105200   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 682201   |
| value_loss              | 6.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0355  |
| entropy                 | 2.58     |
| episodes                | 105300   |
| lives                   | 105300   |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 682883   |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.018   |
| entropy                 | 2.6      |
| episodes                | 105400   |
| lives                   | 105400   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 683601   |
| value_loss              | 7.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0299  |
| entropy                 | 2.67     |
| episodes                | 105500   |
| lives                   | 105500   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 684365   |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0242  |
| entropy                 | 2.71     |
| episodes                | 105600   |
| lives                   | 105600   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 685147   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.63     |
| episodes                | 105700   |
| lives                   | 105700   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0007   |
| steps                   | 685934   |
| value_loss              | 6.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.012   |
| entropy                 | 2.66     |
| episodes                | 105800   |
| lives                   | 105800   |
| mean 100 episode length | 9.16     |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 686750   |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.59     |
| episodes                | 105900   |
| lives                   | 105900   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0186   |
| steps                   | 687448   |
| value_loss              | 6.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0656  |
| entropy                 | 2.52     |
| episodes                | 106000   |
| lives                   | 106000   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0138   |
| steps                   | 688087   |
| value_loss              | 6.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0367  |
| entropy                 | 2.57     |
| episodes                | 106100   |
| lives                   | 106100   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 688738   |
| value_loss              | 6.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0685  |
| entropy                 | 2.54     |
| episodes                | 106200   |
| lives                   | 106200   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 689402   |
| value_loss              | 7.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0781  |
| entropy                 | 2.53     |
| episodes                | 106300   |
| lives                   | 106300   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0185   |
| steps                   | 690007   |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.024   |
| entropy                 | 2.76     |
| episodes                | 106400   |
| lives                   | 106400   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 690714   |
| value_loss              | 6.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.71     |
| episodes                | 106500   |
| lives                   | 106500   |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 691350   |
| value_loss              | 6.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0346  |
| entropy                 | 2.71     |
| episodes                | 106600   |
| lives                   | 106600   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 692081   |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0319  |
| entropy                 | 2.72     |
| episodes                | 106700   |
| lives                   | 106700   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 692791   |
| value_loss              | 6.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0361  |
| entropy                 | 2.7      |
| episodes                | 106800   |
| lives                   | 106800   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 693470   |
| value_loss              | 6.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.64     |
| episodes                | 106900   |
| lives                   | 106900   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 694164   |
| value_loss              | 6.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0484  |
| entropy                 | 2.74     |
| episodes                | 107000   |
| lives                   | 107000   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 694837   |
| value_loss              | 6.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0174  |
| entropy                 | 2.68     |
| episodes                | 107100   |
| lives                   | 107100   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 695515   |
| value_loss              | 5.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.017   |
| entropy                 | 2.64     |
| episodes                | 107200   |
| lives                   | 107200   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 696199   |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0169  |
| entropy                 | 2.65     |
| episodes                | 107300   |
| lives                   | 107300   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 696934   |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0177  |
| entropy                 | 2.66     |
| episodes                | 107400   |
| lives                   | 107400   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 697629   |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0177  |
| entropy                 | 2.66     |
| episodes                | 107500   |
| lives                   | 107500   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 698355   |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0274  |
| entropy                 | 2.6      |
| episodes                | 107600   |
| lives                   | 107600   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 699031   |
| value_loss              | 6.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0315  |
| entropy                 | 2.58     |
| episodes                | 107700   |
| lives                   | 107700   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 699686   |
| value_loss              | 5.94     |
--------------------------------------
Saving model due to running mean reward increase: 5.6271 -> 6.6436
--------------------------------------
| approx_kl               | -0.0267  |
| entropy                 | 2.65     |
| episodes                | 107800   |
| lives                   | 107800   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.002    |
| steps                   | 700372   |
| value_loss              | 6.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0235  |
| entropy                 | 2.64     |
| episodes                | 107900   |
| lives                   | 107900   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 701036   |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0268  |
| entropy                 | 2.57     |
| episodes                | 108000   |
| lives                   | 108000   |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 701686   |
| value_loss              | 7.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0267  |
| entropy                 | 2.62     |
| episodes                | 108100   |
| lives                   | 108100   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 702393   |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.63     |
| episodes                | 108200   |
| lives                   | 108200   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 703093   |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0199  |
| entropy                 | 2.67     |
| episodes                | 108300   |
| lives                   | 108300   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 703771   |
| value_loss              | 6.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0134  |
| entropy                 | 2.67     |
| episodes                | 108400   |
| lives                   | 108400   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0187  |
| steps                   | 704444   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0107  |
| entropy                 | 2.72     |
| episodes                | 108500   |
| lives                   | 108500   |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 705090   |
| value_loss              | 6.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0228  |
| entropy                 | 2.78     |
| episodes                | 108600   |
| lives                   | 108600   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 705827   |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0211  |
| entropy                 | 2.75     |
| episodes                | 108700   |
| lives                   | 108700   |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 706499   |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.7      |
| episodes                | 108800   |
| lives                   | 108800   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 707224   |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0147  |
| entropy                 | 2.73     |
| episodes                | 108900   |
| lives                   | 108900   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 707938   |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0346  |
| entropy                 | 2.71     |
| episodes                | 109000   |
| lives                   | 109000   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 708663   |
| value_loss              | 6.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0672  |
| entropy                 | 2.66     |
| episodes                | 109100   |
| lives                   | 109100   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 7.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.03     |
| steps                   | 709385   |
| value_loss              | 7.45     |
--------------------------------------
Saving model due to mean reward increase: 6.8561 -> 6.883
--------------------------------------
| approx_kl               | -0.0305  |
| entropy                 | 2.58     |
| episodes                | 109200   |
| lives                   | 109200   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 710096   |
| value_loss              | 7.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0343  |
| entropy                 | 2.54     |
| episodes                | 109300   |
| lives                   | 109300   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 710787   |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.5      |
| episodes                | 109400   |
| lives                   | 109400   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 711458   |
| value_loss              | 8.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.029   |
| entropy                 | 2.58     |
| episodes                | 109500   |
| lives                   | 109500   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 7.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 712184   |
| value_loss              | 7.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.62     |
| episodes                | 109600   |
| lives                   | 109600   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0065   |
| steps                   | 712881   |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0092  |
| entropy                 | 2.73     |
| episodes                | 109700   |
| lives                   | 109700   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 713614   |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0191  |
| entropy                 | 2.69     |
| episodes                | 109800   |
| lives                   | 109800   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 714366   |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.65     |
| episodes                | 109900   |
| lives                   | 109900   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0005   |
| steps                   | 715074   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0205  |
| entropy                 | 2.62     |
| episodes                | 110000   |
| lives                   | 110000   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 715769   |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.61     |
| episodes                | 110100   |
| lives                   | 110100   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 716497   |
| value_loss              | 6.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0382  |
| entropy                 | 2.45     |
| episodes                | 110200   |
| lives                   | 110200   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0067   |
| steps                   | 717087   |
| value_loss              | 7.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0388  |
| entropy                 | 2.57     |
| episodes                | 110300   |
| lives                   | 110300   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 717699   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0373  |
| entropy                 | 2.66     |
| episodes                | 110400   |
| lives                   | 110400   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 718409   |
| value_loss              | 8.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.018   |
| entropy                 | 2.58     |
| episodes                | 110500   |
| lives                   | 110500   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 719141   |
| value_loss              | 8.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0124  |
| entropy                 | 2.55     |
| episodes                | 110600   |
| lives                   | 110600   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 719835   |
| value_loss              | 7.58     |
--------------------------------------
Saving model due to running mean reward increase: 6.7125 -> 6.8295
--------------------------------------
| approx_kl               | -0.0208  |
| entropy                 | 2.48     |
| episodes                | 110700   |
| lives                   | 110700   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 7.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 720556   |
| value_loss              | 8.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0131  |
| entropy                 | 2.5      |
| episodes                | 110800   |
| lives                   | 110800   |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 721214   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0594  |
| entropy                 | 2.43     |
| episodes                | 110900   |
| lives                   | 110900   |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0049   |
| steps                   | 721825   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0403  |
| entropy                 | 2.45     |
| episodes                | 111000   |
| lives                   | 111000   |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 722418   |
| value_loss              | 7.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0342  |
| entropy                 | 2.44     |
| episodes                | 111100   |
| lives                   | 111100   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 723017   |
| value_loss              | 7.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.45     |
| episodes                | 111200   |
| lives                   | 111200   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 723654   |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0351  |
| entropy                 | 2.44     |
| episodes                | 111300   |
| lives                   | 111300   |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 724276   |
| value_loss              | 7.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0501  |
| entropy                 | 2.5      |
| episodes                | 111400   |
| lives                   | 111400   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 724889   |
| value_loss              | 7.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0463  |
| entropy                 | 2.54     |
| episodes                | 111500   |
| lives                   | 111500   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0163  |
| steps                   | 725527   |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0461  |
| entropy                 | 2.58     |
| episodes                | 111600   |
| lives                   | 111600   |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 726180   |
| value_loss              | 7.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0523  |
| entropy                 | 2.48     |
| episodes                | 111700   |
| lives                   | 111700   |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 726788   |
| value_loss              | 7.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0458  |
| entropy                 | 2.49     |
| episodes                | 111800   |
| lives                   | 111800   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 727425   |
| value_loss              | 7.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.44     |
| episodes                | 111900   |
| lives                   | 111900   |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0008   |
| steps                   | 728016   |
| value_loss              | 8.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.081   |
| entropy                 | 2.65     |
| episodes                | 112000   |
| lives                   | 112000   |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.003    |
| steps                   | 728691   |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0709  |
| entropy                 | 2.62     |
| episodes                | 112100   |
| lives                   | 112100   |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 729354   |
| value_loss              | 7.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0716  |
| entropy                 | 2.6      |
| episodes                | 112200   |
| lives                   | 112200   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 729966   |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0755  |
| entropy                 | 2.5      |
| episodes                | 112300   |
| lives                   | 112300   |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 730567   |
| value_loss              | 7.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.056   |
| entropy                 | 2.55     |
| episodes                | 112400   |
| lives                   | 112400   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 731204   |
| value_loss              | 7.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0721  |
| entropy                 | 2.47     |
| episodes                | 112500   |
| lives                   | 112500   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0051   |
| steps                   | 731852   |
| value_loss              | 7.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.049   |
| entropy                 | 2.53     |
| episodes                | 112600   |
| lives                   | 112600   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 732528   |
| value_loss              | 8.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0659  |
| entropy                 | 2.58     |
| episodes                | 112700   |
| lives                   | 112700   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 733167   |
| value_loss              | 8.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0351  |
| entropy                 | 2.54     |
| episodes                | 112800   |
| lives                   | 112800   |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 733810   |
| value_loss              | 7.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0692  |
| entropy                 | 2.45     |
| episodes                | 112900   |
| lives                   | 112900   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 734398   |
| value_loss              | 7.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0683  |
| entropy                 | 2.53     |
| episodes                | 113000   |
| lives                   | 113000   |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 735028   |
| value_loss              | 8.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.064   |
| entropy                 | 2.55     |
| episodes                | 113100   |
| lives                   | 113100   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 735659   |
| value_loss              | 8.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.56     |
| episodes                | 113200   |
| lives                   | 113200   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0023   |
| steps                   | 736319   |
| value_loss              | 8.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0562  |
| entropy                 | 2.54     |
| episodes                | 113300   |
| lives                   | 113300   |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 736975   |
| value_loss              | 8.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0683  |
| entropy                 | 2.61     |
| episodes                | 113400   |
| lives                   | 113400   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 737624   |
| value_loss              | 8.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0501  |
| entropy                 | 2.65     |
| episodes                | 113500   |
| lives                   | 113500   |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 738264   |
| value_loss              | 9.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0563  |
| entropy                 | 2.79     |
| episodes                | 113600   |
| lives                   | 113600   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0143   |
| steps                   | 738954   |
| value_loss              | 8.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0412  |
| entropy                 | 2.68     |
| episodes                | 113700   |
| lives                   | 113700   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0042   |
| steps                   | 739634   |
| value_loss              | 8.45     |
--------------------------------------
Saving model due to running mean reward increase: 5.8788 -> 6.1375
--------------------------------------
| approx_kl               | -0.0317  |
| entropy                 | 2.72     |
| episodes                | 113800   |
| lives                   | 113800   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0001   |
| steps                   | 740381   |
| value_loss              | 8.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0146  |
| entropy                 | 2.64     |
| episodes                | 113900   |
| lives                   | 113900   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 741073   |
| value_loss              | 8.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.019   |
| entropy                 | 2.63     |
| episodes                | 114000   |
| lives                   | 114000   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 741732   |
| value_loss              | 8.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0228  |
| entropy                 | 2.67     |
| episodes                | 114100   |
| lives                   | 114100   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 7.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 742505   |
| value_loss              | 7.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0287  |
| entropy                 | 2.61     |
| episodes                | 114200   |
| lives                   | 114200   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 743209   |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.59     |
| episodes                | 114300   |
| lives                   | 114300   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 743910   |
| value_loss              | 8.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.62     |
| episodes                | 114400   |
| lives                   | 114400   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 744627   |
| value_loss              | 8.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.014   |
| entropy                 | 2.57     |
| episodes                | 114500   |
| lives                   | 114500   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 7.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 745361   |
| value_loss              | 8.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0235  |
| entropy                 | 2.57     |
| episodes                | 114600   |
| lives                   | 114600   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 7.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 746077   |
| value_loss              | 9.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0067  |
| entropy                 | 2.63     |
| episodes                | 114700   |
| lives                   | 114700   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 7.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 746809   |
| value_loss              | 9.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.63     |
| episodes                | 114800   |
| lives                   | 114800   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 747522   |
| value_loss              | 8.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.65     |
| episodes                | 114900   |
| lives                   | 114900   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 748260   |
| value_loss              | 8.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0218  |
| entropy                 | 2.65     |
| episodes                | 115000   |
| lives                   | 115000   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 749007   |
| value_loss              | 8.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0163  |
| entropy                 | 2.64     |
| episodes                | 115100   |
| lives                   | 115100   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 749721   |
| value_loss              | 7.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.59     |
| episodes                | 115200   |
| lives                   | 115200   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 750409   |
| value_loss              | 8.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0095  |
| entropy                 | 2.57     |
| episodes                | 115300   |
| lives                   | 115300   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 751110   |
| value_loss              | 7.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0443  |
| entropy                 | 2.57     |
| episodes                | 115400   |
| lives                   | 115400   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 751822   |
| value_loss              | 8.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0092  |
| entropy                 | 2.57     |
| episodes                | 115500   |
| lives                   | 115500   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 752533   |
| value_loss              | 8.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0247  |
| entropy                 | 2.61     |
| episodes                | 115600   |
| lives                   | 115600   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.006    |
| steps                   | 753232   |
| value_loss              | 8.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0236  |
| entropy                 | 2.68     |
| episodes                | 115700   |
| lives                   | 115700   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 753962   |
| value_loss              | 7.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0232  |
| entropy                 | 2.61     |
| episodes                | 115800   |
| lives                   | 115800   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 754668   |
| value_loss              | 7.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0128  |
| entropy                 | 2.61     |
| episodes                | 115900   |
| lives                   | 115900   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 755363   |
| value_loss              | 7.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.6      |
| episodes                | 116000   |
| lives                   | 116000   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 756056   |
| value_loss              | 7.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.67     |
| episodes                | 116100   |
| lives                   | 116100   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 756776   |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.66     |
| episodes                | 116200   |
| lives                   | 116200   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 757465   |
| value_loss              | 7.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0219  |
| entropy                 | 2.61     |
| episodes                | 116300   |
| lives                   | 116300   |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 758106   |
| value_loss              | 7.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.68     |
| episodes                | 116400   |
| lives                   | 116400   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 758829   |
| value_loss              | 7.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.64     |
| episodes                | 116500   |
| lives                   | 116500   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 759532   |
| value_loss              | 8.46     |
--------------------------------------
Saving model due to running mean reward increase: 6.5526 -> 6.8194
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.68     |
| episodes                | 116600   |
| lives                   | 116600   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 760279   |
| value_loss              | 8.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0444  |
| entropy                 | 2.67     |
| episodes                | 116700   |
| lives                   | 116700   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0037   |
| steps                   | 760977   |
| value_loss              | 8.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.64     |
| episodes                | 116800   |
| lives                   | 116800   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 761692   |
| value_loss              | 8.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.037   |
| entropy                 | 2.64     |
| episodes                | 116900   |
| lives                   | 116900   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 762404   |
| value_loss              | 8.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0204  |
| entropy                 | 2.58     |
| episodes                | 117000   |
| lives                   | 117000   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 763102   |
| value_loss              | 8.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0379  |
| entropy                 | 2.61     |
| episodes                | 117100   |
| lives                   | 117100   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 763850   |
| value_loss              | 8.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0367  |
| entropy                 | 2.56     |
| episodes                | 117200   |
| lives                   | 117200   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 764549   |
| value_loss              | 8.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.57     |
| episodes                | 117300   |
| lives                   | 117300   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 765263   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.59     |
| episodes                | 117400   |
| lives                   | 117400   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 7.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 765968   |
| value_loss              | 8.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.015   |
| entropy                 | 2.53     |
| episodes                | 117500   |
| lives                   | 117500   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 766669   |
| value_loss              | 8.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0226  |
| entropy                 | 2.56     |
| episodes                | 117600   |
| lives                   | 117600   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 767365   |
| value_loss              | 7.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.65     |
| episodes                | 117700   |
| lives                   | 117700   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 768091   |
| value_loss              | 8.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0221  |
| entropy                 | 2.63     |
| episodes                | 117800   |
| lives                   | 117800   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 768775   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.65     |
| episodes                | 117900   |
| lives                   | 117900   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 769508   |
| value_loss              | 8.14     |
--------------------------------------
Saving model due to running mean reward increase: 6.5738 -> 6.5806
--------------------------------------
| approx_kl               | -0.0387  |
| entropy                 | 2.62     |
| episodes                | 118000   |
| lives                   | 118000   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 770163   |
| value_loss              | 7.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0428  |
| entropy                 | 2.65     |
| episodes                | 118100   |
| lives                   | 118100   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 770861   |
| value_loss              | 8.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.039   |
| entropy                 | 2.67     |
| episodes                | 118200   |
| lives                   | 118200   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0046   |
| steps                   | 771539   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0162  |
| entropy                 | 2.68     |
| episodes                | 118300   |
| lives                   | 118300   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 772263   |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.62     |
| episodes                | 118400   |
| lives                   | 118400   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 772976   |
| value_loss              | 8.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0153  |
| entropy                 | 2.52     |
| episodes                | 118500   |
| lives                   | 118500   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 773643   |
| value_loss              | 7.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.031   |
| entropy                 | 2.58     |
| episodes                | 118600   |
| lives                   | 118600   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 774332   |
| value_loss              | 7.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0237  |
| entropy                 | 2.57     |
| episodes                | 118700   |
| lives                   | 118700   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 775017   |
| value_loss              | 9.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0286  |
| entropy                 | 2.6      |
| episodes                | 118800   |
| lives                   | 118800   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 775715   |
| value_loss              | 8.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0225  |
| entropy                 | 2.6      |
| episodes                | 118900   |
| lives                   | 118900   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0206  |
| steps                   | 776407   |
| value_loss              | 8.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.53     |
| episodes                | 119000   |
| lives                   | 119000   |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 777030   |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0135  |
| entropy                 | 2.45     |
| episodes                | 119100   |
| lives                   | 119100   |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 777649   |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0339  |
| entropy                 | 2.41     |
| episodes                | 119200   |
| lives                   | 119200   |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.005    |
| steps                   | 778231   |
| value_loss              | 8.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.52     |
| episodes                | 119300   |
| lives                   | 119300   |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0051   |
| steps                   | 778885   |
| value_loss              | 8.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0156  |
| entropy                 | 2.57     |
| episodes                | 119400   |
| lives                   | 119400   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0012   |
| steps                   | 779556   |
| value_loss              | 8.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0186  |
| entropy                 | 2.64     |
| episodes                | 119500   |
| lives                   | 119500   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 780262   |
| value_loss              | 7.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0111  |
| entropy                 | 2.69     |
| episodes                | 119600   |
| lives                   | 119600   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0209  |
| steps                   | 780996   |
| value_loss              | 7.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0079  |
| entropy                 | 2.65     |
| episodes                | 119700   |
| lives                   | 119700   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 781704   |
| value_loss              | 8.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0159  |
| entropy                 | 2.64     |
| episodes                | 119800   |
| lives                   | 119800   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0052   |
| steps                   | 782413   |
| value_loss              | 7.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0103  |
| entropy                 | 2.59     |
| episodes                | 119900   |
| lives                   | 119900   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 783120   |
| value_loss              | 8.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0086  |
| entropy                 | 2.56     |
| episodes                | 120000   |
| lives                   | 120000   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 783833   |
| value_loss              | 8.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0119  |
| entropy                 | 2.57     |
| episodes                | 120100   |
| lives                   | 120100   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 784579   |
| value_loss              | 8.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0301  |
| entropy                 | 2.56     |
| episodes                | 120200   |
| lives                   | 120200   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 785285   |
| value_loss              | 8.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.56     |
| episodes                | 120300   |
| lives                   | 120300   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 786010   |
| value_loss              | 8.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.64     |
| episodes                | 120400   |
| lives                   | 120400   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0042   |
| steps                   | 786702   |
| value_loss              | 7.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.58     |
| episodes                | 120500   |
| lives                   | 120500   |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 787368   |
| value_loss              | 7.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0061  |
| entropy                 | 2.59     |
| episodes                | 120600   |
| lives                   | 120600   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0244  |
| steps                   | 788010   |
| value_loss              | 8.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0094  |
| entropy                 | 2.57     |
| episodes                | 120700   |
| lives                   | 120700   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 788721   |
| value_loss              | 8.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0092  |
| entropy                 | 2.65     |
| episodes                | 120800   |
| lives                   | 120800   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 789445   |
| value_loss              | 8.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0076  |
| entropy                 | 2.58     |
| episodes                | 120900   |
| lives                   | 120900   |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 790080   |
| value_loss              | 8.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.63     |
| episodes                | 121000   |
| lives                   | 121000   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 790792   |
| value_loss              | 8.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0169  |
| entropy                 | 2.59     |
| episodes                | 121100   |
| lives                   | 121100   |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0037   |
| steps                   | 791473   |
| value_loss              | 8.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0147  |
| entropy                 | 2.61     |
| episodes                | 121200   |
| lives                   | 121200   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 792200   |
| value_loss              | 8.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.65     |
| episodes                | 121300   |
| lives                   | 121300   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 792913   |
| value_loss              | 7.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0157  |
| entropy                 | 2.6      |
| episodes                | 121400   |
| lives                   | 121400   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 793637   |
| value_loss              | 8.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.58     |
| episodes                | 121500   |
| lives                   | 121500   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 794360   |
| value_loss              | 8.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0144  |
| entropy                 | 2.59     |
| episodes                | 121600   |
| lives                   | 121600   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 795117   |
| value_loss              | 8.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0239  |
| entropy                 | 2.67     |
| episodes                | 121700   |
| lives                   | 121700   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 795874   |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0274  |
| entropy                 | 2.6      |
| episodes                | 121800   |
| lives                   | 121800   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 796595   |
| value_loss              | 8.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0317  |
| entropy                 | 2.63     |
| episodes                | 121900   |
| lives                   | 121900   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 797306   |
| value_loss              | 7.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.69     |
| episodes                | 122000   |
| lives                   | 122000   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 7.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 798113   |
| value_loss              | 8.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.017   |
| entropy                 | 2.58     |
| episodes                | 122100   |
| lives                   | 122100   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 798815   |
| value_loss              | 7.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0218  |
| entropy                 | 2.56     |
| episodes                | 122200   |
| lives                   | 122200   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 799532   |
| value_loss              | 7.74     |
--------------------------------------
Saving model due to running mean reward increase: 6.351 -> 6.6499
--------------------------------------
| approx_kl               | -0.0085  |
| entropy                 | 2.54     |
| episodes                | 122300   |
| lives                   | 122300   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 800273   |
| value_loss              | 8.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.47     |
| episodes                | 122400   |
| lives                   | 122400   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 800956   |
| value_loss              | 7.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.53     |
| episodes                | 122500   |
| lives                   | 122500   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 801671   |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0183  |
| entropy                 | 2.58     |
| episodes                | 122600   |
| lives                   | 122600   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 802371   |
| value_loss              | 7.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.54     |
| episodes                | 122700   |
| lives                   | 122700   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 803056   |
| value_loss              | 7.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.55     |
| episodes                | 122800   |
| lives                   | 122800   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 803786   |
| value_loss              | 8.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.52     |
| episodes                | 122900   |
| lives                   | 122900   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 804486   |
| value_loss              | 8.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.51     |
| episodes                | 123000   |
| lives                   | 123000   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 805202   |
| value_loss              | 7.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.52     |
| episodes                | 123100   |
| lives                   | 123100   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 805915   |
| value_loss              | 7.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0216  |
| entropy                 | 2.56     |
| episodes                | 123200   |
| lives                   | 123200   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 806631   |
| value_loss              | 7.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.019   |
| entropy                 | 2.54     |
| episodes                | 123300   |
| lives                   | 123300   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 807352   |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.52     |
| episodes                | 123400   |
| lives                   | 123400   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 808051   |
| value_loss              | 7.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.52     |
| episodes                | 123500   |
| lives                   | 123500   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 808772   |
| value_loss              | 7.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0217  |
| entropy                 | 2.52     |
| episodes                | 123600   |
| lives                   | 123600   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 809474   |
| value_loss              | 8.14     |
--------------------------------------
Saving model due to running mean reward increase: 6.0143 -> 6.2238
--------------------------------------
| approx_kl               | -0.0203  |
| entropy                 | 2.5      |
| episodes                | 123700   |
| lives                   | 123700   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 810196   |
| value_loss              | 8.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0156  |
| entropy                 | 2.41     |
| episodes                | 123800   |
| lives                   | 123800   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 810887   |
| value_loss              | 8.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0135  |
| entropy                 | 2.44     |
| episodes                | 123900   |
| lives                   | 123900   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 811592   |
| value_loss              | 8.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0187  |
| entropy                 | 2.42     |
| episodes                | 124000   |
| lives                   | 124000   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 812283   |
| value_loss              | 8.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0232  |
| entropy                 | 2.44     |
| episodes                | 124100   |
| lives                   | 124100   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 812970   |
| value_loss              | 8.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0273  |
| entropy                 | 2.62     |
| episodes                | 124200   |
| lives                   | 124200   |
| mean 100 episode length | 9.06     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 813776   |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0242  |
| entropy                 | 2.58     |
| episodes                | 124300   |
| lives                   | 124300   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0222  |
| steps                   | 814541   |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0773  |
| entropy                 | 2.53     |
| episodes                | 124400   |
| lives                   | 124400   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0239   |
| steps                   | 815246   |
| value_loss              | 7.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0383  |
| entropy                 | 2.49     |
| episodes                | 124500   |
| lives                   | 124500   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 815922   |
| value_loss              | 8.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0337  |
| entropy                 | 2.6      |
| episodes                | 124600   |
| lives                   | 124600   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 816626   |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.034   |
| entropy                 | 2.69     |
| episodes                | 124700   |
| lives                   | 124700   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 817374   |
| value_loss              | 6.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0269  |
| entropy                 | 2.64     |
| episodes                | 124800   |
| lives                   | 124800   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 818082   |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0453  |
| entropy                 | 2.53     |
| episodes                | 124900   |
| lives                   | 124900   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 818730   |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0419  |
| entropy                 | 2.64     |
| episodes                | 125000   |
| lives                   | 125000   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 819448   |
| value_loss              | 6.65     |
--------------------------------------
Saving model due to running mean reward increase: 6.2771 -> 6.4826
--------------------------------------
| approx_kl               | -0.0398  |
| entropy                 | 2.58     |
| episodes                | 125100   |
| lives                   | 125100   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 820096   |
| value_loss              | 7.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0298  |
| entropy                 | 2.64     |
| episodes                | 125200   |
| lives                   | 125200   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 820810   |
| value_loss              | 7.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0318  |
| entropy                 | 2.5      |
| episodes                | 125300   |
| lives                   | 125300   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 7.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 821500   |
| value_loss              | 8.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0609  |
| entropy                 | 2.62     |
| episodes                | 125400   |
| lives                   | 125400   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.006    |
| steps                   | 822237   |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0496  |
| entropy                 | 2.54     |
| episodes                | 125500   |
| lives                   | 125500   |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 822909   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0154  |
| entropy                 | 2.5      |
| episodes                | 125600   |
| lives                   | 125600   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 823568   |
| value_loss              | 7.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0153  |
| entropy                 | 2.49     |
| episodes                | 125700   |
| lives                   | 125700   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0222  |
| steps                   | 824267   |
| value_loss              | 7.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0129  |
| entropy                 | 2.54     |
| episodes                | 125800   |
| lives                   | 125800   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0212  |
| steps                   | 824987   |
| value_loss              | 7.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0231  |
| entropy                 | 2.61     |
| episodes                | 125900   |
| lives                   | 125900   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 825738   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.52     |
| episodes                | 126000   |
| lives                   | 126000   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 826430   |
| value_loss              | 7.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0205  |
| entropy                 | 2.62     |
| episodes                | 126100   |
| lives                   | 126100   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 827161   |
| value_loss              | 8.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0301  |
| entropy                 | 2.64     |
| episodes                | 126200   |
| lives                   | 126200   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 827924   |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0297  |
| entropy                 | 2.6      |
| episodes                | 126300   |
| lives                   | 126300   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 828625   |
| value_loss              | 8.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.59     |
| episodes                | 126400   |
| lives                   | 126400   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 829334   |
| value_loss              | 8.34     |
--------------------------------------
Saving model due to running mean reward increase: 6.5145 -> 6.7465
--------------------------------------
| approx_kl               | -0.0316  |
| entropy                 | 2.6      |
| episodes                | 126500   |
| lives                   | 126500   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 830090   |
| value_loss              | 7.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0367  |
| entropy                 | 2.55     |
| episodes                | 126600   |
| lives                   | 126600   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 830852   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0356  |
| entropy                 | 2.57     |
| episodes                | 126700   |
| lives                   | 126700   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0026   |
| steps                   | 831572   |
| value_loss              | 7.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.5      |
| episodes                | 126800   |
| lives                   | 126800   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 832276   |
| value_loss              | 6.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0233  |
| entropy                 | 2.45     |
| episodes                | 126900   |
| lives                   | 126900   |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 832948   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0173  |
| entropy                 | 2.4      |
| episodes                | 127000   |
| lives                   | 127000   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0162  |
| steps                   | 833646   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0338  |
| entropy                 | 2.59     |
| episodes                | 127100   |
| lives                   | 127100   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 834403   |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0329  |
| entropy                 | 2.69     |
| episodes                | 127200   |
| lives                   | 127200   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 835141   |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.61     |
| episodes                | 127300   |
| lives                   | 127300   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 835860   |
| value_loss              | 7.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0161  |
| entropy                 | 2.7      |
| episodes                | 127400   |
| lives                   | 127400   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 836594   |
| value_loss              | 7.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0337  |
| entropy                 | 2.61     |
| episodes                | 127500   |
| lives                   | 127500   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 837253   |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.58     |
| episodes                | 127600   |
| lives                   | 127600   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 837956   |
| value_loss              | 7.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0187  |
| entropy                 | 2.51     |
| episodes                | 127700   |
| lives                   | 127700   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 838658   |
| value_loss              | 8.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.5      |
| episodes                | 127800   |
| lives                   | 127800   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 839392   |
| value_loss              | 7.46     |
--------------------------------------
Saving model due to mean reward increase: 6.883 -> 6.9772
--------------------------------------
| approx_kl               | -0.0197  |
| entropy                 | 2.51     |
| episodes                | 127900   |
| lives                   | 127900   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 7.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 840146   |
| value_loss              | 7.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0133  |
| entropy                 | 2.51     |
| episodes                | 128000   |
| lives                   | 128000   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 840871   |
| value_loss              | 8.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0146  |
| entropy                 | 2.51     |
| episodes                | 128100   |
| lives                   | 128100   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 841600   |
| value_loss              | 7.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0205  |
| entropy                 | 2.53     |
| episodes                | 128200   |
| lives                   | 128200   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 842319   |
| value_loss              | 8.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0167  |
| entropy                 | 2.58     |
| episodes                | 128300   |
| lives                   | 128300   |
| mean 100 episode length | 8.74     |
| mean 100 episode reward | 7.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 843093   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0431  |
| entropy                 | 2.47     |
| episodes                | 128400   |
| lives                   | 128400   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 843825   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0436  |
| entropy                 | 2.51     |
| episodes                | 128500   |
| lives                   | 128500   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0182   |
| steps                   | 844534   |
| value_loss              | 7.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0147  |
| entropy                 | 2.54     |
| episodes                | 128600   |
| lives                   | 128600   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 845286   |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0188  |
| entropy                 | 2.42     |
| episodes                | 128700   |
| lives                   | 128700   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 845989   |
| value_loss              | 8.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0188  |
| entropy                 | 2.33     |
| episodes                | 128800   |
| lives                   | 128800   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 846676   |
| value_loss              | 6.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0178  |
| entropy                 | 2.35     |
| episodes                | 128900   |
| lives                   | 128900   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 847387   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0158  |
| entropy                 | 2.49     |
| episodes                | 129000   |
| lives                   | 129000   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 848147   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0167  |
| entropy                 | 2.45     |
| episodes                | 129100   |
| lives                   | 129100   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 848895   |
| value_loss              | 7.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0118  |
| entropy                 | 2.47     |
| episodes                | 129200   |
| lives                   | 129200   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 849620   |
| value_loss              | 7.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0174  |
| entropy                 | 2.48     |
| episodes                | 129300   |
| lives                   | 129300   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 850381   |
| value_loss              | 7.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0159  |
| entropy                 | 2.46     |
| episodes                | 129400   |
| lives                   | 129400   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 851082   |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0059  |
| entropy                 | 2.54     |
| episodes                | 129500   |
| lives                   | 129500   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 851820   |
| value_loss              | 7.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.02    |
| entropy                 | 2.6      |
| episodes                | 129600   |
| lives                   | 129600   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 852559   |
| value_loss              | 6.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0141  |
| entropy                 | 2.58     |
| episodes                | 129700   |
| lives                   | 129700   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 853341   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0141  |
| entropy                 | 2.54     |
| episodes                | 129800   |
| lives                   | 129800   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 854099   |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0086  |
| entropy                 | 2.53     |
| episodes                | 129900   |
| lives                   | 129900   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 854833   |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.012   |
| entropy                 | 2.57     |
| episodes                | 130000   |
| lives                   | 130000   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 7.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 855656   |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0257  |
| entropy                 | 2.49     |
| episodes                | 130100   |
| lives                   | 130100   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 856389   |
| value_loss              | 7.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0236  |
| entropy                 | 2.42     |
| episodes                | 130200   |
| lives                   | 130200   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 857073   |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0367  |
| entropy                 | 2.46     |
| episodes                | 130300   |
| lives                   | 130300   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 857772   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0376  |
| entropy                 | 2.56     |
| episodes                | 130400   |
| lives                   | 130400   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 858513   |
| value_loss              | 7.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0269  |
| entropy                 | 2.6      |
| episodes                | 130500   |
| lives                   | 130500   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 7.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 859271   |
| value_loss              | 7.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0299  |
| entropy                 | 2.63     |
| episodes                | 130600   |
| lives                   | 130600   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 860004   |
| value_loss              | 7.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0094  |
| entropy                 | 2.63     |
| episodes                | 130700   |
| lives                   | 130700   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 860710   |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0526  |
| entropy                 | 2.55     |
| episodes                | 130800   |
| lives                   | 130800   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 861416   |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0416  |
| entropy                 | 2.58     |
| episodes                | 130900   |
| lives                   | 130900   |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 862072   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0324  |
| entropy                 | 2.52     |
| episodes                | 131000   |
| lives                   | 131000   |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 862686   |
| value_loss              | 8.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.032   |
| entropy                 | 2.52     |
| episodes                | 131100   |
| lives                   | 131100   |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 863318   |
| value_loss              | 7.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0327  |
| entropy                 | 2.57     |
| episodes                | 131200   |
| lives                   | 131200   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 864005   |
| value_loss              | 7.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.57     |
| episodes                | 131300   |
| lives                   | 131300   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 864703   |
| value_loss              | 7.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.59     |
| episodes                | 131400   |
| lives                   | 131400   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 7.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 865434   |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0151  |
| entropy                 | 2.56     |
| episodes                | 131500   |
| lives                   | 131500   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 866101   |
| value_loss              | 7.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0296  |
| entropy                 | 2.64     |
| episodes                | 131600   |
| lives                   | 131600   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 7.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 866855   |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0436  |
| entropy                 | 2.61     |
| episodes                | 131700   |
| lives                   | 131700   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 867599   |
| value_loss              | 7.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0519  |
| entropy                 | 2.6      |
| episodes                | 131800   |
| lives                   | 131800   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 868296   |
| value_loss              | 6.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0528  |
| entropy                 | 2.57     |
| episodes                | 131900   |
| lives                   | 131900   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 868947   |
| value_loss              | 7.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0322  |
| entropy                 | 2.55     |
| episodes                | 132000   |
| lives                   | 132000   |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 869646   |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0388  |
| entropy                 | 2.54     |
| episodes                | 132100   |
| lives                   | 132100   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 870356   |
| value_loss              | 7.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0301  |
| entropy                 | 2.59     |
| episodes                | 132200   |
| lives                   | 132200   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 871089   |
| value_loss              | 7.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.53     |
| episodes                | 132300   |
| lives                   | 132300   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 871802   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.64     |
| episodes                | 132400   |
| lives                   | 132400   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0061   |
| steps                   | 872510   |
| value_loss              | 7.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0368  |
| entropy                 | 2.55     |
| episodes                | 132500   |
| lives                   | 132500   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 873205   |
| value_loss              | 7.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0395  |
| entropy                 | 2.52     |
| episodes                | 132600   |
| lives                   | 132600   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 873885   |
| value_loss              | 7.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.041   |
| entropy                 | 2.53     |
| episodes                | 132700   |
| lives                   | 132700   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 874527   |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0649  |
| entropy                 | 2.58     |
| episodes                | 132800   |
| lives                   | 132800   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 875179   |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0671  |
| entropy                 | 2.59     |
| episodes                | 132900   |
| lives                   | 132900   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 875830   |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.05    |
| entropy                 | 2.58     |
| episodes                | 133000   |
| lives                   | 133000   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 876494   |
| value_loss              | 6.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0717  |
| entropy                 | 2.6      |
| episodes                | 133100   |
| lives                   | 133100   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 877180   |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.58     |
| episodes                | 133200   |
| lives                   | 133200   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 877840   |
| value_loss              | 6.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0377  |
| entropy                 | 2.58     |
| episodes                | 133300   |
| lives                   | 133300   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0139   |
| steps                   | 878508   |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0326  |
| entropy                 | 2.56     |
| episodes                | 133400   |
| lives                   | 133400   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 879231   |
| value_loss              | 7.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0224  |
| entropy                 | 2.46     |
| episodes                | 133500   |
| lives                   | 133500   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0036   |
| steps                   | 879921   |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0137  |
| entropy                 | 2.55     |
| episodes                | 133600   |
| lives                   | 133600   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 7.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 880673   |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.63     |
| episodes                | 133700   |
| lives                   | 133700   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 881449   |
| value_loss              | 6.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.6      |
| episodes                | 133800   |
| lives                   | 133800   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 882201   |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0158  |
| entropy                 | 2.53     |
| episodes                | 133900   |
| lives                   | 133900   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 882922   |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0209  |
| entropy                 | 2.5      |
| episodes                | 134000   |
| lives                   | 134000   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 883619   |
| value_loss              | 6.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.51     |
| episodes                | 134100   |
| lives                   | 134100   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 884338   |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0186  |
| entropy                 | 2.5      |
| episodes                | 134200   |
| lives                   | 134200   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 885069   |
| value_loss              | 6.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.56     |
| episodes                | 134300   |
| lives                   | 134300   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 885793   |
| value_loss              | 6.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0317  |
| entropy                 | 2.56     |
| episodes                | 134400   |
| lives                   | 134400   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 886501   |
| value_loss              | 6.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0376  |
| entropy                 | 2.58     |
| episodes                | 134500   |
| lives                   | 134500   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 887234   |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0315  |
| entropy                 | 2.51     |
| episodes                | 134600   |
| lives                   | 134600   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 887938   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.57     |
| episodes                | 134700   |
| lives                   | 134700   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0        |
| steps                   | 888632   |
| value_loss              | 7.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0392  |
| entropy                 | 2.62     |
| episodes                | 134800   |
| lives                   | 134800   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 889357   |
| value_loss              | 6.84     |
--------------------------------------
Saving model due to running mean reward increase: 6.2342 -> 6.5642
--------------------------------------
| approx_kl               | -0.0347  |
| entropy                 | 2.59     |
| episodes                | 134900   |
| lives                   | 134900   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 890100   |
| value_loss              | 6.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.61     |
| episodes                | 135000   |
| lives                   | 135000   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 890809   |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0297  |
| entropy                 | 2.67     |
| episodes                | 135100   |
| lives                   | 135100   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 891534   |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0825  |
| entropy                 | 2.7      |
| episodes                | 135200   |
| lives                   | 135200   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 892223   |
| value_loss              | 6.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0934  |
| entropy                 | 2.67     |
| episodes                | 135300   |
| lives                   | 135300   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0165   |
| steps                   | 892872   |
| value_loss              | 6.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0667  |
| entropy                 | 2.58     |
| episodes                | 135400   |
| lives                   | 135400   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0007   |
| steps                   | 893506   |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0457  |
| entropy                 | 2.58     |
| episodes                | 135500   |
| lives                   | 135500   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 894173   |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0532  |
| entropy                 | 2.54     |
| episodes                | 135600   |
| lives                   | 135600   |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 894791   |
| value_loss              | 7.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0428  |
| entropy                 | 2.57     |
| episodes                | 135700   |
| lives                   | 135700   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 895461   |
| value_loss              | 7.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.52     |
| episodes                | 135800   |
| lives                   | 135800   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 896146   |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0728  |
| entropy                 | 2.59     |
| episodes                | 135900   |
| lives                   | 135900   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.005    |
| steps                   | 896740   |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0725  |
| entropy                 | 2.49     |
| episodes                | 136000   |
| lives                   | 136000   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0073   |
| steps                   | 897355   |
| value_loss              | 7.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0624  |
| entropy                 | 2.49     |
| episodes                | 136100   |
| lives                   | 136100   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0066   |
| steps                   | 898033   |
| value_loss              | 7.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0611  |
| entropy                 | 2.6      |
| episodes                | 136200   |
| lives                   | 136200   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0061   |
| steps                   | 898726   |
| value_loss              | 6.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0378  |
| entropy                 | 2.44     |
| episodes                | 136300   |
| lives                   | 136300   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 899417   |
| value_loss              | 6.69     |
--------------------------------------
Saving model due to running mean reward increase: 6.3449 -> 6.4058
--------------------------------------
| approx_kl               | -0.0383  |
| entropy                 | 2.34     |
| episodes                | 136400   |
| lives                   | 136400   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 900090   |
| value_loss              | 7.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0479  |
| entropy                 | 2.58     |
| episodes                | 136500   |
| lives                   | 136500   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 900799   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0578  |
| entropy                 | 2.68     |
| episodes                | 136600   |
| lives                   | 136600   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0002   |
| steps                   | 901528   |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0735  |
| entropy                 | 2.73     |
| episodes                | 136700   |
| lives                   | 136700   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0055   |
| steps                   | 902231   |
| value_loss              | 6.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0586  |
| entropy                 | 2.62     |
| episodes                | 136800   |
| lives                   | 136800   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0065   |
| steps                   | 902910   |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0663  |
| entropy                 | 2.68     |
| episodes                | 136900   |
| lives                   | 136900   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0004   |
| steps                   | 903571   |
| value_loss              | 6.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0911  |
| entropy                 | 2.68     |
| episodes                | 137000   |
| lives                   | 137000   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.007    |
| steps                   | 904230   |
| value_loss              | 6.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.7      |
| episodes                | 137100   |
| lives                   | 137100   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0155   |
| steps                   | 904989   |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0197  |
| entropy                 | 2.53     |
| episodes                | 137200   |
| lives                   | 137200   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 905761   |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0081  |
| entropy                 | 2.55     |
| episodes                | 137300   |
| lives                   | 137300   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 906503   |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0121  |
| entropy                 | 2.49     |
| episodes                | 137400   |
| lives                   | 137400   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 907249   |
| value_loss              | 6.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.42     |
| episodes                | 137500   |
| lives                   | 137500   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 908011   |
| value_loss              | 7.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.36     |
| episodes                | 137600   |
| lives                   | 137600   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 908744   |
| value_loss              | 7.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.44     |
| episodes                | 137700   |
| lives                   | 137700   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 7.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 909480   |
| value_loss              | 7.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0348  |
| entropy                 | 2.45     |
| episodes                | 137800   |
| lives                   | 137800   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 910178   |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.51     |
| episodes                | 137900   |
| lives                   | 137900   |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 910825   |
| value_loss              | 7.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0525  |
| entropy                 | 2.61     |
| episodes                | 138000   |
| lives                   | 138000   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0059   |
| steps                   | 911530   |
| value_loss              | 7.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0478  |
| entropy                 | 2.51     |
| episodes                | 138100   |
| lives                   | 138100   |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0037   |
| steps                   | 912171   |
| value_loss              | 7.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0369  |
| entropy                 | 2.47     |
| episodes                | 138200   |
| lives                   | 138200   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 912822   |
| value_loss              | 7.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.38     |
| episodes                | 138300   |
| lives                   | 138300   |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 913425   |
| value_loss              | 7.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0386  |
| entropy                 | 2.38     |
| episodes                | 138400   |
| lives                   | 138400   |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 914091   |
| value_loss              | 6.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.37     |
| episodes                | 138500   |
| lives                   | 138500   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.01     |
| steps                   | 914742   |
| value_loss              | 8        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0569  |
| entropy                 | 2.45     |
| episodes                | 138600   |
| lives                   | 138600   |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 915362   |
| value_loss              | 7.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0627  |
| entropy                 | 2.4      |
| episodes                | 138700   |
| lives                   | 138700   |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 916003   |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0839  |
| entropy                 | 2.37     |
| episodes                | 138800   |
| lives                   | 138800   |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 916633   |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.38     |
| episodes                | 138900   |
| lives                   | 138900   |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 917256   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.031   |
| entropy                 | 2.4      |
| episodes                | 139000   |
| lives                   | 139000   |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0002   |
| steps                   | 917891   |
| value_loss              | 8.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.024   |
| entropy                 | 2.28     |
| episodes                | 139100   |
| lives                   | 139100   |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 918544   |
| value_loss              | 7.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0329  |
| entropy                 | 2.25     |
| episodes                | 139200   |
| lives                   | 139200   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 919192   |
| value_loss              | 8.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0323  |
| entropy                 | 2.44     |
| episodes                | 139300   |
| lives                   | 139300   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 919897   |
| value_loss              | 8.73     |
--------------------------------------
Saving model due to running mean reward increase: 6.483 -> 6.6346
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.5      |
| episodes                | 139400   |
| lives                   | 139400   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 920599   |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.4      |
| episodes                | 139500   |
| lives                   | 139500   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0172  |
| steps                   | 921306   |
| value_loss              | 8.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0217  |
| entropy                 | 2.37     |
| episodes                | 139600   |
| lives                   | 139600   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 921984   |
| value_loss              | 8.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0191  |
| entropy                 | 2.39     |
| episodes                | 139700   |
| lives                   | 139700   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 922671   |
| value_loss              | 8.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0267  |
| entropy                 | 2.44     |
| episodes                | 139800   |
| lives                   | 139800   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 923358   |
| value_loss              | 8.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0278  |
| entropy                 | 2.41     |
| episodes                | 139900   |
| lives                   | 139900   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 924062   |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0346  |
| entropy                 | 2.45     |
| episodes                | 140000   |
| lives                   | 140000   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0017   |
| steps                   | 924769   |
| value_loss              | 8.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.41     |
| episodes                | 140100   |
| lives                   | 140100   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 925449   |
| value_loss              | 8.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0251  |
| entropy                 | 2.35     |
| episodes                | 140200   |
| lives                   | 140200   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 926054   |
| value_loss              | 7.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.46     |
| episodes                | 140300   |
| lives                   | 140300   |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 926717   |
| value_loss              | 7.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0201  |
| entropy                 | 2.46     |
| episodes                | 140400   |
| lives                   | 140400   |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 927394   |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0201  |
| entropy                 | 2.51     |
| episodes                | 140500   |
| lives                   | 140500   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 928110   |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0168  |
| entropy                 | 2.45     |
| episodes                | 140600   |
| lives                   | 140600   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 928828   |
| value_loss              | 8        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.02    |
| entropy                 | 2.39     |
| episodes                | 140700   |
| lives                   | 140700   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 929522   |
| value_loss              | 7.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0187  |
| entropy                 | 2.58     |
| episodes                | 140800   |
| lives                   | 140800   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 930308   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0104  |
| entropy                 | 2.55     |
| episodes                | 140900   |
| lives                   | 140900   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 931052   |
| value_loss              | 6.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0097  |
| entropy                 | 2.52     |
| episodes                | 141000   |
| lives                   | 141000   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 931840   |
| value_loss              | 7.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0178  |
| entropy                 | 2.46     |
| episodes                | 141100   |
| lives                   | 141100   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 932569   |
| value_loss              | 8.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.53     |
| episodes                | 141200   |
| lives                   | 141200   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 933325   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0137  |
| entropy                 | 2.49     |
| episodes                | 141300   |
| lives                   | 141300   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 934061   |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0073  |
| entropy                 | 2.45     |
| episodes                | 141400   |
| lives                   | 141400   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 6.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 934815   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0196  |
| entropy                 | 2.43     |
| episodes                | 141500   |
| lives                   | 141500   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 935525   |
| value_loss              | 7.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0272  |
| entropy                 | 2.51     |
| episodes                | 141600   |
| lives                   | 141600   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 936237   |
| value_loss              | 7.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0151  |
| entropy                 | 2.58     |
| episodes                | 141700   |
| lives                   | 141700   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.001    |
| steps                   | 937006   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0248  |
| entropy                 | 2.43     |
| episodes                | 141800   |
| lives                   | 141800   |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 937663   |
| value_loss              | 7.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0207  |
| entropy                 | 2.53     |
| episodes                | 141900   |
| lives                   | 141900   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 7.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 938446   |
| value_loss              | 7.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0125  |
| entropy                 | 2.44     |
| episodes                | 142000   |
| lives                   | 142000   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 939174   |
| value_loss              | 7.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0327  |
| entropy                 | 2.41     |
| episodes                | 142100   |
| lives                   | 142100   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0199  |
| steps                   | 939862   |
| value_loss              | 7.35     |
--------------------------------------
Saving model due to running mean reward increase: 6.3926 -> 6.5679
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.51     |
| episodes                | 142200   |
| lives                   | 142200   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 940599   |
| value_loss              | 7.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0306  |
| entropy                 | 2.53     |
| episodes                | 142300   |
| lives                   | 142300   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 941302   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.47     |
| episodes                | 142400   |
| lives                   | 142400   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 942007   |
| value_loss              | 8.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0192  |
| entropy                 | 2.43     |
| episodes                | 142500   |
| lives                   | 142500   |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 942632   |
| value_loss              | 8.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0209  |
| entropy                 | 2.65     |
| episodes                | 142600   |
| lives                   | 142600   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 943397   |
| value_loss              | 7.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0345  |
| entropy                 | 2.64     |
| episodes                | 142700   |
| lives                   | 142700   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0166  |
| steps                   | 944113   |
| value_loss              | 7.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0248  |
| entropy                 | 2.6      |
| episodes                | 142800   |
| lives                   | 142800   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 944891   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.55     |
| episodes                | 142900   |
| lives                   | 142900   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 945681   |
| value_loss              | 6.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0236  |
| entropy                 | 2.55     |
| episodes                | 143000   |
| lives                   | 143000   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 946448   |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.58     |
| episodes                | 143100   |
| lives                   | 143100   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 947182   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0333  |
| entropy                 | 2.48     |
| episodes                | 143200   |
| lives                   | 143200   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0204  |
| steps                   | 947833   |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0182  |
| entropy                 | 2.56     |
| episodes                | 143300   |
| lives                   | 143300   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 948568   |
| value_loss              | 7.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.031   |
| entropy                 | 2.47     |
| episodes                | 143400   |
| lives                   | 143400   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 949248   |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0562  |
| entropy                 | 2.57     |
| episodes                | 143500   |
| lives                   | 143500   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0073   |
| steps                   | 949919   |
| value_loss              | 7.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0249  |
| entropy                 | 2.53     |
| episodes                | 143600   |
| lives                   | 143600   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 950556   |
| value_loss              | 7.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0131  |
| entropy                 | 2.52     |
| episodes                | 143700   |
| lives                   | 143700   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 951262   |
| value_loss              | 7.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0177  |
| entropy                 | 2.42     |
| episodes                | 143800   |
| lives                   | 143800   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 951904   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.025   |
| entropy                 | 2.44     |
| episodes                | 143900   |
| lives                   | 143900   |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 952551   |
| value_loss              | 7.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.46     |
| episodes                | 144000   |
| lives                   | 144000   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 953218   |
| value_loss              | 7.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0343  |
| entropy                 | 2.44     |
| episodes                | 144100   |
| lives                   | 144100   |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 953854   |
| value_loss              | 6.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0175  |
| entropy                 | 2.55     |
| episodes                | 144200   |
| lives                   | 144200   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 954577   |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0233  |
| entropy                 | 2.48     |
| episodes                | 144300   |
| lives                   | 144300   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 955280   |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0137  |
| entropy                 | 2.53     |
| episodes                | 144400   |
| lives                   | 144400   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 956020   |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0154  |
| entropy                 | 2.52     |
| episodes                | 144500   |
| lives                   | 144500   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 7.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 956768   |
| value_loss              | 7.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.012   |
| entropy                 | 2.47     |
| episodes                | 144600   |
| lives                   | 144600   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 957500   |
| value_loss              | 7.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0299  |
| entropy                 | 2.51     |
| episodes                | 144700   |
| lives                   | 144700   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 958220   |
| value_loss              | 7.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.5      |
| episodes                | 144800   |
| lives                   | 144800   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 958903   |
| value_loss              | 7.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.53     |
| episodes                | 144900   |
| lives                   | 144900   |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 959584   |
| value_loss              | 7.55     |
--------------------------------------
Saving model due to running mean reward increase: 6.4196 -> 6.5369
--------------------------------------
| approx_kl               | -0.0509  |
| entropy                 | 2.6      |
| episodes                | 145000   |
| lives                   | 145000   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0011   |
| steps                   | 960306   |
| value_loss              | 7.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0348  |
| entropy                 | 2.64     |
| episodes                | 145100   |
| lives                   | 145100   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.003    |
| steps                   | 961002   |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0243  |
| entropy                 | 2.62     |
| episodes                | 145200   |
| lives                   | 145200   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 961728   |
| value_loss              | 7.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0434  |
| entropy                 | 2.54     |
| episodes                | 145300   |
| lives                   | 145300   |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 962364   |
| value_loss              | 8.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0411  |
| entropy                 | 2.58     |
| episodes                | 145400   |
| lives                   | 145400   |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 963036   |
| value_loss              | 8.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.61     |
| episodes                | 145500   |
| lives                   | 145500   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 963767   |
| value_loss              | 7.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0268  |
| entropy                 | 2.6      |
| episodes                | 145600   |
| lives                   | 145600   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 964445   |
| value_loss              | 7.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0497  |
| entropy                 | 2.59     |
| episodes                | 145700   |
| lives                   | 145700   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 965118   |
| value_loss              | 7.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0523  |
| entropy                 | 2.67     |
| episodes                | 145800   |
| lives                   | 145800   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0018   |
| steps                   | 965862   |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.63     |
| episodes                | 145900   |
| lives                   | 145900   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0029   |
| steps                   | 966573   |
| value_loss              | 7.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.52     |
| episodes                | 146000   |
| lives                   | 146000   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 967237   |
| value_loss              | 6.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0377  |
| entropy                 | 2.5      |
| episodes                | 146100   |
| lives                   | 146100   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 967871   |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.57     |
| episodes                | 146200   |
| lives                   | 146200   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 968583   |
| value_loss              | 6.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0218  |
| entropy                 | 2.57     |
| episodes                | 146300   |
| lives                   | 146300   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0036   |
| steps                   | 969298   |
| value_loss              | 7.5      |
--------------------------------------
Saving model due to running mean reward increase: 6.9445 -> 6.9591
--------------------------------------
| approx_kl               | -0.0165  |
| entropy                 | 2.58     |
| episodes                | 146400   |
| lives                   | 146400   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 970020   |
| value_loss              | 7.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.59     |
| episodes                | 146500   |
| lives                   | 146500   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 970708   |
| value_loss              | 7.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.025   |
| entropy                 | 2.61     |
| episodes                | 146600   |
| lives                   | 146600   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 971458   |
| value_loss              | 6.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0235  |
| entropy                 | 2.62     |
| episodes                | 146700   |
| lives                   | 146700   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 972216   |
| value_loss              | 7.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0413  |
| entropy                 | 2.6      |
| episodes                | 146800   |
| lives                   | 146800   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 972947   |
| value_loss              | 7.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0251  |
| entropy                 | 2.63     |
| episodes                | 146900   |
| lives                   | 146900   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 973725   |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0273  |
| entropy                 | 2.54     |
| episodes                | 147000   |
| lives                   | 147000   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0       |
| steps                   | 974461   |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0343  |
| entropy                 | 2.55     |
| episodes                | 147100   |
| lives                   | 147100   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 975172   |
| value_loss              | 7.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0486  |
| entropy                 | 2.59     |
| episodes                | 147200   |
| lives                   | 147200   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 7.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0031   |
| steps                   | 975925   |
| value_loss              | 7.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0562  |
| entropy                 | 2.57     |
| episodes                | 147300   |
| lives                   | 147300   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0111   |
| steps                   | 976574   |
| value_loss              | 8.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0516  |
| entropy                 | 2.51     |
| episodes                | 147400   |
| lives                   | 147400   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 977226   |
| value_loss              | 7.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0506  |
| entropy                 | 2.46     |
| episodes                | 147500   |
| lives                   | 147500   |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 977840   |
| value_loss              | 7.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0335  |
| entropy                 | 2.49     |
| episodes                | 147600   |
| lives                   | 147600   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0089   |
| steps                   | 978533   |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0312  |
| entropy                 | 2.49     |
| episodes                | 147700   |
| lives                   | 147700   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 979240   |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0312  |
| entropy                 | 2.4      |
| episodes                | 147800   |
| lives                   | 147800   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.003    |
| steps                   | 979913   |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0144  |
| entropy                 | 2.53     |
| episodes                | 147900   |
| lives                   | 147900   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 980611   |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.63     |
| episodes                | 148000   |
| lives                   | 148000   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 981388   |
| value_loss              | 7.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0195  |
| entropy                 | 2.55     |
| episodes                | 148100   |
| lives                   | 148100   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 982120   |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0378  |
| entropy                 | 2.48     |
| episodes                | 148200   |
| lives                   | 148200   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 982822   |
| value_loss              | 7.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0081  |
| entropy                 | 2.47     |
| episodes                | 148300   |
| lives                   | 148300   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.026   |
| steps                   | 983522   |
| value_loss              | 7.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.51     |
| episodes                | 148400   |
| lives                   | 148400   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 984192   |
| value_loss              | 7.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0435  |
| entropy                 | 2.56     |
| episodes                | 148500   |
| lives                   | 148500   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 984805   |
| value_loss              | 6.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0504  |
| entropy                 | 2.5      |
| episodes                | 148600   |
| lives                   | 148600   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 985421   |
| value_loss              | 7.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.08    |
| entropy                 | 2.56     |
| episodes                | 148700   |
| lives                   | 148700   |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 986042   |
| value_loss              | 6.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0546  |
| entropy                 | 2.63     |
| episodes                | 148800   |
| lives                   | 148800   |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 986699   |
| value_loss              | 7.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0575  |
| entropy                 | 2.63     |
| episodes                | 148900   |
| lives                   | 148900   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0021   |
| steps                   | 987397   |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0701  |
| entropy                 | 2.65     |
| episodes                | 149000   |
| lives                   | 149000   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 988070   |
| value_loss              | 7.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0693  |
| entropy                 | 2.56     |
| episodes                | 149100   |
| lives                   | 149100   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 988697   |
| value_loss              | 7.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0397  |
| entropy                 | 2.55     |
| episodes                | 149200   |
| lives                   | 149200   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 989367   |
| value_loss              | 6.92     |
--------------------------------------
Saving model due to mean reward increase: 6.9772 -> 7.0225
Saving model due to running mean reward increase: 6.4603 -> 7.0225
--------------------------------------
| approx_kl               | -0.0301  |
| entropy                 | 2.6      |
| episodes                | 149300   |
| lives                   | 149300   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 7.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 990107   |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0298  |
| entropy                 | 2.53     |
| episodes                | 149400   |
| lives                   | 149400   |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 990718   |
| value_loss              | 7.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0188  |
| entropy                 | 2.65     |
| episodes                | 149500   |
| lives                   | 149500   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 991429   |
| value_loss              | 8.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0425  |
| entropy                 | 2.61     |
| episodes                | 149600   |
| lives                   | 149600   |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 992091   |
| value_loss              | 8.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0241  |
| entropy                 | 2.55     |
| episodes                | 149700   |
| lives                   | 149700   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 992825   |
| value_loss              | 8.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.51     |
| episodes                | 149800   |
| lives                   | 149800   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 993484   |
| value_loss              | 7.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.028   |
| entropy                 | 2.48     |
| episodes                | 149900   |
| lives                   | 149900   |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 994146   |
| value_loss              | 7.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0338  |
| entropy                 | 2.55     |
| episodes                | 150000   |
| lives                   | 150000   |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 994815   |
| value_loss              | 7.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.52     |
| episodes                | 150100   |
| lives                   | 150100   |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 995481   |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0281  |
| entropy                 | 2.5      |
| episodes                | 150200   |
| lives                   | 150200   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 996145   |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0543  |
| entropy                 | 2.51     |
| episodes                | 150300   |
| lives                   | 150300   |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 996768   |
| value_loss              | 7.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0719  |
| entropy                 | 2.54     |
| episodes                | 150400   |
| lives                   | 150400   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 997410   |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0519  |
| entropy                 | 2.51     |
| episodes                | 150500   |
| lives                   | 150500   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 998037   |
| value_loss              | 7.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0573  |
| entropy                 | 2.54     |
| episodes                | 150600   |
| lives                   | 150600   |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 998670   |
| value_loss              | 8.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0805  |
| entropy                 | 2.53     |
| episodes                | 150700   |
| lives                   | 150700   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 999331   |
| value_loss              | 7.61     |
--------------------------------------
Saving model due to running mean reward increase: 6.128 -> 6.1567
--------------------------------------
| approx_kl               | -0.0625  |
| entropy                 | 2.55     |
| episodes                | 150800   |
| lives                   | 150800   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1000009  |
| value_loss              | 7.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0185  |
| entropy                 | 2.61     |
| episodes                | 150900   |
| lives                   | 150900   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1000744  |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.034   |
| entropy                 | 2.52     |
| episodes                | 151000   |
| lives                   | 151000   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 1001405  |
| value_loss              | 7.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0371  |
| entropy                 | 2.51     |
| episodes                | 151100   |
| lives                   | 151100   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1002069  |
| value_loss              | 7.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0395  |
| entropy                 | 2.58     |
| episodes                | 151200   |
| lives                   | 151200   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 1002795  |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0457  |
| entropy                 | 2.52     |
| episodes                | 151300   |
| lives                   | 151300   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1003489  |
| value_loss              | 7.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0556  |
| entropy                 | 2.44     |
| episodes                | 151400   |
| lives                   | 151400   |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 1004118  |
| value_loss              | 8.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0548  |
| entropy                 | 2.5      |
| episodes                | 151500   |
| lives                   | 151500   |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 1004737  |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0551  |
| entropy                 | 2.49     |
| episodes                | 151600   |
| lives                   | 151600   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1005430  |
| value_loss              | 8        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0414  |
| entropy                 | 2.46     |
| episodes                | 151700   |
| lives                   | 151700   |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0218  |
| steps                   | 1006065  |
| value_loss              | 7.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0576  |
| entropy                 | 2.5      |
| episodes                | 151800   |
| lives                   | 151800   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1006659  |
| value_loss              | 7.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0596  |
| entropy                 | 2.56     |
| episodes                | 151900   |
| lives                   | 151900   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 1007298  |
| value_loss              | 7.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.069   |
| entropy                 | 2.42     |
| episodes                | 152000   |
| lives                   | 152000   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1007878  |
| value_loss              | 8.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.058   |
| entropy                 | 2.58     |
| episodes                | 152100   |
| lives                   | 152100   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1008608  |
| value_loss              | 7.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0478  |
| entropy                 | 2.49     |
| episodes                | 152200   |
| lives                   | 152200   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1009269  |
| value_loss              | 7.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0433  |
| entropy                 | 2.56     |
| episodes                | 152300   |
| lives                   | 152300   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 1009982  |
| value_loss              | 7.29     |
--------------------------------------
Saving model due to running mean reward increase: 6.4092 -> 6.5583
--------------------------------------
| approx_kl               | -0.0458  |
| entropy                 | 2.46     |
| episodes                | 152400   |
| lives                   | 152400   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1010683  |
| value_loss              | 7.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.068   |
| entropy                 | 2.42     |
| episodes                | 152500   |
| lives                   | 152500   |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 1011287  |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.059   |
| entropy                 | 2.53     |
| episodes                | 152600   |
| lives                   | 152600   |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 1011949  |
| value_loss              | 7.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0646  |
| entropy                 | 2.5      |
| episodes                | 152700   |
| lives                   | 152700   |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1012611  |
| value_loss              | 7.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0488  |
| entropy                 | 2.62     |
| episodes                | 152800   |
| lives                   | 152800   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1013355  |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0554  |
| entropy                 | 2.54     |
| episodes                | 152900   |
| lives                   | 152900   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0023   |
| steps                   | 1014003  |
| value_loss              | 7.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0459  |
| entropy                 | 2.55     |
| episodes                | 153000   |
| lives                   | 153000   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0001   |
| steps                   | 1014679  |
| value_loss              | 6.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0279  |
| entropy                 | 2.57     |
| episodes                | 153100   |
| lives                   | 153100   |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1015319  |
| value_loss              | 6.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0433  |
| entropy                 | 2.55     |
| episodes                | 153200   |
| lives                   | 153200   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0019   |
| steps                   | 1016004  |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0551  |
| entropy                 | 2.57     |
| episodes                | 153300   |
| lives                   | 153300   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1016692  |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0394  |
| entropy                 | 2.59     |
| episodes                | 153400   |
| lives                   | 153400   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1017415  |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0443  |
| entropy                 | 2.61     |
| episodes                | 153500   |
| lives                   | 153500   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0049   |
| steps                   | 1018145  |
| value_loss              | 7.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0706  |
| entropy                 | 2.51     |
| episodes                | 153600   |
| lives                   | 153600   |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0147   |
| steps                   | 1018753  |
| value_loss              | 7.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0789  |
| entropy                 | 2.59     |
| episodes                | 153700   |
| lives                   | 153700   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1019358  |
| value_loss              | 7.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0781  |
| entropy                 | 2.54     |
| episodes                | 153800   |
| lives                   | 153800   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 1019989  |
| value_loss              | 7.26     |
--------------------------------------
Saving model due to running mean reward increase: 5.0825 -> 6.0526
--------------------------------------
| approx_kl               | -0.0711  |
| entropy                 | 2.45     |
| episodes                | 153900   |
| lives                   | 153900   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0222  |
| steps                   | 1020616  |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0763  |
| entropy                 | 2.55     |
| episodes                | 154000   |
| lives                   | 154000   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.004    |
| steps                   | 1021301  |
| value_loss              | 7.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0639  |
| entropy                 | 2.63     |
| episodes                | 154100   |
| lives                   | 154100   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0046   |
| steps                   | 1022046  |
| value_loss              | 7.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0245  |
| entropy                 | 2.59     |
| episodes                | 154200   |
| lives                   | 154200   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1022867  |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0281  |
| entropy                 | 2.55     |
| episodes                | 154300   |
| lives                   | 154300   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1023591  |
| value_loss              | 6.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0321  |
| entropy                 | 2.49     |
| episodes                | 154400   |
| lives                   | 154400   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 1024243  |
| value_loss              | 7.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0701  |
| entropy                 | 2.56     |
| episodes                | 154500   |
| lives                   | 154500   |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 1024888  |
| value_loss              | 7.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0428  |
| entropy                 | 2.52     |
| episodes                | 154600   |
| lives                   | 154600   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 1025548  |
| value_loss              | 7.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0642  |
| entropy                 | 2.46     |
| episodes                | 154700   |
| lives                   | 154700   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 1026161  |
| value_loss              | 7.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0721  |
| entropy                 | 2.52     |
| episodes                | 154800   |
| lives                   | 154800   |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1026787  |
| value_loss              | 7.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.073   |
| entropy                 | 2.55     |
| episodes                | 154900   |
| lives                   | 154900   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0012   |
| steps                   | 1027463  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0593  |
| entropy                 | 2.5      |
| episodes                | 155000   |
| lives                   | 155000   |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 1028099  |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0667  |
| entropy                 | 2.54     |
| episodes                | 155100   |
| lives                   | 155100   |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1028743  |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0782  |
| entropy                 | 2.51     |
| episodes                | 155200   |
| lives                   | 155200   |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1029357  |
| value_loss              | 7.39     |
--------------------------------------
Saving model due to running mean reward increase: 5.8054 -> 6.3439
--------------------------------------
| approx_kl               | -0.0512  |
| entropy                 | 2.57     |
| episodes                | 155300   |
| lives                   | 155300   |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1030038  |
| value_loss              | 7.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0483  |
| entropy                 | 2.59     |
| episodes                | 155400   |
| lives                   | 155400   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 1030690  |
| value_loss              | 7.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0587  |
| entropy                 | 2.52     |
| episodes                | 155500   |
| lives                   | 155500   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1031306  |
| value_loss              | 8.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0651  |
| entropy                 | 2.55     |
| episodes                | 155600   |
| lives                   | 155600   |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1031934  |
| value_loss              | 8.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0649  |
| entropy                 | 2.54     |
| episodes                | 155700   |
| lives                   | 155700   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1032533  |
| value_loss              | 7.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0683  |
| entropy                 | 2.6      |
| episodes                | 155800   |
| lives                   | 155800   |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1033210  |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0271  |
| entropy                 | 2.58     |
| episodes                | 155900   |
| lives                   | 155900   |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1033887  |
| value_loss              | 7.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0241  |
| entropy                 | 2.52     |
| episodes                | 156000   |
| lives                   | 156000   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 1034591  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0337  |
| entropy                 | 2.43     |
| episodes                | 156100   |
| lives                   | 156100   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1035258  |
| value_loss              | 7.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.46     |
| episodes                | 156200   |
| lives                   | 156200   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 1035945  |
| value_loss              | 7.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.028   |
| entropy                 | 2.47     |
| episodes                | 156300   |
| lives                   | 156300   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 1036597  |
| value_loss              | 7.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0249  |
| entropy                 | 2.52     |
| episodes                | 156400   |
| lives                   | 156400   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1037318  |
| value_loss              | 7.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.5      |
| episodes                | 156500   |
| lives                   | 156500   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 1038009  |
| value_loss              | 7.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.52     |
| episodes                | 156600   |
| lives                   | 156600   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0155   |
| steps                   | 1038699  |
| value_loss              | 7.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0448  |
| entropy                 | 2.45     |
| episodes                | 156700   |
| lives                   | 156700   |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1039320  |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0414  |
| entropy                 | 2.45     |
| episodes                | 156800   |
| lives                   | 156800   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 1039919  |
| value_loss              | 7.3      |
--------------------------------------
Saving model due to running mean reward increase: 5.2367 -> 5.8928
--------------------------------------
| approx_kl               | -0.0354  |
| entropy                 | 2.54     |
| episodes                | 156900   |
| lives                   | 156900   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1040587  |
| value_loss              | 7.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0401  |
| entropy                 | 2.57     |
| episodes                | 157000   |
| lives                   | 157000   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1041296  |
| value_loss              | 7.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0496  |
| entropy                 | 2.55     |
| episodes                | 157100   |
| lives                   | 157100   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1041990  |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0411  |
| entropy                 | 2.54     |
| episodes                | 157200   |
| lives                   | 157200   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0005   |
| steps                   | 1042747  |
| value_loss              | 8        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0402  |
| entropy                 | 2.46     |
| episodes                | 157300   |
| lives                   | 157300   |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 1043429  |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0556  |
| entropy                 | 2.37     |
| episodes                | 157400   |
| lives                   | 157400   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1043995  |
| value_loss              | 7.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0445  |
| entropy                 | 2.43     |
| episodes                | 157500   |
| lives                   | 157500   |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 1044601  |
| value_loss              | 7.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0296  |
| entropy                 | 2.51     |
| episodes                | 157600   |
| lives                   | 157600   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1045250  |
| value_loss              | 7.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0655  |
| entropy                 | 2.42     |
| episodes                | 157700   |
| lives                   | 157700   |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1045876  |
| value_loss              | 8.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.48     |
| episodes                | 157800   |
| lives                   | 157800   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 1046527  |
| value_loss              | 7.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0433  |
| entropy                 | 2.41     |
| episodes                | 157900   |
| lives                   | 157900   |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 1047171  |
| value_loss              | 7.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.075   |
| entropy                 | 2.37     |
| episodes                | 158000   |
| lives                   | 158000   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 1047776  |
| value_loss              | 8.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0611  |
| entropy                 | 2.37     |
| episodes                | 158100   |
| lives                   | 158100   |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 1048382  |
| value_loss              | 9.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0717  |
| entropy                 | 2.36     |
| episodes                | 158200   |
| lives                   | 158200   |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 1048954  |
| value_loss              | 8.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0559  |
| entropy                 | 2.5      |
| episodes                | 158300   |
| lives                   | 158300   |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 1049582  |
| value_loss              | 7.71     |
--------------------------------------
Saving model due to running mean reward increase: 5.8734 -> 6.427
--------------------------------------
| approx_kl               | -0.0761  |
| entropy                 | 2.55     |
| episodes                | 158400   |
| lives                   | 158400   |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 1050240  |
| value_loss              | 8.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0844  |
| entropy                 | 2.56     |
| episodes                | 158500   |
| lives                   | 158500   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 1050911  |
| value_loss              | 7.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.056   |
| entropy                 | 2.56     |
| episodes                | 158600   |
| lives                   | 158600   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1051647  |
| value_loss              | 7.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0642  |
| entropy                 | 2.45     |
| episodes                | 158700   |
| lives                   | 158700   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0121   |
| steps                   | 1052339  |
| value_loss              | 8.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0384  |
| entropy                 | 2.52     |
| episodes                | 158800   |
| lives                   | 158800   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1053067  |
| value_loss              | 7.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0589  |
| entropy                 | 2.42     |
| episodes                | 158900   |
| lives                   | 158900   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 1053719  |
| value_loss              | 8.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0465  |
| entropy                 | 2.45     |
| episodes                | 159000   |
| lives                   | 159000   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0047   |
| steps                   | 1054417  |
| value_loss              | 7.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0501  |
| entropy                 | 2.48     |
| episodes                | 159100   |
| lives                   | 159100   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0021   |
| steps                   | 1055118  |
| value_loss              | 7.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0577  |
| entropy                 | 2.53     |
| episodes                | 159200   |
| lives                   | 159200   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 7.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0014   |
| steps                   | 1055869  |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0327  |
| entropy                 | 2.46     |
| episodes                | 159300   |
| lives                   | 159300   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1056563  |
| value_loss              | 7.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.51     |
| episodes                | 159400   |
| lives                   | 159400   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1057283  |
| value_loss              | 7.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0323  |
| entropy                 | 2.52     |
| episodes                | 159500   |
| lives                   | 159500   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 7.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1058035  |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0318  |
| entropy                 | 2.54     |
| episodes                | 159600   |
| lives                   | 159600   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0036   |
| steps                   | 1058774  |
| value_loss              | 6.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0138  |
| entropy                 | 2.58     |
| episodes                | 159700   |
| lives                   | 159700   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 7.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 1059575  |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.54     |
| episodes                | 159800   |
| lives                   | 159800   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1060308  |
| value_loss              | 6.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0237  |
| entropy                 | 2.57     |
| episodes                | 159900   |
| lives                   | 159900   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 7.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0006   |
| steps                   | 1061087  |
| value_loss              | 7.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0266  |
| entropy                 | 2.55     |
| episodes                | 160000   |
| lives                   | 160000   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1061840  |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0311  |
| entropy                 | 2.57     |
| episodes                | 160100   |
| lives                   | 160100   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1062572  |
| value_loss              | 6.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.118   |
| entropy                 | 2.43     |
| episodes                | 160200   |
| lives                   | 160200   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0266   |
| steps                   | 1063262  |
| value_loss              | 6.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0859  |
| entropy                 | 2.53     |
| episodes                | 160300   |
| lives                   | 160300   |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.032    |
| steps                   | 1063868  |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0823  |
| entropy                 | 2.55     |
| episodes                | 160400   |
| lives                   | 160400   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 1064519  |
| value_loss              | 8.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0655  |
| entropy                 | 2.5      |
| episodes                | 160500   |
| lives                   | 160500   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1065131  |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0572  |
| entropy                 | 2.57     |
| episodes                | 160600   |
| lives                   | 160600   |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 1065778  |
| value_loss              | 7.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.053   |
| entropy                 | 2.61     |
| episodes                | 160700   |
| lives                   | 160700   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 1066461  |
| value_loss              | 7.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0491  |
| entropy                 | 2.58     |
| episodes                | 160800   |
| lives                   | 160800   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 1067137  |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.06    |
| entropy                 | 2.61     |
| episodes                | 160900   |
| lives                   | 160900   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1067850  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.69     |
| episodes                | 161000   |
| lives                   | 161000   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1068619  |
| value_loss              | 7.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0492  |
| entropy                 | 2.57     |
| episodes                | 161100   |
| lives                   | 161100   |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1069272  |
| value_loss              | 8.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0409  |
| entropy                 | 2.62     |
| episodes                | 161200   |
| lives                   | 161200   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1069975  |
| value_loss              | 7.28     |
--------------------------------------
Saving model due to running mean reward increase: 6.0103 -> 6.6429
--------------------------------------
| approx_kl               | -0.0497  |
| entropy                 | 2.56     |
| episodes                | 161300   |
| lives                   | 161300   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 1070630  |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.55     |
| episodes                | 161400   |
| lives                   | 161400   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0074   |
| steps                   | 1071315  |
| value_loss              | 7.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0552  |
| entropy                 | 2.48     |
| episodes                | 161500   |
| lives                   | 161500   |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 1071972  |
| value_loss              | 7.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.5      |
| episodes                | 161600   |
| lives                   | 161600   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1072660  |
| value_loss              | 7.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0609  |
| entropy                 | 2.5      |
| episodes                | 161700   |
| lives                   | 161700   |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0116   |
| steps                   | 1073326  |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0256  |
| entropy                 | 2.52     |
| episodes                | 161800   |
| lives                   | 161800   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1074056  |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0287  |
| entropy                 | 2.51     |
| episodes                | 161900   |
| lives                   | 161900   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 1074780  |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0228  |
| entropy                 | 2.52     |
| episodes                | 162000   |
| lives                   | 162000   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 7.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1075526  |
| value_loss              | 7.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.45     |
| episodes                | 162100   |
| lives                   | 162100   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 1076252  |
| value_loss              | 6.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0564  |
| entropy                 | 2.42     |
| episodes                | 162200   |
| lives                   | 162200   |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0075   |
| steps                   | 1076897  |
| value_loss              | 6.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0405  |
| entropy                 | 2.33     |
| episodes                | 162300   |
| lives                   | 162300   |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1077459  |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0618  |
| entropy                 | 2.43     |
| episodes                | 162400   |
| lives                   | 162400   |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1078042  |
| value_loss              | 8.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0799  |
| entropy                 | 2.45     |
| episodes                | 162500   |
| lives                   | 162500   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 1078613  |
| value_loss              | 7.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0836  |
| entropy                 | 2.5      |
| episodes                | 162600   |
| lives                   | 162600   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1079228  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.063   |
| entropy                 | 2.52     |
| episodes                | 162700   |
| lives                   | 162700   |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1079849  |
| value_loss              | 7.97     |
--------------------------------------
Saving model due to running mean reward increase: 5.8828 -> 6.6107
--------------------------------------
| approx_kl               | -0.064   |
| entropy                 | 2.54     |
| episodes                | 162800   |
| lives                   | 162800   |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1080478  |
| value_loss              | 7.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0327  |
| entropy                 | 2.63     |
| episodes                | 162900   |
| lives                   | 162900   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1081221  |
| value_loss              | 7.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.63     |
| episodes                | 163000   |
| lives                   | 163000   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1081973  |
| value_loss              | 7.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0431  |
| entropy                 | 2.55     |
| episodes                | 163100   |
| lives                   | 163100   |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1082642  |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0514  |
| entropy                 | 2.65     |
| episodes                | 163200   |
| lives                   | 163200   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1083359  |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.58     |
| episodes                | 163300   |
| lives                   | 163300   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1084091  |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0729  |
| entropy                 | 2.45     |
| episodes                | 163400   |
| lives                   | 163400   |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1084713  |
| value_loss              | 6.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0715  |
| entropy                 | 2.55     |
| episodes                | 163500   |
| lives                   | 163500   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0117   |
| steps                   | 1085424  |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.035   |
| entropy                 | 2.62     |
| episodes                | 163600   |
| lives                   | 163600   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1086119  |
| value_loss              | 7.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0564  |
| entropy                 | 2.61     |
| episodes                | 163700   |
| lives                   | 163700   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 1086783  |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0392  |
| entropy                 | 2.58     |
| episodes                | 163800   |
| lives                   | 163800   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1087462  |
| value_loss              | 7.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0511  |
| entropy                 | 2.58     |
| episodes                | 163900   |
| lives                   | 163900   |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1088088  |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0699  |
| entropy                 | 2.59     |
| episodes                | 164000   |
| lives                   | 164000   |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 1088706  |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0434  |
| entropy                 | 2.64     |
| episodes                | 164100   |
| lives                   | 164100   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 1089419  |
| value_loss              | 6.9      |
--------------------------------------
Saving model due to mean reward increase: 7.0225 -> 7.1166
Saving model due to running mean reward increase: 6.5349 -> 7.1166
--------------------------------------
| approx_kl               | -0.039   |
| entropy                 | 2.57     |
| episodes                | 164200   |
| lives                   | 164200   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 7.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1090161  |
| value_loss              | 7.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0179  |
| entropy                 | 2.51     |
| episodes                | 164300   |
| lives                   | 164300   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 1090849  |
| value_loss              | 7.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0476  |
| entropy                 | 2.55     |
| episodes                | 164400   |
| lives                   | 164400   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 1091544  |
| value_loss              | 6.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0528  |
| entropy                 | 2.59     |
| episodes                | 164500   |
| lives                   | 164500   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 1092256  |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.039   |
| entropy                 | 2.54     |
| episodes                | 164600   |
| lives                   | 164600   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 1092980  |
| value_loss              | 6.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0811  |
| entropy                 | 2.5      |
| episodes                | 164700   |
| lives                   | 164700   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 1093648  |
| value_loss              | 6.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0968  |
| entropy                 | 2.52     |
| episodes                | 164800   |
| lives                   | 164800   |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0334   |
| steps                   | 1094329  |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0246  |
| entropy                 | 2.6      |
| episodes                | 164900   |
| lives                   | 164900   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1095105  |
| value_loss              | 6        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.018   |
| entropy                 | 2.49     |
| episodes                | 165000   |
| lives                   | 165000   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1095836  |
| value_loss              | 6.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0127  |
| entropy                 | 2.47     |
| episodes                | 165100   |
| lives                   | 165100   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1096598  |
| value_loss              | 5.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.024   |
| entropy                 | 2.49     |
| episodes                | 165200   |
| lives                   | 165200   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 7.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1097392  |
| value_loss              | 6.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.45     |
| episodes                | 165300   |
| lives                   | 165300   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 7.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1098119  |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0409  |
| entropy                 | 2.44     |
| episodes                | 165400   |
| lives                   | 165400   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 1098846  |
| value_loss              | 6.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0119  |
| entropy                 | 2.47     |
| episodes                | 165500   |
| lives                   | 165500   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 7.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 1099630  |
| value_loss              | 6.39     |
--------------------------------------
Saving model due to running mean reward increase: 6.7031 -> 7.0615
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.47     |
| episodes                | 165600   |
| lives                   | 165600   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 6.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1100385  |
| value_loss              | 6.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0179  |
| entropy                 | 2.47     |
| episodes                | 165700   |
| lives                   | 165700   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 7.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1101131  |
| value_loss              | 6.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.54     |
| episodes                | 165800   |
| lives                   | 165800   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1101908  |
| value_loss              | 6.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0387  |
| entropy                 | 2.5      |
| episodes                | 165900   |
| lives                   | 165900   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1102660  |
| value_loss              | 6.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0451  |
| entropy                 | 2.47     |
| episodes                | 166000   |
| lives                   | 166000   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 7.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 1103382  |
| value_loss              | 6.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0449  |
| entropy                 | 2.44     |
| episodes                | 166100   |
| lives                   | 166100   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 1104099  |
| value_loss              | 6.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0211  |
| entropy                 | 2.56     |
| episodes                | 166200   |
| lives                   | 166200   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 1104860  |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0188  |
| entropy                 | 2.48     |
| episodes                | 166300   |
| lives                   | 166300   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 1105592  |
| value_loss              | 6.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.43     |
| episodes                | 166400   |
| lives                   | 166400   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 7.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1106338  |
| value_loss              | 6.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0447  |
| entropy                 | 2.41     |
| episodes                | 166500   |
| lives                   | 166500   |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1106948  |
| value_loss              | 6.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0843  |
| entropy                 | 2.48     |
| episodes                | 166600   |
| lives                   | 166600   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 1107515  |
| value_loss              | 7.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0578  |
| entropy                 | 2.51     |
| episodes                | 166700   |
| lives                   | 166700   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1108128  |
| value_loss              | 6.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0412  |
| entropy                 | 2.47     |
| episodes                | 166800   |
| lives                   | 166800   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1108709  |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0668  |
| entropy                 | 2.44     |
| episodes                | 166900   |
| lives                   | 166900   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1109321  |
| value_loss              | 6.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0856  |
| entropy                 | 2.29     |
| episodes                | 167000   |
| lives                   | 167000   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1109884  |
| value_loss              | 6.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0644  |
| entropy                 | 2.36     |
| episodes                | 167100   |
| lives                   | 167100   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1110451  |
| value_loss              | 6.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0777  |
| entropy                 | 2.39     |
| episodes                | 167200   |
| lives                   | 167200   |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0205  |
| steps                   | 1111008  |
| value_loss              | 6.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0426  |
| entropy                 | 2.46     |
| episodes                | 167300   |
| lives                   | 167300   |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0011   |
| steps                   | 1111604  |
| value_loss              | 7.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0723  |
| entropy                 | 2.33     |
| episodes                | 167400   |
| lives                   | 167400   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 1112170  |
| value_loss              | 7.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0983  |
| entropy                 | 2.34     |
| episodes                | 167500   |
| lives                   | 167500   |
| mean 100 episode length | 6.48     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1112718  |
| value_loss              | 7.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.085   |
| entropy                 | 2.39     |
| episodes                | 167600   |
| lives                   | 167600   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 1113298  |
| value_loss              | 7.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.46     |
| episodes                | 167700   |
| lives                   | 167700   |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1113882  |
| value_loss              | 7.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.109   |
| entropy                 | 2.44     |
| episodes                | 167800   |
| lives                   | 167800   |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1114460  |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.096   |
| entropy                 | 2.53     |
| episodes                | 167900   |
| lives                   | 167900   |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1115063  |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.46     |
| episodes                | 168000   |
| lives                   | 168000   |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1115637  |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.111   |
| entropy                 | 2.52     |
| episodes                | 168100   |
| lives                   | 168100   |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1116240  |
| value_loss              | 7.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.45     |
| episodes                | 168200   |
| lives                   | 168200   |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 1116804  |
| value_loss              | 7.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.082   |
| entropy                 | 2.54     |
| episodes                | 168300   |
| lives                   | 168300   |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 1117433  |
| value_loss              | 7.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0603  |
| entropy                 | 2.58     |
| episodes                | 168400   |
| lives                   | 168400   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 1118060  |
| value_loss              | 7.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0622  |
| entropy                 | 2.58     |
| episodes                | 168500   |
| lives                   | 168500   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0324   |
| steps                   | 1118744  |
| value_loss              | 6.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0195  |
| entropy                 | 2.54     |
| episodes                | 168600   |
| lives                   | 168600   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1119497  |
| value_loss              | 7.11     |
--------------------------------------
Saving model due to running mean reward increase: 6.0132 -> 6.7593
--------------------------------------
| approx_kl               | -0.0184  |
| entropy                 | 2.55     |
| episodes                | 168700   |
| lives                   | 168700   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1120251  |
| value_loss              | 7        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0221  |
| entropy                 | 2.47     |
| episodes                | 168800   |
| lives                   | 168800   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 1120994  |
| value_loss              | 6.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.48     |
| episodes                | 168900   |
| lives                   | 168900   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1121714  |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0364  |
| entropy                 | 2.55     |
| episodes                | 169000   |
| lives                   | 169000   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 7.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1122485  |
| value_loss              | 7.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0275  |
| entropy                 | 2.51     |
| episodes                | 169100   |
| lives                   | 169100   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1123245  |
| value_loss              | 6.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0178  |
| entropy                 | 2.51     |
| episodes                | 169200   |
| lives                   | 169200   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 1123976  |
| value_loss              | 6.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0268  |
| entropy                 | 2.49     |
| episodes                | 169300   |
| lives                   | 169300   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0008   |
| steps                   | 1124676  |
| value_loss              | 6.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0515  |
| entropy                 | 2.5      |
| episodes                | 169400   |
| lives                   | 169400   |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1125272  |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0753  |
| entropy                 | 2.52     |
| episodes                | 169500   |
| lives                   | 169500   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 1125877  |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0421  |
| entropy                 | 2.44     |
| episodes                | 169600   |
| lives                   | 169600   |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0085   |
| steps                   | 1126488  |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.32     |
| episodes                | 169700   |
| lives                   | 169700   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1127076  |
| value_loss              | 6.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0376  |
| entropy                 | 2.42     |
| episodes                | 169800   |
| lives                   | 169800   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0156  |
| steps                   | 1127715  |
| value_loss              | 7.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0945  |
| entropy                 | 2.34     |
| episodes                | 169900   |
| lives                   | 169900   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 1128266  |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0653  |
| entropy                 | 2.49     |
| episodes                | 170000   |
| lives                   | 170000   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0172  |
| steps                   | 1128878  |
| value_loss              | 6.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0774  |
| entropy                 | 2.49     |
| episodes                | 170100   |
| lives                   | 170100   |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1129502  |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0546  |
| entropy                 | 2.58     |
| episodes                | 170200   |
| lives                   | 170200   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 1130166  |
| value_loss              | 5.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0735  |
| entropy                 | 2.57     |
| episodes                | 170300   |
| lives                   | 170300   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 1130857  |
| value_loss              | 6.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0578  |
| entropy                 | 2.61     |
| episodes                | 170400   |
| lives                   | 170400   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1131551  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0448  |
| entropy                 | 2.54     |
| episodes                | 170500   |
| lives                   | 170500   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1132246  |
| value_loss              | 6.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0583  |
| entropy                 | 2.57     |
| episodes                | 170600   |
| lives                   | 170600   |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 1132893  |
| value_loss              | 6.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0616  |
| entropy                 | 2.43     |
| episodes                | 170700   |
| lives                   | 170700   |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 1133549  |
| value_loss              | 6.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0841  |
| entropy                 | 2.26     |
| episodes                | 170800   |
| lives                   | 170800   |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0204  |
| steps                   | 1134083  |
| value_loss              | 7.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0921  |
| entropy                 | 2.44     |
| episodes                | 170900   |
| lives                   | 170900   |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1134667  |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.116   |
| entropy                 | 2.4      |
| episodes                | 171000   |
| lives                   | 171000   |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1135235  |
| value_loss              | 6.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.135   |
| entropy                 | 2.46     |
| episodes                | 171100   |
| lives                   | 171100   |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 1135812  |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.085   |
| entropy                 | 2.49     |
| episodes                | 171200   |
| lives                   | 171200   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1136398  |
| value_loss              | 6.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.105   |
| entropy                 | 2.61     |
| episodes                | 171300   |
| lives                   | 171300   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1137058  |
| value_loss              | 6.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0734  |
| entropy                 | 2.56     |
| episodes                | 171400   |
| lives                   | 171400   |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 1137693  |
| value_loss              | 5.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0844  |
| entropy                 | 2.47     |
| episodes                | 171500   |
| lives                   | 171500   |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 1138268  |
| value_loss              | 6.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0729  |
| entropy                 | 2.52     |
| episodes                | 171600   |
| lives                   | 171600   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1138880  |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0657  |
| entropy                 | 2.5      |
| episodes                | 171700   |
| lives                   | 171700   |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 1139490  |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0599  |
| entropy                 | 2.36     |
| episodes                | 171800   |
| lives                   | 171800   |
| mean 100 episode length | 6.55     |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0187  |
| steps                   | 1140045  |
| value_loss              | 6.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0894  |
| entropy                 | 2.49     |
| episodes                | 171900   |
| lives                   | 171900   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1140626  |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0505  |
| entropy                 | 2.53     |
| episodes                | 172000   |
| lives                   | 172000   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1141239  |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0523  |
| entropy                 | 2.62     |
| episodes                | 172100   |
| lives                   | 172100   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1141930  |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0671  |
| entropy                 | 2.65     |
| episodes                | 172200   |
| lives                   | 172200   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1142636  |
| value_loss              | 6.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0445  |
| entropy                 | 2.67     |
| episodes                | 172300   |
| lives                   | 172300   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0046   |
| steps                   | 1143374  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0361  |
| entropy                 | 2.62     |
| episodes                | 172400   |
| lives                   | 172400   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1144136  |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.55     |
| episodes                | 172500   |
| lives                   | 172500   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 6.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 1144906  |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0607  |
| entropy                 | 2.46     |
| episodes                | 172600   |
| lives                   | 172600   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0204   |
| steps                   | 1145571  |
| value_loss              | 5.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.36     |
| episodes                | 172700   |
| lives                   | 172700   |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 1146136  |
| value_loss              | 6.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0703  |
| entropy                 | 2.36     |
| episodes                | 172800   |
| lives                   | 172800   |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 1146727  |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0621  |
| entropy                 | 2.48     |
| episodes                | 172900   |
| lives                   | 172900   |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0187  |
| steps                   | 1147356  |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0716  |
| entropy                 | 2.47     |
| episodes                | 173000   |
| lives                   | 173000   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1147955  |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0836  |
| entropy                 | 2.36     |
| episodes                | 173100   |
| lives                   | 173100   |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1148494  |
| value_loss              | 7.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0686  |
| entropy                 | 2.39     |
| episodes                | 173200   |
| lives                   | 173200   |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 1149070  |
| value_loss              | 7.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0769  |
| entropy                 | 2.54     |
| episodes                | 173300   |
| lives                   | 173300   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1149729  |
| value_loss              | 6.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0695  |
| entropy                 | 2.46     |
| episodes                | 173400   |
| lives                   | 173400   |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1150338  |
| value_loss              | 6.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0684  |
| entropy                 | 2.4      |
| episodes                | 173500   |
| lives                   | 173500   |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1150939  |
| value_loss              | 6.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0595  |
| entropy                 | 2.39     |
| episodes                | 173600   |
| lives                   | 173600   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 1151551  |
| value_loss              | 6.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0691  |
| entropy                 | 2.2      |
| episodes                | 173700   |
| lives                   | 173700   |
| mean 100 episode length | 6.03     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1152054  |
| value_loss              | 7.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0694  |
| entropy                 | 2.26     |
| episodes                | 173800   |
| lives                   | 173800   |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 1152608  |
| value_loss              | 6.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0632  |
| entropy                 | 2.38     |
| episodes                | 173900   |
| lives                   | 173900   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1153250  |
| value_loss              | 7.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0606  |
| entropy                 | 2.4      |
| episodes                | 174000   |
| lives                   | 174000   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0209  |
| steps                   | 1153863  |
| value_loss              | 6.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0233  |
| entropy                 | 2.54     |
| episodes                | 174100   |
| lives                   | 174100   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1154555  |
| value_loss              | 6.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0595  |
| entropy                 | 2.51     |
| episodes                | 174200   |
| lives                   | 174200   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1155193  |
| value_loss              | 6.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0545  |
| entropy                 | 2.52     |
| episodes                | 174300   |
| lives                   | 174300   |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 1155828  |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.04    |
| entropy                 | 2.42     |
| episodes                | 174400   |
| lives                   | 174400   |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0001   |
| steps                   | 1156402  |
| value_loss              | 5.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0953  |
| entropy                 | 2.4      |
| episodes                | 174500   |
| lives                   | 174500   |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1156958  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0458  |
| entropy                 | 2.52     |
| episodes                | 174600   |
| lives                   | 174600   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0006   |
| steps                   | 1157652  |
| value_loss              | 6.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0282  |
| entropy                 | 2.51     |
| episodes                | 174700   |
| lives                   | 174700   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 1158389  |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0095  |
| entropy                 | 2.47     |
| episodes                | 174800   |
| lives                   | 174800   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 1159114  |
| value_loss              | 6.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0097  |
| entropy                 | 2.5      |
| episodes                | 174900   |
| lives                   | 174900   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1159913  |
| value_loss              | 6.23     |
--------------------------------------
Saving model due to running mean reward increase: 6.6124 -> 6.6716
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.47     |
| episodes                | 175000   |
| lives                   | 175000   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 1160663  |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.53     |
| episodes                | 175100   |
| lives                   | 175100   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 1161455  |
| value_loss              | 5.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.46     |
| episodes                | 175200   |
| lives                   | 175200   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1162215  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0206  |
| entropy                 | 2.42     |
| episodes                | 175300   |
| lives                   | 175300   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1162922  |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.48     |
| episodes                | 175400   |
| lives                   | 175400   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 1163682  |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.44     |
| episodes                | 175500   |
| lives                   | 175500   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 7.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1164405  |
| value_loss              | 6.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0161  |
| entropy                 | 2.47     |
| episodes                | 175600   |
| lives                   | 175600   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 1165135  |
| value_loss              | 5.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.018   |
| entropy                 | 2.53     |
| episodes                | 175700   |
| lives                   | 175700   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 7.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1165914  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0133  |
| entropy                 | 2.46     |
| episodes                | 175800   |
| lives                   | 175800   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.004    |
| steps                   | 1166679  |
| value_loss              | 5.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0179  |
| entropy                 | 2.38     |
| episodes                | 175900   |
| lives                   | 175900   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 1167398  |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0312  |
| entropy                 | 2.47     |
| episodes                | 176000   |
| lives                   | 176000   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1168174  |
| value_loss              | 5.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0104  |
| entropy                 | 2.5      |
| episodes                | 176100   |
| lives                   | 176100   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1169009  |
| value_loss              | 5.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0246  |
| entropy                 | 2.49     |
| episodes                | 176200   |
| lives                   | 176200   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0245  |
| steps                   | 1169804  |
| value_loss              | 4.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0642  |
| entropy                 | 2.46     |
| episodes                | 176300   |
| lives                   | 176300   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1170536  |
| value_loss              | 4.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0483  |
| entropy                 | 2.47     |
| episodes                | 176400   |
| lives                   | 176400   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1171282  |
| value_loss              | 5.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.027   |
| entropy                 | 2.48     |
| episodes                | 176500   |
| lives                   | 176500   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0199  |
| steps                   | 1172050  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.43     |
| episodes                | 176600   |
| lives                   | 176600   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0007   |
| steps                   | 1172743  |
| value_loss              | 5.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0502  |
| entropy                 | 2.55     |
| episodes                | 176700   |
| lives                   | 176700   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1173492  |
| value_loss              | 5.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0545  |
| entropy                 | 2.54     |
| episodes                | 176800   |
| lives                   | 176800   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0199  |
| steps                   | 1174190  |
| value_loss              | 5.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0312  |
| entropy                 | 2.52     |
| episodes                | 176900   |
| lives                   | 176900   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1174893  |
| value_loss              | 5.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0582  |
| entropy                 | 2.61     |
| episodes                | 177000   |
| lives                   | 177000   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1175600  |
| value_loss              | 4.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0594  |
| entropy                 | 2.63     |
| episodes                | 177100   |
| lives                   | 177100   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1176323  |
| value_loss              | 5.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0604  |
| entropy                 | 2.59     |
| episodes                | 177200   |
| lives                   | 177200   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 1177025  |
| value_loss              | 4.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.53     |
| episodes                | 177300   |
| lives                   | 177300   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1177743  |
| value_loss              | 5.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0508  |
| entropy                 | 2.62     |
| episodes                | 177400   |
| lives                   | 177400   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1178521  |
| value_loss              | 5.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0755  |
| entropy                 | 2.58     |
| episodes                | 177500   |
| lives                   | 177500   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1179248  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0539  |
| entropy                 | 2.47     |
| episodes                | 177600   |
| lives                   | 177600   |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1179925  |
| value_loss              | 5.04     |
--------------------------------------
Saving model due to running mean reward increase: 5.9972 -> 6.1164
--------------------------------------
| approx_kl               | -0.0596  |
| entropy                 | 2.58     |
| episodes                | 177700   |
| lives                   | 177700   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0084   |
| steps                   | 1180677  |
| value_loss              | 6.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0758  |
| entropy                 | 2.48     |
| episodes                | 177800   |
| lives                   | 177800   |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1181322  |
| value_loss              | 5.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0628  |
| entropy                 | 2.43     |
| episodes                | 177900   |
| lives                   | 177900   |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1181879  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.57     |
| episodes                | 178000   |
| lives                   | 178000   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 1182445  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0904  |
| entropy                 | 2.62     |
| episodes                | 178100   |
| lives                   | 178100   |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 1183065  |
| value_loss              | 5.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0547  |
| entropy                 | 2.6      |
| episodes                | 178200   |
| lives                   | 178200   |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0065   |
| steps                   | 1183683  |
| value_loss              | 5.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0586  |
| entropy                 | 2.56     |
| episodes                | 178300   |
| lives                   | 178300   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 1184299  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.59     |
| episodes                | 178400   |
| lives                   | 178400   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1185043  |
| value_loss              | 5.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0473  |
| entropy                 | 2.4      |
| episodes                | 178500   |
| lives                   | 178500   |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 1185699  |
| value_loss              | 5.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0602  |
| entropy                 | 2.45     |
| episodes                | 178600   |
| lives                   | 178600   |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1186295  |
| value_loss              | 5.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0803  |
| entropy                 | 2.54     |
| episodes                | 178700   |
| lives                   | 178700   |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 1186957  |
| value_loss              | 5.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.138   |
| entropy                 | 2.48     |
| episodes                | 178800   |
| lives                   | 178800   |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0239   |
| steps                   | 1187623  |
| value_loss              | 5.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0742  |
| entropy                 | 2.58     |
| episodes                | 178900   |
| lives                   | 178900   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.03     |
| steps                   | 1188282  |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0651  |
| entropy                 | 2.49     |
| episodes                | 179000   |
| lives                   | 179000   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1188870  |
| value_loss              | 5.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0445  |
| entropy                 | 2.62     |
| episodes                | 179100   |
| lives                   | 179100   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1189575  |
| value_loss              | 5.48     |
--------------------------------------
Saving model due to running mean reward increase: 5.6616 -> 5.7257
--------------------------------------
| approx_kl               | -0.0334  |
| entropy                 | 2.49     |
| episodes                | 179200   |
| lives                   | 179200   |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0033   |
| steps                   | 1190229  |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0405  |
| entropy                 | 2.52     |
| episodes                | 179300   |
| lives                   | 179300   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 1190867  |
| value_loss              | 5.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0484  |
| entropy                 | 2.6      |
| episodes                | 179400   |
| lives                   | 179400   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0018   |
| steps                   | 1191577  |
| value_loss              | 5.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0586  |
| entropy                 | 2.55     |
| episodes                | 179500   |
| lives                   | 179500   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0018   |
| steps                   | 1192241  |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0897  |
| entropy                 | 2.45     |
| episodes                | 179600   |
| lives                   | 179600   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 1192826  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.097   |
| entropy                 | 2.53     |
| episodes                | 179700   |
| lives                   | 179700   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0095   |
| steps                   | 1193460  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0801  |
| entropy                 | 2.41     |
| episodes                | 179800   |
| lives                   | 179800   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 1194055  |
| value_loss              | 5.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0663  |
| entropy                 | 2.54     |
| episodes                | 179900   |
| lives                   | 179900   |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 1194711  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0631  |
| entropy                 | 2.48     |
| episodes                | 180000   |
| lives                   | 180000   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0115   |
| steps                   | 1195345  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0189  |
| entropy                 | 2.33     |
| episodes                | 180100   |
| lives                   | 180100   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0014   |
| steps                   | 1195987  |
| value_loss              | 5.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0363  |
| entropy                 | 2.5      |
| episodes                | 180200   |
| lives                   | 180200   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1196694  |
| value_loss              | 5.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0213  |
| entropy                 | 2.48     |
| episodes                | 180300   |
| lives                   | 180300   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1197391  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0309  |
| entropy                 | 2.57     |
| episodes                | 180400   |
| lives                   | 180400   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1198148  |
| value_loss              | 6.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0314  |
| entropy                 | 2.47     |
| episodes                | 180500   |
| lives                   | 180500   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 1198866  |
| value_loss              | 5.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0385  |
| entropy                 | 2.52     |
| episodes                | 180600   |
| lives                   | 180600   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1199613  |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.47     |
| episodes                | 180700   |
| lives                   | 180700   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1200339  |
| value_loss              | 6.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0231  |
| entropy                 | 2.49     |
| episodes                | 180800   |
| lives                   | 180800   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1201092  |
| value_loss              | 5.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.5      |
| episodes                | 180900   |
| lives                   | 180900   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1201829  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0211  |
| entropy                 | 2.43     |
| episodes                | 181000   |
| lives                   | 181000   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1202567  |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0431  |
| entropy                 | 2.45     |
| episodes                | 181100   |
| lives                   | 181100   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 1203272  |
| value_loss              | 5.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0419  |
| entropy                 | 2.52     |
| episodes                | 181200   |
| lives                   | 181200   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1203995  |
| value_loss              | 5.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0355  |
| entropy                 | 2.56     |
| episodes                | 181300   |
| lives                   | 181300   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 1204733  |
| value_loss              | 5.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.53     |
| episodes                | 181400   |
| lives                   | 181400   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1205484  |
| value_loss              | 5.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.49     |
| episodes                | 181500   |
| lives                   | 181500   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1206220  |
| value_loss              | 5.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0439  |
| entropy                 | 2.47     |
| episodes                | 181600   |
| lives                   | 181600   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0022   |
| steps                   | 1206913  |
| value_loss              | 5.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.055   |
| entropy                 | 2.52     |
| episodes                | 181700   |
| lives                   | 181700   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1207648  |
| value_loss              | 5.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.5      |
| episodes                | 181800   |
| lives                   | 181800   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 1208414  |
| value_loss              | 5.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.45     |
| episodes                | 181900   |
| lives                   | 181900   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1209143  |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0271  |
| entropy                 | 2.52     |
| episodes                | 182000   |
| lives                   | 182000   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1209913  |
| value_loss              | 6.21     |
--------------------------------------
Saving model due to running mean reward increase: 6.2606 -> 6.3887
--------------------------------------
| approx_kl               | -0.0163  |
| entropy                 | 2.5      |
| episodes                | 182100   |
| lives                   | 182100   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0005   |
| steps                   | 1210699  |
| value_loss              | 5.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0132  |
| entropy                 | 2.46     |
| episodes                | 182200   |
| lives                   | 182200   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1211429  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.025   |
| entropy                 | 2.47     |
| episodes                | 182300   |
| lives                   | 182300   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 1212208  |
| value_loss              | 5.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.42     |
| episodes                | 182400   |
| lives                   | 182400   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 1212953  |
| value_loss              | 5.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0396  |
| entropy                 | 2.42     |
| episodes                | 182500   |
| lives                   | 182500   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1213690  |
| value_loss              | 5.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0998  |
| entropy                 | 2.38     |
| episodes                | 182600   |
| lives                   | 182600   |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0132   |
| steps                   | 1214340  |
| value_loss              | 4.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0902  |
| entropy                 | 2.37     |
| episodes                | 182700   |
| lives                   | 182700   |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0053   |
| steps                   | 1214916  |
| value_loss              | 5.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0771  |
| entropy                 | 2.34     |
| episodes                | 182800   |
| lives                   | 182800   |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1215473  |
| value_loss              | 6.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0828  |
| entropy                 | 2.49     |
| episodes                | 182900   |
| lives                   | 182900   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1216088  |
| value_loss              | 5.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.087   |
| entropy                 | 2.3      |
| episodes                | 183000   |
| lives                   | 183000   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0005   |
| steps                   | 1216639  |
| value_loss              | 6.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.11    |
| entropy                 | 2.33     |
| episodes                | 183100   |
| lives                   | 183100   |
| mean 100 episode length | 6.35     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1217174  |
| value_loss              | 6.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.52     |
| episodes                | 183200   |
| lives                   | 183200   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0039   |
| steps                   | 1217760  |
| value_loss              | 6.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.114   |
| entropy                 | 2.46     |
| episodes                | 183300   |
| lives                   | 183300   |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 1218319  |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.111   |
| entropy                 | 2.49     |
| episodes                | 183400   |
| lives                   | 183400   |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1218938  |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0645  |
| entropy                 | 2.39     |
| episodes                | 183500   |
| lives                   | 183500   |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1219535  |
| value_loss              | 6.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.3      |
| episodes                | 183600   |
| lives                   | 183600   |
| mean 100 episode length | 6.42     |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1220077  |
| value_loss              | 6.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0925  |
| entropy                 | 2.39     |
| episodes                | 183700   |
| lives                   | 183700   |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1220651  |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.37     |
| episodes                | 183800   |
| lives                   | 183800   |
| mean 100 episode length | 6.46     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1221197  |
| value_loss              | 6.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.099   |
| entropy                 | 2.35     |
| episodes                | 183900   |
| lives                   | 183900   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0083   |
| steps                   | 1221785  |
| value_loss              | 6.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0904  |
| entropy                 | 2.38     |
| episodes                | 184000   |
| lives                   | 184000   |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1222394  |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0982  |
| entropy                 | 2.28     |
| episodes                | 184100   |
| lives                   | 184100   |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1222933  |
| value_loss              | 6.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.3      |
| episodes                | 184200   |
| lives                   | 184200   |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0189  |
| steps                   | 1223487  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.116   |
| entropy                 | 2.4      |
| episodes                | 184300   |
| lives                   | 184300   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 1224100  |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.122   |
| entropy                 | 2.43     |
| episodes                | 184400   |
| lives                   | 184400   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0079   |
| steps                   | 1224686  |
| value_loss              | 6.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0797  |
| entropy                 | 2.31     |
| episodes                | 184500   |
| lives                   | 184500   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 1225267  |
| value_loss              | 6.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0782  |
| entropy                 | 2.39     |
| episodes                | 184600   |
| lives                   | 184600   |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1225840  |
| value_loss              | 6.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.27     |
| episodes                | 184700   |
| lives                   | 184700   |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 1226361  |
| value_loss              | 6.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.37     |
| episodes                | 184800   |
| lives                   | 184800   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1226941  |
| value_loss              | 6.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.11    |
| entropy                 | 2.29     |
| episodes                | 184900   |
| lives                   | 184900   |
| mean 100 episode length | 6.25     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 1227466  |
| value_loss              | 6.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.098   |
| entropy                 | 2.39     |
| episodes                | 185000   |
| lives                   | 185000   |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0023   |
| steps                   | 1228042  |
| value_loss              | 6.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.109   |
| entropy                 | 2.26     |
| episodes                | 185100   |
| lives                   | 185100   |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1228563  |
| value_loss              | 7.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.099   |
| entropy                 | 2.18     |
| episodes                | 185200   |
| lives                   | 185200   |
| mean 100 episode length | 5.97     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1229060  |
| value_loss              | 7.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0965  |
| entropy                 | 2.33     |
| episodes                | 185300   |
| lives                   | 185300   |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 1229638  |
| value_loss              | 7.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0865  |
| entropy                 | 2.38     |
| episodes                | 185400   |
| lives                   | 185400   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1230224  |
| value_loss              | 6.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0983  |
| entropy                 | 2.35     |
| episodes                | 185500   |
| lives                   | 185500   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 1230790  |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0826  |
| entropy                 | 2.45     |
| episodes                | 185600   |
| lives                   | 185600   |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 1231399  |
| value_loss              | 6.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.48     |
| episodes                | 185700   |
| lives                   | 185700   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 1231994  |
| value_loss              | 6.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.103   |
| entropy                 | 2.47     |
| episodes                | 185800   |
| lives                   | 185800   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1232582  |
| value_loss              | 6.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.097   |
| entropy                 | 2.42     |
| episodes                | 185900   |
| lives                   | 185900   |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 1233135  |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.059   |
| entropy                 | 2.34     |
| episodes                | 186000   |
| lives                   | 186000   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0024   |
| steps                   | 1233729  |
| value_loss              | 6.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0788  |
| entropy                 | 2.48     |
| episodes                | 186100   |
| lives                   | 186100   |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 1234353  |
| value_loss              | 6.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0715  |
| entropy                 | 2.42     |
| episodes                | 186200   |
| lives                   | 186200   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1234941  |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.082   |
| entropy                 | 2.44     |
| episodes                | 186300   |
| lives                   | 186300   |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1235551  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0821  |
| entropy                 | 2.39     |
| episodes                | 186400   |
| lives                   | 186400   |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1236125  |
| value_loss              | 6.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0917  |
| entropy                 | 2.32     |
| episodes                | 186500   |
| lives                   | 186500   |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1236668  |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0723  |
| entropy                 | 2.4      |
| episodes                | 186600   |
| lives                   | 186600   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1237249  |
| value_loss              | 6.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.076   |
| entropy                 | 2.32     |
| episodes                | 186700   |
| lives                   | 186700   |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 1237805  |
| value_loss              | 6.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0803  |
| entropy                 | 2.38     |
| episodes                | 186800   |
| lives                   | 186800   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1238393  |
| value_loss              | 6.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.119   |
| entropy                 | 2.39     |
| episodes                | 186900   |
| lives                   | 186900   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1238981  |
| value_loss              | 5.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.26     |
| episodes                | 187000   |
| lives                   | 187000   |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 4.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0165  |
| steps                   | 1239565  |
| value_loss              | 5.41     |
--------------------------------------
Saving model due to running mean reward increase: 4.5804 -> 5.1734
--------------------------------------
| approx_kl               | -0.0857  |
| entropy                 | 2.45     |
| episodes                | 187100   |
| lives                   | 187100   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 1240226  |
| value_loss              | 6.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0811  |
| entropy                 | 2.39     |
| episodes                | 187200   |
| lives                   | 187200   |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1240830  |
| value_loss              | 6.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0911  |
| entropy                 | 2.46     |
| episodes                | 187300   |
| lives                   | 187300   |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1241414  |
| value_loss              | 6.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.38     |
| episodes                | 187400   |
| lives                   | 187400   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.02    |
| steps                   | 1241980  |
| value_loss              | 6.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.108   |
| entropy                 | 2.33     |
| episodes                | 187500   |
| lives                   | 187500   |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1242539  |
| value_loss              | 6.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.133   |
| entropy                 | 2.28     |
| episodes                | 187600   |
| lives                   | 187600   |
| mean 100 episode length | 6.37     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 1243076  |
| value_loss              | 7.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.093   |
| entropy                 | 2.24     |
| episodes                | 187700   |
| lives                   | 187700   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1243643  |
| value_loss              | 6.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.118   |
| entropy                 | 2.32     |
| episodes                | 187800   |
| lives                   | 187800   |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1244243  |
| value_loss              | 6.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.39     |
| episodes                | 187900   |
| lives                   | 187900   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 1244837  |
| value_loss              | 6.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.127   |
| entropy                 | 2.27     |
| episodes                | 188000   |
| lives                   | 188000   |
| mean 100 episode length | 6.23     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 1245360  |
| value_loss              | 6.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.139   |
| entropy                 | 2.41     |
| episodes                | 188100   |
| lives                   | 188100   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0243  |
| steps                   | 1245931  |
| value_loss              | 6.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.51     |
| episodes                | 188200   |
| lives                   | 188200   |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 1246555  |
| value_loss              | 6.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0851  |
| entropy                 | 2.36     |
| episodes                | 188300   |
| lives                   | 188300   |
| mean 100 episode length | 6.45     |
| mean 100 episode reward | 4.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1247100  |
| value_loss              | 6.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.38     |
| episodes                | 188400   |
| lives                   | 188400   |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1247689  |
| value_loss              | 5.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0997  |
| entropy                 | 2.41     |
| episodes                | 188500   |
| lives                   | 188500   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1248274  |
| value_loss              | 6.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.37     |
| episodes                | 188600   |
| lives                   | 188600   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 1248864  |
| value_loss              | 6.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.42     |
| episodes                | 188700   |
| lives                   | 188700   |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 1249474  |
| value_loss              | 6.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.34     |
| episodes                | 188800   |
| lives                   | 188800   |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 5.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1250049  |
| value_loss              | 6.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.108   |
| entropy                 | 2.31     |
| episodes                | 188900   |
| lives                   | 188900   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1250676  |
| value_loss              | 6.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.05    |
| entropy                 | 2.43     |
| episodes                | 189000   |
| lives                   | 189000   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0233   |
| steps                   | 1251422  |
| value_loss              | 6.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0052  |
| entropy                 | 2.41     |
| episodes                | 189100   |
| lives                   | 189100   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 1252182  |
| value_loss              | 5.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.49     |
| episodes                | 189200   |
| lives                   | 189200   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1252921  |
| value_loss              | 5.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.012   |
| entropy                 | 2.37     |
| episodes                | 189300   |
| lives                   | 189300   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1253629  |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0251  |
| entropy                 | 2.48     |
| episodes                | 189400   |
| lives                   | 189400   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1254348  |
| value_loss              | 5.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.051   |
| entropy                 | 2.45     |
| episodes                | 189500   |
| lives                   | 189500   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1255080  |
| value_loss              | 5.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0946  |
| entropy                 | 2.44     |
| episodes                | 189600   |
| lives                   | 189600   |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0087   |
| steps                   | 1255738  |
| value_loss              | 5.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.111   |
| entropy                 | 2.5      |
| episodes                | 189700   |
| lives                   | 189700   |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 1256336  |
| value_loss              | 4.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.098   |
| entropy                 | 2.36     |
| episodes                | 189800   |
| lives                   | 189800   |
| mean 100 episode length | 6.52     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 1256888  |
| value_loss              | 4.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0933  |
| entropy                 | 2.39     |
| episodes                | 189900   |
| lives                   | 189900   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 1257476  |
| value_loss              | 5.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0693  |
| entropy                 | 2.36     |
| episodes                | 190000   |
| lives                   | 190000   |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1258067  |
| value_loss              | 5.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0659  |
| entropy                 | 2.36     |
| episodes                | 190100   |
| lives                   | 190100   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0143  |
| steps                   | 1258666  |
| value_loss              | 5.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.115   |
| entropy                 | 2.46     |
| episodes                | 190200   |
| lives                   | 190200   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0183  |
| steps                   | 1259254  |
| value_loss              | 5.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.118   |
| entropy                 | 2.45     |
| episodes                | 190300   |
| lives                   | 190300   |
| mean 100 episode length | 6.49     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1259803  |
| value_loss              | 5.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0662  |
| entropy                 | 2.39     |
| episodes                | 190400   |
| lives                   | 190400   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0033   |
| steps                   | 1260391  |
| value_loss              | 5.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.46     |
| episodes                | 190500   |
| lives                   | 190500   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 1261028  |
| value_loss              | 4.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0454  |
| entropy                 | 2.57     |
| episodes                | 190600   |
| lives                   | 190600   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 1261791  |
| value_loss              | 5.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.51     |
| episodes                | 190700   |
| lives                   | 190700   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1262528  |
| value_loss              | 5.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0459  |
| entropy                 | 2.53     |
| episodes                | 190800   |
| lives                   | 190800   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0007   |
| steps                   | 1263232  |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0466  |
| entropy                 | 2.45     |
| episodes                | 190900   |
| lives                   | 190900   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 1263911  |
| value_loss              | 5.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.136   |
| entropy                 | 2.35     |
| episodes                | 191000   |
| lives                   | 191000   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1264492  |
| value_loss              | 6.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.105   |
| entropy                 | 2.24     |
| episodes                | 191100   |
| lives                   | 191100   |
| mean 100 episode length | 6.11     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1265003  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0963  |
| entropy                 | 2.39     |
| episodes                | 191200   |
| lives                   | 191200   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1265597  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0917  |
| entropy                 | 2.36     |
| episodes                | 191300   |
| lives                   | 191300   |
| mean 100 episode length | 5.97     |
| mean 100 episode reward | 4.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1266094  |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.51     |
| episodes                | 191400   |
| lives                   | 191400   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1266731  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.103   |
| entropy                 | 2.38     |
| episodes                | 191500   |
| lives                   | 191500   |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1267292  |
| value_loss              | 5.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0775  |
| entropy                 | 2.23     |
| episodes                | 191600   |
| lives                   | 191600   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0181  |
| steps                   | 1267843  |
| value_loss              | 6.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0858  |
| entropy                 | 2.25     |
| episodes                | 191700   |
| lives                   | 191700   |
| mean 100 episode length | 6.3      |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 1268373  |
| value_loss              | 5.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.39     |
| episodes                | 191800   |
| lives                   | 191800   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1268924  |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.143   |
| entropy                 | 2.42     |
| episodes                | 191900   |
| lives                   | 191900   |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 1269445  |
| value_loss              | 6.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.146   |
| entropy                 | 2.4      |
| episodes                | 192000   |
| lives                   | 192000   |
| mean 100 episode length | 6.44     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1269989  |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.131   |
| entropy                 | 2.39     |
| episodes                | 192100   |
| lives                   | 192100   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 4.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1270552  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.119   |
| entropy                 | 2.4      |
| episodes                | 192200   |
| lives                   | 192200   |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1271110  |
| value_loss              | 5.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0998  |
| entropy                 | 2.42     |
| episodes                | 192300   |
| lives                   | 192300   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1271676  |
| value_loss              | 5.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.41     |
| episodes                | 192400   |
| lives                   | 192400   |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 1272276  |
| value_loss              | 5.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.11    |
| entropy                 | 2.41     |
| episodes                | 192500   |
| lives                   | 192500   |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 1272860  |
| value_loss              | 5.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.37     |
| episodes                | 192600   |
| lives                   | 192600   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 1273454  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.145   |
| entropy                 | 2.48     |
| episodes                | 192700   |
| lives                   | 192700   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0185  |
| steps                   | 1274020  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.111   |
| entropy                 | 2.45     |
| episodes                | 192800   |
| lives                   | 192800   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 1274636  |
| value_loss              | 5.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.32     |
| episodes                | 192900   |
| lives                   | 192900   |
| mean 100 episode length | 6.06     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 1275142  |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.38     |
| episodes                | 193000   |
| lives                   | 193000   |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1275695  |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.45     |
| episodes                | 193100   |
| lives                   | 193100   |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1276296  |
| value_loss              | 5.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.43     |
| episodes                | 193200   |
| lives                   | 193200   |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 1276875  |
| value_loss              | 5.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.51     |
| episodes                | 193300   |
| lives                   | 193300   |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0102   |
| steps                   | 1277471  |
| value_loss              | 5.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.128   |
| entropy                 | 2.57     |
| episodes                | 193400   |
| lives                   | 193400   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1278086  |
| value_loss              | 5.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.137   |
| entropy                 | 2.49     |
| episodes                | 193500   |
| lives                   | 193500   |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1278687  |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.145   |
| entropy                 | 2.4      |
| episodes                | 193600   |
| lives                   | 193600   |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 1279246  |
| value_loss              | 5.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.123   |
| entropy                 | 2.52     |
| episodes                | 193700   |
| lives                   | 193700   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1279894  |
| value_loss              | 5.63     |
--------------------------------------
Saving model due to running mean reward increase: 5.4747 -> 5.9442
--------------------------------------
| approx_kl               | -0.0428  |
| entropy                 | 2.6      |
| episodes                | 193800   |
| lives                   | 193800   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1280620  |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.107   |
| entropy                 | 2.45     |
| episodes                | 193900   |
| lives                   | 193900   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 1281225  |
| value_loss              | 5.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.46     |
| episodes                | 194000   |
| lives                   | 194000   |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1281834  |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.54     |
| episodes                | 194100   |
| lives                   | 194100   |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 1282444  |
| value_loss              | 5.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.085   |
| entropy                 | 2.51     |
| episodes                | 194200   |
| lives                   | 194200   |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1283052  |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0993  |
| entropy                 | 2.59     |
| episodes                | 194300   |
| lives                   | 194300   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1283713  |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.46     |
| episodes                | 194400   |
| lives                   | 194400   |
| mean 100 episode length | 7        |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0163  |
| steps                   | 1284313  |
| value_loss              | 6.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.148   |
| entropy                 | 2.47     |
| episodes                | 194500   |
| lives                   | 194500   |
| mean 100 episode length | 6.47     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 1284860  |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.115   |
| entropy                 | 2.45     |
| episodes                | 194600   |
| lives                   | 194600   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1285427  |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.43     |
| episodes                | 194700   |
| lives                   | 194700   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1285998  |
| value_loss              | 6.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.47     |
| episodes                | 194800   |
| lives                   | 194800   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1286614  |
| value_loss              | 6.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0869  |
| entropy                 | 2.41     |
| episodes                | 194900   |
| lives                   | 194900   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1287252  |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0984  |
| entropy                 | 2.29     |
| episodes                | 195000   |
| lives                   | 195000   |
| mean 100 episode length | 6.44     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1287796  |
| value_loss              | 5.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0898  |
| entropy                 | 2.46     |
| episodes                | 195100   |
| lives                   | 195100   |
| mean 100 episode length | 6.5      |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 1288346  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0931  |
| entropy                 | 2.33     |
| episodes                | 195200   |
| lives                   | 195200   |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1288923  |
| value_loss              | 5.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.4      |
| episodes                | 195300   |
| lives                   | 195300   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 1289494  |
| value_loss              | 5.33     |
--------------------------------------
Saving model due to running mean reward increase: 4.9189 -> 5.1338
--------------------------------------
| approx_kl               | -0.0723  |
| entropy                 | 2.49     |
| episodes                | 195400   |
| lives                   | 195400   |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 1290118  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.064   |
| entropy                 | 2.62     |
| episodes                | 195500   |
| lives                   | 195500   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0038   |
| steps                   | 1290829  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.138   |
| entropy                 | 2.48     |
| episodes                | 195600   |
| lives                   | 195600   |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1291411  |
| value_loss              | 5.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0761  |
| entropy                 | 2.55     |
| episodes                | 195700   |
| lives                   | 195700   |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1292088  |
| value_loss              | 5.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.116   |
| entropy                 | 2.42     |
| episodes                | 195800   |
| lives                   | 195800   |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0084   |
| steps                   | 1292648  |
| value_loss              | 5.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.133   |
| entropy                 | 2.41     |
| episodes                | 195900   |
| lives                   | 195900   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 4.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1293199  |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0731  |
| entropy                 | 2.35     |
| episodes                | 196000   |
| lives                   | 196000   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1293762  |
| value_loss              | 5.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.4      |
| episodes                | 196100   |
| lives                   | 196100   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1294328  |
| value_loss              | 5.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.126   |
| entropy                 | 2.51     |
| episodes                | 196200   |
| lives                   | 196200   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1294923  |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0791  |
| entropy                 | 2.55     |
| episodes                | 196300   |
| lives                   | 196300   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 1295578  |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0386  |
| entropy                 | 2.49     |
| episodes                | 196400   |
| lives                   | 196400   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0011   |
| steps                   | 1296295  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.56     |
| episodes                | 196500   |
| lives                   | 196500   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 1297081  |
| value_loss              | 5.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.62     |
| episodes                | 196600   |
| lives                   | 196600   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1297900  |
| value_loss              | 5.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0291  |
| entropy                 | 2.61     |
| episodes                | 196700   |
| lives                   | 196700   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1298651  |
| value_loss              | 5.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0312  |
| entropy                 | 2.53     |
| episodes                | 196800   |
| lives                   | 196800   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1299344  |
| value_loss              | 5.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0159  |
| entropy                 | 2.45     |
| episodes                | 196900   |
| lives                   | 196900   |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 4.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1299998  |
| value_loss              | 5.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0189  |
| entropy                 | 2.43     |
| episodes                | 197000   |
| lives                   | 197000   |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1300667  |
| value_loss              | 5.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0316  |
| entropy                 | 2.3      |
| episodes                | 197100   |
| lives                   | 197100   |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 1301263  |
| value_loss              | 4.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.37     |
| episodes                | 197200   |
| lives                   | 197200   |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 1301916  |
| value_loss              | 5.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.33     |
| episodes                | 197300   |
| lives                   | 197300   |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1302535  |
| value_loss              | 5.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0578  |
| entropy                 | 2.4      |
| episodes                | 197400   |
| lives                   | 197400   |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1303167  |
| value_loss              | 5.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.5      |
| episodes                | 197500   |
| lives                   | 197500   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1303859  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.39     |
| episodes                | 197600   |
| lives                   | 197600   |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1304432  |
| value_loss              | 4.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0462  |
| entropy                 | 2.52     |
| episodes                | 197700   |
| lives                   | 197700   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1305093  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0652  |
| entropy                 | 2.54     |
| episodes                | 197800   |
| lives                   | 197800   |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1305751  |
| value_loss              | 4.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0287  |
| entropy                 | 2.43     |
| episodes                | 197900   |
| lives                   | 197900   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1306449  |
| value_loss              | 5.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0551  |
| entropy                 | 2.55     |
| episodes                | 198000   |
| lives                   | 198000   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 1307164  |
| value_loss              | 4.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0413  |
| entropy                 | 2.44     |
| episodes                | 198100   |
| lives                   | 198100   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1307829  |
| value_loss              | 4.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0655  |
| entropy                 | 2.44     |
| episodes                | 198200   |
| lives                   | 198200   |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0021   |
| steps                   | 1308448  |
| value_loss              | 4.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0779  |
| entropy                 | 2.34     |
| episodes                | 198300   |
| lives                   | 198300   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1309011  |
| value_loss              | 4.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0915  |
| entropy                 | 2.33     |
| episodes                | 198400   |
| lives                   | 198400   |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1309583  |
| value_loss              | 5.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.45     |
| episodes                | 198500   |
| lives                   | 198500   |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1310170  |
| value_loss              | 5.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.49     |
| episodes                | 198600   |
| lives                   | 198600   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 1310801  |
| value_loss              | 5.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.115   |
| entropy                 | 2.44     |
| episodes                | 198700   |
| lives                   | 198700   |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1311370  |
| value_loss              | 5.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.124   |
| entropy                 | 2.39     |
| episodes                | 198800   |
| lives                   | 198800   |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1311942  |
| value_loss              | 5.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0815  |
| entropy                 | 2.34     |
| episodes                | 198900   |
| lives                   | 198900   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1312541  |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.109   |
| entropy                 | 2.35     |
| episodes                | 199000   |
| lives                   | 199000   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 1313107  |
| value_loss              | 5.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.114   |
| entropy                 | 2.44     |
| episodes                | 199100   |
| lives                   | 199100   |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 1313714  |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0889  |
| entropy                 | 2.46     |
| episodes                | 199200   |
| lives                   | 199200   |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 1314314  |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.153   |
| entropy                 | 2.51     |
| episodes                | 199300   |
| lives                   | 199300   |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 1314878  |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.156   |
| entropy                 | 2.46     |
| episodes                | 199400   |
| lives                   | 199400   |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1315436  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.35     |
| episodes                | 199500   |
| lives                   | 199500   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 1316003  |
| value_loss              | 5.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.36     |
| episodes                | 199600   |
| lives                   | 199600   |
| mean 100 episode length | 6.41     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0139  |
| steps                   | 1316544  |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.137   |
| entropy                 | 2.36     |
| episodes                | 199700   |
| lives                   | 199700   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 1317130  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.111   |
| entropy                 | 2.27     |
| episodes                | 199800   |
| lives                   | 199800   |
| mean 100 episode length | 6.44     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1317674  |
| value_loss              | 5.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.121   |
| entropy                 | 2.34     |
| episodes                | 199900   |
| lives                   | 199900   |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1318249  |
| value_loss              | 6.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.42     |
| episodes                | 200000   |
| lives                   | 200000   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1318830  |
| value_loss              | 5.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.126   |
| entropy                 | 2.33     |
| episodes                | 200100   |
| lives                   | 200100   |
| mean 100 episode length | 6.47     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 1319377  |
| value_loss              | 5.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.179   |
| entropy                 | 2.35     |
| episodes                | 200200   |
| lives                   | 200200   |
| mean 100 episode length | 6.01     |
| mean 100 episode reward | 4.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 1319878  |
| value_loss              | 4.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.159   |
| entropy                 | 2.23     |
| episodes                | 200300   |
| lives                   | 200300   |
| mean 100 episode length | 5.98     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 1320376  |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.154   |
| entropy                 | 2.33     |
| episodes                | 200400   |
| lives                   | 200400   |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1320910  |
| value_loss              | 5.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.154   |
| entropy                 | 2.3      |
| episodes                | 200500   |
| lives                   | 200500   |
| mean 100 episode length | 6.18     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 1321428  |
| value_loss              | 5.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0644  |
| entropy                 | 2.33     |
| episodes                | 200600   |
| lives                   | 200600   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0173  |
| steps                   | 1321994  |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0747  |
| entropy                 | 2.41     |
| episodes                | 200700   |
| lives                   | 200700   |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1322596  |
| value_loss              | 5.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.108   |
| entropy                 | 2.39     |
| episodes                | 200800   |
| lives                   | 200800   |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0197  |
| steps                   | 1323165  |
| value_loss              | 4.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.36     |
| episodes                | 200900   |
| lives                   | 200900   |
| mean 100 episode length | 6.49     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0201  |
| steps                   | 1323714  |
| value_loss              | 5.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.28     |
| episodes                | 201000   |
| lives                   | 201000   |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 1324287  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.158   |
| entropy                 | 2.36     |
| episodes                | 201100   |
| lives                   | 201100   |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1324844  |
| value_loss              | 5.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.155   |
| entropy                 | 2.34     |
| episodes                | 201200   |
| lives                   | 201200   |
| mean 100 episode length | 6.41     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1325385  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.13    |
| entropy                 | 2.29     |
| episodes                | 201300   |
| lives                   | 201300   |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.007    |
| steps                   | 1325912  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.44     |
| episodes                | 201400   |
| lives                   | 201400   |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1326521  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.138   |
| entropy                 | 2.5      |
| episodes                | 201500   |
| lives                   | 201500   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0014   |
| steps                   | 1327111  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.142   |
| entropy                 | 2.4      |
| episodes                | 201600   |
| lives                   | 201600   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1327699  |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.149   |
| entropy                 | 2.39     |
| episodes                | 201700   |
| lives                   | 201700   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1328287  |
| value_loss              | 5.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.129   |
| entropy                 | 2.46     |
| episodes                | 201800   |
| lives                   | 201800   |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 1328888  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.47     |
| episodes                | 201900   |
| lives                   | 201900   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1329478  |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.156   |
| entropy                 | 2.53     |
| episodes                | 202000   |
| lives                   | 202000   |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1330043  |
| value_loss              | 5.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.121   |
| entropy                 | 2.45     |
| episodes                | 202100   |
| lives                   | 202100   |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1330607  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.15    |
| entropy                 | 2.45     |
| episodes                | 202200   |
| lives                   | 202200   |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 4.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 1331146  |
| value_loss              | 6.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.39     |
| episodes                | 202300   |
| lives                   | 202300   |
| mean 100 episode length | 6.47     |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 1331693  |
| value_loss              | 5.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.08    |
| entropy                 | 2.64     |
| episodes                | 202400   |
| lives                   | 202400   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0197   |
| steps                   | 1332331  |
| value_loss              | 5.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0468  |
| entropy                 | 2.44     |
| episodes                | 202500   |
| lives                   | 202500   |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 4.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1332956  |
| value_loss              | 4.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0233  |
| entropy                 | 2.48     |
| episodes                | 202600   |
| lives                   | 202600   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 4.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1333598  |
| value_loss              | 4.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0747  |
| entropy                 | 2.48     |
| episodes                | 202700   |
| lives                   | 202700   |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1334202  |
| value_loss              | 4.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0507  |
| entropy                 | 2.51     |
| episodes                | 202800   |
| lives                   | 202800   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 1334836  |
| value_loss              | 5.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0412  |
| entropy                 | 2.55     |
| episodes                | 202900   |
| lives                   | 202900   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 1335564  |
| value_loss              | 5.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0212  |
| entropy                 | 2.49     |
| episodes                | 203000   |
| lives                   | 203000   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 1336277  |
| value_loss              | 5.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0278  |
| entropy                 | 2.46     |
| episodes                | 203100   |
| lives                   | 203100   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1336989  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0139  |
| entropy                 | 2.5      |
| episodes                | 203200   |
| lives                   | 203200   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1337684  |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0207  |
| entropy                 | 2.5      |
| episodes                | 203300   |
| lives                   | 203300   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1338431  |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0707  |
| entropy                 | 2.46     |
| episodes                | 203400   |
| lives                   | 203400   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.002    |
| steps                   | 1339158  |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.46     |
| episodes                | 203500   |
| lives                   | 203500   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0064   |
| steps                   | 1339738  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0841  |
| entropy                 | 2.44     |
| episodes                | 203600   |
| lives                   | 203600   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1340328  |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.124   |
| entropy                 | 2.38     |
| episodes                | 203700   |
| lives                   | 203700   |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 1340855  |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.37     |
| episodes                | 203800   |
| lives                   | 203800   |
| mean 100 episode length | 6.41     |
| mean 100 episode reward | 4.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1341396  |
| value_loss              | 5.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.131   |
| entropy                 | 2.39     |
| episodes                | 203900   |
| lives                   | 203900   |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1341950  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0952  |
| entropy                 | 2.54     |
| episodes                | 204000   |
| lives                   | 204000   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0144   |
| steps                   | 1342584  |
| value_loss              | 5.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0808  |
| entropy                 | 2.55     |
| episodes                | 204100   |
| lives                   | 204100   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0025   |
| steps                   | 1343221  |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.088   |
| entropy                 | 2.52     |
| episodes                | 204200   |
| lives                   | 204200   |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0045   |
| steps                   | 1343849  |
| value_loss              | 5.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.09    |
| entropy                 | 2.47     |
| episodes                | 204300   |
| lives                   | 204300   |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1344448  |
| value_loss              | 5.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0797  |
| entropy                 | 2.55     |
| episodes                | 204400   |
| lives                   | 204400   |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.003    |
| steps                   | 1345092  |
| value_loss              | 5.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0795  |
| entropy                 | 2.46     |
| episodes                | 204500   |
| lives                   | 204500   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1345663  |
| value_loss              | 6.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0989  |
| entropy                 | 2.33     |
| episodes                | 204600   |
| lives                   | 204600   |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0375   |
| steps                   | 1346206  |
| value_loss              | 5.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0874  |
| entropy                 | 2.25     |
| episodes                | 204700   |
| lives                   | 204700   |
| mean 100 episode length | 6.55     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0199  |
| steps                   | 1346761  |
| value_loss              | 5.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0862  |
| entropy                 | 2.35     |
| episodes                | 204800   |
| lives                   | 204800   |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 1347288  |
| value_loss              | 6.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0551  |
| entropy                 | 2.35     |
| episodes                | 204900   |
| lives                   | 204900   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1347874  |
| value_loss              | 6.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.166   |
| entropy                 | 2.34     |
| episodes                | 205000   |
| lives                   | 205000   |
| mean 100 episode length | 6.03     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 1348377  |
| value_loss              | 6.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.111   |
| entropy                 | 2.43     |
| episodes                | 205100   |
| lives                   | 205100   |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 1348953  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.136   |
| entropy                 | 2.47     |
| episodes                | 205200   |
| lives                   | 205200   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0077   |
| steps                   | 1349516  |
| value_loss              | 6.14     |
--------------------------------------
Saving model due to running mean reward increase: 5.227 -> 5.3175
--------------------------------------
| approx_kl               | -0.119   |
| entropy                 | 2.39     |
| episodes                | 205300   |
| lives                   | 205300   |
| mean 100 episode length | 6.45     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 1350061  |
| value_loss              | 6.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0699  |
| entropy                 | 2.33     |
| episodes                | 205400   |
| lives                   | 205400   |
| mean 100 episode length | 6.7      |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0232  |
| steps                   | 1350631  |
| value_loss              | 6.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0913  |
| entropy                 | 2.5      |
| episodes                | 205500   |
| lives                   | 205500   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1351243  |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.109   |
| entropy                 | 2.5      |
| episodes                | 205600   |
| lives                   | 205600   |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 1351861  |
| value_loss              | 5.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.42     |
| episodes                | 205700   |
| lives                   | 205700   |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0207  |
| steps                   | 1352436  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.45     |
| episodes                | 205800   |
| lives                   | 205800   |
| mean 100 episode length | 6.45     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 1352981  |
| value_loss              | 4.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0932  |
| entropy                 | 2.48     |
| episodes                | 205900   |
| lives                   | 205900   |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1353590  |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.127   |
| entropy                 | 2.43     |
| episodes                | 206000   |
| lives                   | 206000   |
| mean 100 episode length | 6.7      |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 1354160  |
| value_loss              | 5.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0921  |
| entropy                 | 2.31     |
| episodes                | 206100   |
| lives                   | 206100   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1354711  |
| value_loss              | 5.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0993  |
| entropy                 | 2.45     |
| episodes                | 206200   |
| lives                   | 206200   |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.011    |
| steps                   | 1355319  |
| value_loss              | 6.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0677  |
| entropy                 | 2.45     |
| episodes                | 206300   |
| lives                   | 206300   |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1355936  |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.42     |
| episodes                | 206400   |
| lives                   | 206400   |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0212  |
| steps                   | 1356557  |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0855  |
| entropy                 | 2.48     |
| episodes                | 206500   |
| lives                   | 206500   |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1357171  |
| value_loss              | 6.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.089   |
| entropy                 | 2.53     |
| episodes                | 206600   |
| lives                   | 206600   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 4.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1357798  |
| value_loss              | 5.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.33     |
| episodes                | 206700   |
| lives                   | 206700   |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1358351  |
| value_loss              | 5.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.128   |
| entropy                 | 2.41     |
| episodes                | 206800   |
| lives                   | 206800   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0185  |
| steps                   | 1358917  |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.43     |
| episodes                | 206900   |
| lives                   | 206900   |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1359494  |
| value_loss              | 6.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.13    |
| entropy                 | 2.27     |
| episodes                | 207000   |
| lives                   | 207000   |
| mean 100 episode length | 6.22     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1360016  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.127   |
| entropy                 | 2.43     |
| episodes                | 207100   |
| lives                   | 207100   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1360596  |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0705  |
| entropy                 | 2.46     |
| episodes                | 207200   |
| lives                   | 207200   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 1361247  |
| value_loss              | 5.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.11    |
| entropy                 | 2.5      |
| episodes                | 207300   |
| lives                   | 207300   |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1361879  |
| value_loss              | 6.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0784  |
| entropy                 | 2.38     |
| episodes                | 207400   |
| lives                   | 207400   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1362474  |
| value_loss              | 6.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.121   |
| entropy                 | 2.38     |
| episodes                | 207500   |
| lives                   | 207500   |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 1363025  |
| value_loss              | 7.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0947  |
| entropy                 | 2.38     |
| episodes                | 207600   |
| lives                   | 207600   |
| mean 100 episode length | 6.44     |
| mean 100 episode reward | 4.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1363569  |
| value_loss              | 6.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0701  |
| entropy                 | 2.39     |
| episodes                | 207700   |
| lives                   | 207700   |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 5.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 1364141  |
| value_loss              | 6.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.065   |
| entropy                 | 2.47     |
| episodes                | 207800   |
| lives                   | 207800   |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0225  |
| steps                   | 1364764  |
| value_loss              | 6.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0607  |
| entropy                 | 2.5      |
| episodes                | 207900   |
| lives                   | 207900   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1365395  |
| value_loss              | 6.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.54     |
| episodes                | 208000   |
| lives                   | 208000   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 1366102  |
| value_loss              | 5.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0248  |
| entropy                 | 2.5      |
| episodes                | 208100   |
| lives                   | 208100   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 1366809  |
| value_loss              | 6.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.02    |
| entropy                 | 2.48     |
| episodes                | 208200   |
| lives                   | 208200   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 1367540  |
| value_loss              | 6.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0396  |
| entropy                 | 2.31     |
| episodes                | 208300   |
| lives                   | 208300   |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1368209  |
| value_loss              | 5.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0941  |
| entropy                 | 2.32     |
| episodes                | 208400   |
| lives                   | 208400   |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0152  |
| steps                   | 1368796  |
| value_loss              | 5.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.128   |
| entropy                 | 2.46     |
| episodes                | 208500   |
| lives                   | 208500   |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 4.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0238   |
| steps                   | 1369383  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.113   |
| entropy                 | 2.29     |
| episodes                | 208600   |
| lives                   | 208600   |
| mean 100 episode length | 5.99     |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1369882  |
| value_loss              | 5.4      |
--------------------------------------
Saving model due to running mean reward increase: 4.4756 -> 4.4973
--------------------------------------
| approx_kl               | -0.0919  |
| entropy                 | 2.35     |
| episodes                | 208700   |
| lives                   | 208700   |
| mean 100 episode length | 6.38     |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 1370420  |
| value_loss              | 4.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0669  |
| entropy                 | 2.4      |
| episodes                | 208800   |
| lives                   | 208800   |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 1371026  |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0715  |
| entropy                 | 2.49     |
| episodes                | 208900   |
| lives                   | 208900   |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1371648  |
| value_loss              | 5.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.108   |
| entropy                 | 2.41     |
| episodes                | 209000   |
| lives                   | 209000   |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 1372240  |
| value_loss              | 5.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0934  |
| entropy                 | 2.45     |
| episodes                | 209100   |
| lives                   | 209100   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 1372845  |
| value_loss              | 5.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0907  |
| entropy                 | 2.45     |
| episodes                | 209200   |
| lives                   | 209200   |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1373419  |
| value_loss              | 4.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0513  |
| entropy                 | 2.52     |
| episodes                | 209300   |
| lives                   | 209300   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1374079  |
| value_loss              | 5.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.043   |
| entropy                 | 2.5      |
| episodes                | 209400   |
| lives                   | 209400   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1374774  |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0752  |
| entropy                 | 2.52     |
| episodes                | 209500   |
| lives                   | 209500   |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1375409  |
| value_loss              | 4.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0989  |
| entropy                 | 2.48     |
| episodes                | 209600   |
| lives                   | 209600   |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 5        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1376001  |
| value_loss              | 5.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0949  |
| entropy                 | 2.47     |
| episodes                | 209700   |
| lives                   | 209700   |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1376563  |
| value_loss              | 5.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0953  |
| entropy                 | 2.56     |
| episodes                | 209800   |
| lives                   | 209800   |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 4.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 1377132  |
| value_loss              | 5.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0662  |
| entropy                 | 2.43     |
| episodes                | 209900   |
| lives                   | 209900   |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0068   |
| steps                   | 1377725  |
| value_loss              | 6.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0524  |
| entropy                 | 2.4      |
| episodes                | 210000   |
| lives                   | 210000   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0185  |
| steps                   | 1378306  |
| value_loss              | 4.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0873  |
| entropy                 | 2.36     |
| episodes                | 210100   |
| lives                   | 210100   |
| mean 100 episode length | 6.3      |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0198  |
| steps                   | 1378836  |
| value_loss              | 4.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.41     |
| episodes                | 210200   |
| lives                   | 210200   |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0216  |
| steps                   | 1379398  |
| value_loss              | 5.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0698  |
| entropy                 | 2.37     |
| episodes                | 210300   |
| lives                   | 210300   |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1379955  |
| value_loss              | 5.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0948  |
| entropy                 | 2.3      |
| episodes                | 210400   |
| lives                   | 210400   |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 4.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0166  |
| steps                   | 1380498  |
| value_loss              | 5.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0882  |
| entropy                 | 2.49     |
| episodes                | 210500   |
| lives                   | 210500   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1381092  |
| value_loss              | 6.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0858  |
| entropy                 | 2.29     |
| episodes                | 210600   |
| lives                   | 210600   |
| mean 100 episode length | 6.32     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0174  |
| steps                   | 1381624  |
| value_loss              | 5.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.118   |
| entropy                 | 2.42     |
| episodes                | 210700   |
| lives                   | 210700   |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1382145  |
| value_loss              | 5.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.126   |
| entropy                 | 2.43     |
| episodes                | 210800   |
| lives                   | 210800   |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 4.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 1382679  |
| value_loss              | 5.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0771  |
| entropy                 | 2.47     |
| episodes                | 210900   |
| lives                   | 210900   |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 1383266  |
| value_loss              | 5.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0727  |
| entropy                 | 2.54     |
| episodes                | 211000   |
| lives                   | 211000   |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 1383869  |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.13    |
| entropy                 | 2.39     |
| episodes                | 211100   |
| lives                   | 211100   |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0219  |
| steps                   | 1384412  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.142   |
| entropy                 | 2.51     |
| episodes                | 211200   |
| lives                   | 211200   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1384997  |
| value_loss              | 5.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.45     |
| episodes                | 211300   |
| lives                   | 211300   |
| mean 100 episode length | 6.41     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 1385538  |
| value_loss              | 5.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.308   |
| entropy                 | 2.46     |
| episodes                | 211400   |
| lives                   | 211400   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0649   |
| steps                   | 1386118  |
| value_loss              | 4.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.129   |
| entropy                 | 2.55     |
| episodes                | 211500   |
| lives                   | 211500   |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0389   |
| steps                   | 1386776  |
| value_loss              | 5.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0388  |
| entropy                 | 2.53     |
| episodes                | 211600   |
| lives                   | 211600   |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1387418  |
| value_loss              | 4.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.48     |
| episodes                | 211700   |
| lives                   | 211700   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1388079  |
| value_loss              | 4.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0346  |
| entropy                 | 2.42     |
| episodes                | 211800   |
| lives                   | 211800   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1388716  |
| value_loss              | 4.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0446  |
| entropy                 | 2.42     |
| episodes                | 211900   |
| lives                   | 211900   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1389365  |
| value_loss              | 4.68     |
--------------------------------------
Saving model due to running mean reward increase: 4.4577 -> 4.9998
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.47     |
| episodes                | 212000   |
| lives                   | 212000   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1390060  |
| value_loss              | 5.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0338  |
| entropy                 | 2.38     |
| episodes                | 212100   |
| lives                   | 212100   |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1390734  |
| value_loss              | 4.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0308  |
| entropy                 | 2.46     |
| episodes                | 212200   |
| lives                   | 212200   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 4.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 1391395  |
| value_loss              | 5.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0378  |
| entropy                 | 2.4      |
| episodes                | 212300   |
| lives                   | 212300   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 4.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1392011  |
| value_loss              | 5.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0407  |
| entropy                 | 2.45     |
| episodes                | 212400   |
| lives                   | 212400   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1392650  |
| value_loss              | 4.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0366  |
| entropy                 | 2.53     |
| episodes                | 212500   |
| lives                   | 212500   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1393358  |
| value_loss              | 5.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0331  |
| entropy                 | 2.57     |
| episodes                | 212600   |
| lives                   | 212600   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1394050  |
| value_loss              | 4.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0722  |
| entropy                 | 2.67     |
| episodes                | 212700   |
| lives                   | 212700   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0041   |
| steps                   | 1394790  |
| value_loss              | 5.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0677  |
| entropy                 | 2.51     |
| episodes                | 212800   |
| lives                   | 212800   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0095   |
| steps                   | 1395518  |
| value_loss              | 5.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.033   |
| entropy                 | 2.55     |
| episodes                | 212900   |
| lives                   | 212900   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1396266  |
| value_loss              | 5.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0648  |
| entropy                 | 2.6      |
| episodes                | 213000   |
| lives                   | 213000   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0023   |
| steps                   | 1396931  |
| value_loss              | 4.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0628  |
| entropy                 | 2.54     |
| episodes                | 213100   |
| lives                   | 213100   |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1397535  |
| value_loss              | 4.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0646  |
| entropy                 | 2.61     |
| episodes                | 213200   |
| lives                   | 213200   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 1398200  |
| value_loss              | 4.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0595  |
| entropy                 | 2.6      |
| episodes                | 213300   |
| lives                   | 213300   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1398849  |
| value_loss              | 4.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0429  |
| entropy                 | 2.55     |
| episodes                | 213400   |
| lives                   | 213400   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1399550  |
| value_loss              | 5.38     |
--------------------------------------
Saving model due to running mean reward increase: 5.9047 -> 6.1539
--------------------------------------
| approx_kl               | -0.0369  |
| entropy                 | 2.61     |
| episodes                | 213500   |
| lives                   | 213500   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1400262  |
| value_loss              | 5.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0341  |
| entropy                 | 2.54     |
| episodes                | 213600   |
| lives                   | 213600   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1400995  |
| value_loss              | 4.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0363  |
| entropy                 | 2.49     |
| episodes                | 213700   |
| lives                   | 213700   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0069   |
| steps                   | 1401711  |
| value_loss              | 4.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0759  |
| entropy                 | 2.63     |
| episodes                | 213800   |
| lives                   | 213800   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1402387  |
| value_loss              | 4.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0368  |
| entropy                 | 2.56     |
| episodes                | 213900   |
| lives                   | 213900   |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1403069  |
| value_loss              | 4.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0712  |
| entropy                 | 2.54     |
| episodes                | 214000   |
| lives                   | 214000   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1403717  |
| value_loss              | 4.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.059   |
| entropy                 | 2.49     |
| episodes                | 214100   |
| lives                   | 214100   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1404400  |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0372  |
| entropy                 | 2.54     |
| episodes                | 214200   |
| lives                   | 214200   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1405136  |
| value_loss              | 5.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0628  |
| entropy                 | 2.57     |
| episodes                | 214300   |
| lives                   | 214300   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.007    |
| steps                   | 1405838  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0813  |
| entropy                 | 2.64     |
| episodes                | 214400   |
| lives                   | 214400   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0128   |
| steps                   | 1406498  |
| value_loss              | 5.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.054   |
| entropy                 | 2.56     |
| episodes                | 214500   |
| lives                   | 214500   |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 1407138  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0543  |
| entropy                 | 2.47     |
| episodes                | 214600   |
| lives                   | 214600   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 1407786  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0512  |
| entropy                 | 2.38     |
| episodes                | 214700   |
| lives                   | 214700   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1408371  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0565  |
| entropy                 | 2.33     |
| episodes                | 214800   |
| lives                   | 214800   |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 1408930  |
| value_loss              | 5.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0693  |
| entropy                 | 2.34     |
| episodes                | 214900   |
| lives                   | 214900   |
| mean 100 episode length | 6.57     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.013   |
| steps                   | 1409487  |
| value_loss              | 5.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0692  |
| entropy                 | 2.37     |
| episodes                | 215000   |
| lives                   | 215000   |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0264  |
| steps                   | 1410049  |
| value_loss              | 5.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0546  |
| entropy                 | 2.54     |
| episodes                | 215100   |
| lives                   | 215100   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1410784  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.036   |
| entropy                 | 2.41     |
| episodes                | 215200   |
| lives                   | 215200   |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1411403  |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0882  |
| entropy                 | 2.54     |
| episodes                | 215300   |
| lives                   | 215300   |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0125   |
| steps                   | 1412006  |
| value_loss              | 5.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0241  |
| entropy                 | 2.46     |
| episodes                | 215400   |
| lives                   | 215400   |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 1412611  |
| value_loss              | 5.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0454  |
| entropy                 | 2.39     |
| episodes                | 215500   |
| lives                   | 215500   |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1413179  |
| value_loss              | 5.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0807  |
| entropy                 | 2.5      |
| episodes                | 215600   |
| lives                   | 215600   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 1413765  |
| value_loss              | 5.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0576  |
| entropy                 | 2.49     |
| episodes                | 215700   |
| lives                   | 215700   |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1414406  |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0489  |
| entropy                 | 2.4      |
| episodes                | 215800   |
| lives                   | 215800   |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1414983  |
| value_loss              | 5.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0661  |
| entropy                 | 2.38     |
| episodes                | 215900   |
| lives                   | 215900   |
| mean 100 episode length | 6.55     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 1415538  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0591  |
| entropy                 | 2.56     |
| episodes                | 216000   |
| lives                   | 216000   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 5.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1416187  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.041   |
| entropy                 | 2.61     |
| episodes                | 216100   |
| lives                   | 216100   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1416882  |
| value_loss              | 5.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0648  |
| entropy                 | 2.54     |
| episodes                | 216200   |
| lives                   | 216200   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 1417542  |
| value_loss              | 5.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0513  |
| entropy                 | 2.38     |
| episodes                | 216300   |
| lives                   | 216300   |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 1418110  |
| value_loss              | 6.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0435  |
| entropy                 | 2.47     |
| episodes                | 216400   |
| lives                   | 216400   |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 1418724  |
| value_loss              | 5.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0671  |
| entropy                 | 2.46     |
| episodes                | 216500   |
| lives                   | 216500   |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1419293  |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0727  |
| entropy                 | 2.44     |
| episodes                | 216600   |
| lives                   | 216600   |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 1419911  |
| value_loss              | 5.54     |
--------------------------------------
Saving model due to running mean reward increase: 5.2264 -> 5.4592
--------------------------------------
| approx_kl               | -0.0642  |
| entropy                 | 2.36     |
| episodes                | 216700   |
| lives                   | 216700   |
| mean 100 episode length | 6.42     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 1420453  |
| value_loss              | 5.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0753  |
| entropy                 | 2.35     |
| episodes                | 216800   |
| lives                   | 216800   |
| mean 100 episode length | 6.38     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 1420991  |
| value_loss              | 5.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0905  |
| entropy                 | 2.37     |
| episodes                | 216900   |
| lives                   | 216900   |
| mean 100 episode length | 6.26     |
| mean 100 episode reward | 4.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0203  |
| steps                   | 1421517  |
| value_loss              | 5.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0988  |
| entropy                 | 2.43     |
| episodes                | 217000   |
| lives                   | 217000   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 1422102  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0648  |
| entropy                 | 2.44     |
| episodes                | 217100   |
| lives                   | 217100   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1422690  |
| value_loss              | 5.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0667  |
| entropy                 | 2.44     |
| episodes                | 217200   |
| lives                   | 217200   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 1423257  |
| value_loss              | 5.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0723  |
| entropy                 | 2.48     |
| episodes                | 217300   |
| lives                   | 217300   |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.021   |
| steps                   | 1423848  |
| value_loss              | 5.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0744  |
| entropy                 | 2.35     |
| episodes                | 217400   |
| lives                   | 217400   |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0192  |
| steps                   | 1424408  |
| value_loss              | 5.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0649  |
| entropy                 | 2.49     |
| episodes                | 217500   |
| lives                   | 217500   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 1425046  |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0947  |
| entropy                 | 2.43     |
| episodes                | 217600   |
| lives                   | 217600   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 1425632  |
| value_loss              | 5.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0666  |
| entropy                 | 2.47     |
| episodes                | 217700   |
| lives                   | 217700   |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0248  |
| steps                   | 1426239  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0611  |
| entropy                 | 2.43     |
| episodes                | 217800   |
| lives                   | 217800   |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 1426843  |
| value_loss              | 5.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0665  |
| entropy                 | 2.4      |
| episodes                | 217900   |
| lives                   | 217900   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1427438  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0591  |
| entropy                 | 2.53     |
| episodes                | 218000   |
| lives                   | 218000   |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1428084  |
| value_loss              | 5.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0574  |
| entropy                 | 2.5      |
| episodes                | 218100   |
| lives                   | 218100   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0181  |
| steps                   | 1428723  |
| value_loss              | 5.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0832  |
| entropy                 | 2.41     |
| episodes                | 218200   |
| lives                   | 218200   |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 1429291  |
| value_loss              | 5.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0806  |
| entropy                 | 2.37     |
| episodes                | 218300   |
| lives                   | 218300   |
| mean 100 episode length | 6.47     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1429838  |
| value_loss              | 5.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0395  |
| entropy                 | 2.37     |
| episodes                | 218400   |
| lives                   | 218400   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1430401  |
| value_loss              | 5.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.051   |
| entropy                 | 2.33     |
| episodes                | 218500   |
| lives                   | 218500   |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 1430955  |
| value_loss              | 5.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0773  |
| entropy                 | 2.45     |
| episodes                | 218600   |
| lives                   | 218600   |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 5.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0122  |
| steps                   | 1431516  |
| value_loss              | 5.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0688  |
| entropy                 | 2.51     |
| episodes                | 218700   |
| lives                   | 218700   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1432131  |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0888  |
| entropy                 | 2.48     |
| episodes                | 218800   |
| lives                   | 218800   |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1432739  |
| value_loss              | 6.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0871  |
| entropy                 | 2.42     |
| episodes                | 218900   |
| lives                   | 218900   |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 1433321  |
| value_loss              | 5.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.126   |
| entropy                 | 2.4      |
| episodes                | 219000   |
| lives                   | 219000   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1433907  |
| value_loss              | 6.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0832  |
| entropy                 | 2.42     |
| episodes                | 219100   |
| lives                   | 219100   |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 5.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1434480  |
| value_loss              | 5.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0526  |
| entropy                 | 2.47     |
| episodes                | 219200   |
| lives                   | 219200   |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1435091  |
| value_loss              | 6.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0391  |
| entropy                 | 2.4      |
| episodes                | 219300   |
| lives                   | 219300   |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1435702  |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0884  |
| entropy                 | 2.34     |
| episodes                | 219400   |
| lives                   | 219400   |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1436262  |
| value_loss              | 5.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0317  |
| entropy                 | 2.33     |
| episodes                | 219500   |
| lives                   | 219500   |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 1436863  |
| value_loss              | 5.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0438  |
| entropy                 | 2.38     |
| episodes                | 219600   |
| lives                   | 219600   |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 1437485  |
| value_loss              | 5.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0625  |
| entropy                 | 2.42     |
| episodes                | 219700   |
| lives                   | 219700   |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 1438124  |
| value_loss              | 5.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0603  |
| entropy                 | 2.43     |
| episodes                | 219800   |
| lives                   | 219800   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 1438740  |
| value_loss              | 5.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0512  |
| entropy                 | 2.35     |
| episodes                | 219900   |
| lives                   | 219900   |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1439306  |
| value_loss              | 5.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0588  |
| entropy                 | 2.3      |
| episodes                | 220000   |
| lives                   | 220000   |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1439859  |
| value_loss              | 6.27     |
--------------------------------------
Saving model due to running mean reward increase: 5.0813 -> 5.0915
--------------------------------------
| approx_kl               | -0.0494  |
| entropy                 | 2.37     |
| episodes                | 220100   |
| lives                   | 220100   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1440447  |
| value_loss              | 6.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0575  |
| entropy                 | 2.46     |
| episodes                | 220200   |
| lives                   | 220200   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 1441148  |
| value_loss              | 6.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0287  |
| entropy                 | 2.46     |
| episodes                | 220300   |
| lives                   | 220300   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 1441827  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0873  |
| entropy                 | 2.44     |
| episodes                | 220400   |
| lives                   | 220400   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1442488  |
| value_loss              | 5.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.15    |
| entropy                 | 2.49     |
| episodes                | 220500   |
| lives                   | 220500   |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0209   |
| steps                   | 1443138  |
| value_loss              | 6.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0568  |
| entropy                 | 2.58     |
| episodes                | 220600   |
| lives                   | 220600   |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0167   |
| steps                   | 1443810  |
| value_loss              | 5.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0199  |
| entropy                 | 2.45     |
| episodes                | 220700   |
| lives                   | 220700   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1444546  |
| value_loss              | 5.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0245  |
| entropy                 | 2.43     |
| episodes                | 220800   |
| lives                   | 220800   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1445242  |
| value_loss              | 4.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.44     |
| episodes                | 220900   |
| lives                   | 220900   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1445933  |
| value_loss              | 4.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.034   |
| entropy                 | 2.5      |
| episodes                | 221000   |
| lives                   | 221000   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1446667  |
| value_loss              | 5.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0279  |
| entropy                 | 2.48     |
| episodes                | 221100   |
| lives                   | 221100   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 1447362  |
| value_loss              | 5.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0241  |
| entropy                 | 2.45     |
| episodes                | 221200   |
| lives                   | 221200   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 1448059  |
| value_loss              | 4.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0344  |
| entropy                 | 2.46     |
| episodes                | 221300   |
| lives                   | 221300   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1448771  |
| value_loss              | 5.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0399  |
| entropy                 | 2.45     |
| episodes                | 221400   |
| lives                   | 221400   |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0205  |
| steps                   | 1449393  |
| value_loss              | 4.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0232  |
| entropy                 | 2.38     |
| episodes                | 221500   |
| lives                   | 221500   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.001    |
| steps                   | 1450027  |
| value_loss              | 4.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.49     |
| episodes                | 221600   |
| lives                   | 221600   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1450679  |
| value_loss              | 5.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0285  |
| entropy                 | 2.5      |
| episodes                | 221700   |
| lives                   | 221700   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 1451406  |
| value_loss              | 6.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.48     |
| episodes                | 221800   |
| lives                   | 221800   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1452102  |
| value_loss              | 5.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.43     |
| episodes                | 221900   |
| lives                   | 221900   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 1452814  |
| value_loss              | 6.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0154  |
| entropy                 | 2.49     |
| episodes                | 222000   |
| lives                   | 222000   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 1453554  |
| value_loss              | 6.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 2.45     |
| episodes                | 222100   |
| lives                   | 222100   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1454276  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.33     |
| episodes                | 222200   |
| lives                   | 222200   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 1454974  |
| value_loss              | 5.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0198  |
| entropy                 | 2.45     |
| episodes                | 222300   |
| lives                   | 222300   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 1455689  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0337  |
| entropy                 | 2.36     |
| episodes                | 222400   |
| lives                   | 222400   |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1456358  |
| value_loss              | 5.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0328  |
| entropy                 | 2.48     |
| episodes                | 222500   |
| lives                   | 222500   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 1457100  |
| value_loss              | 5.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0318  |
| entropy                 | 2.43     |
| episodes                | 222600   |
| lives                   | 222600   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1457786  |
| value_loss              | 5.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0874  |
| entropy                 | 2.59     |
| episodes                | 222700   |
| lives                   | 222700   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0001   |
| steps                   | 1458423  |
| value_loss              | 4.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0449  |
| entropy                 | 2.65     |
| episodes                | 222800   |
| lives                   | 222800   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1459141  |
| value_loss              | 5.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0333  |
| entropy                 | 2.47     |
| episodes                | 222900   |
| lives                   | 222900   |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1459778  |
| value_loss              | 4.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.56     |
| episodes                | 223000   |
| lives                   | 223000   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 1460451  |
| value_loss              | 5.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0269  |
| entropy                 | 2.48     |
| episodes                | 223100   |
| lives                   | 223100   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1461204  |
| value_loss              | 6.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0534  |
| entropy                 | 2.53     |
| episodes                | 223200   |
| lives                   | 223200   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0285   |
| steps                   | 1461855  |
| value_loss              | 5.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0365  |
| entropy                 | 2.61     |
| episodes                | 223300   |
| lives                   | 223300   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0097   |
| steps                   | 1462573  |
| value_loss              | 5.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.53     |
| episodes                | 223400   |
| lives                   | 223400   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0026   |
| steps                   | 1463309  |
| value_loss              | 5.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.42     |
| episodes                | 223500   |
| lives                   | 223500   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1463988  |
| value_loss              | 6.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0167  |
| entropy                 | 2.49     |
| episodes                | 223600   |
| lives                   | 223600   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1464742  |
| value_loss              | 6.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.47     |
| episodes                | 223700   |
| lives                   | 223700   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1465470  |
| value_loss              | 6.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.43     |
| episodes                | 223800   |
| lives                   | 223800   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 1466206  |
| value_loss              | 6.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0632  |
| entropy                 | 2.41     |
| episodes                | 223900   |
| lives                   | 223900   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0288   |
| steps                   | 1466876  |
| value_loss              | 5.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0212  |
| entropy                 | 2.34     |
| episodes                | 224000   |
| lives                   | 224000   |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 3.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 1467444  |
| value_loss              | 5.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0324  |
| entropy                 | 2.34     |
| episodes                | 224100   |
| lives                   | 224100   |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 3.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 1468002  |
| value_loss              | 5.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0332  |
| entropy                 | 2.47     |
| episodes                | 224200   |
| lives                   | 224200   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 1468672  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0542  |
| entropy                 | 2.53     |
| episodes                | 224300   |
| lives                   | 224300   |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 4.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1469306  |
| value_loss              | 5.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0531  |
| entropy                 | 2.53     |
| episodes                | 224400   |
| lives                   | 224400   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 1469961  |
| value_loss              | 5.46     |
--------------------------------------
Saving model due to running mean reward increase: 4.5037 -> 4.5887
--------------------------------------
| approx_kl               | -0.0601  |
| entropy                 | 2.53     |
| episodes                | 224500   |
| lives                   | 224500   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 5.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 1470658  |
| value_loss              | 5.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.112   |
| entropy                 | 2.37     |
| episodes                | 224600   |
| lives                   | 224600   |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0285   |
| steps                   | 1471262  |
| value_loss              | 5.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.033   |
| entropy                 | 2.1      |
| episodes                | 224700   |
| lives                   | 224700   |
| mean 100 episode length | 5.9      |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0023   |
| steps                   | 1471752  |
| value_loss              | 5.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0662  |
| entropy                 | 2.41     |
| episodes                | 224800   |
| lives                   | 224800   |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1472340  |
| value_loss              | 5.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.057   |
| entropy                 | 2.35     |
| episodes                | 224900   |
| lives                   | 224900   |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1472919  |
| value_loss              | 5.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0554  |
| entropy                 | 2.46     |
| episodes                | 225000   |
| lives                   | 225000   |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 1473529  |
| value_loss              | 4.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0637  |
| entropy                 | 2.45     |
| episodes                | 225100   |
| lives                   | 225100   |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1474113  |
| value_loss              | 4.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0197  |
| entropy                 | 2.41     |
| episodes                | 225200   |
| lives                   | 225200   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 1474819  |
| value_loss              | 4.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0118  |
| entropy                 | 2.34     |
| episodes                | 225300   |
| lives                   | 225300   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0138  |
| steps                   | 1475547  |
| value_loss              | 4.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.45     |
| episodes                | 225400   |
| lives                   | 225400   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 1476290  |
| value_loss              | 4.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0669  |
| entropy                 | 2.44     |
| episodes                | 225500   |
| lives                   | 225500   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0086   |
| steps                   | 1477040  |
| value_loss              | 4.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0336  |
| entropy                 | 2.46     |
| episodes                | 225600   |
| lives                   | 225600   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1477705  |
| value_loss              | 4.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.021   |
| entropy                 | 2.45     |
| episodes                | 225700   |
| lives                   | 225700   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1478385  |
| value_loss              | 4.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.44     |
| episodes                | 225800   |
| lives                   | 225800   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0146  |
| steps                   | 1479092  |
| value_loss              | 5.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0084  |
| entropy                 | 2.39     |
| episodes                | 225900   |
| lives                   | 225900   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1479815  |
| value_loss              | 4.91     |
--------------------------------------
Saving model due to running mean reward increase: 5.6791 -> 6.4201
--------------------------------------
| approx_kl               | -0.0325  |
| entropy                 | 2.44     |
| episodes                | 226000   |
| lives                   | 226000   |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 1480527  |
| value_loss              | 4.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.54     |
| episodes                | 226100   |
| lives                   | 226100   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1481262  |
| value_loss              | 4.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0243  |
| entropy                 | 2.44     |
| episodes                | 226200   |
| lives                   | 226200   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1481963  |
| value_loss              | 4.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0303  |
| entropy                 | 2.44     |
| episodes                | 226300   |
| lives                   | 226300   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1482653  |
| value_loss              | 4.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0559  |
| entropy                 | 2.48     |
| episodes                | 226400   |
| lives                   | 226400   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0157  |
| steps                   | 1483304  |
| value_loss              | 4.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0515  |
| entropy                 | 2.45     |
| episodes                | 226500   |
| lives                   | 226500   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1483959  |
| value_loss              | 5.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0082  |
| entropy                 | 2.52     |
| episodes                | 226600   |
| lives                   | 226600   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 6.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 1484770  |
| value_loss              | 5.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0122  |
| entropy                 | 2.47     |
| episodes                | 226700   |
| lives                   | 226700   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 1485532  |
| value_loss              | 5.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0085  |
| entropy                 | 2.41     |
| episodes                | 226800   |
| lives                   | 226800   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 1486253  |
| value_loss              | 5.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0195  |
| entropy                 | 2.41     |
| episodes                | 226900   |
| lives                   | 226900   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 1487000  |
| value_loss              | 4.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0289  |
| entropy                 | 2.43     |
| episodes                | 227000   |
| lives                   | 227000   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1487754  |
| value_loss              | 4.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.013   |
| entropy                 | 2.42     |
| episodes                | 227100   |
| lives                   | 227100   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1488471  |
| value_loss              | 4.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0393  |
| entropy                 | 2.51     |
| episodes                | 227200   |
| lives                   | 227200   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0009   |
| steps                   | 1489191  |
| value_loss              | 4.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0396  |
| entropy                 | 2.53     |
| episodes                | 227300   |
| lives                   | 227300   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0001   |
| steps                   | 1489861  |
| value_loss              | 5.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.046   |
| entropy                 | 2.57     |
| episodes                | 227400   |
| lives                   | 227400   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 1490596  |
| value_loss              | 5.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0524  |
| entropy                 | 2.64     |
| episodes                | 227500   |
| lives                   | 227500   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1491327  |
| value_loss              | 4.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0493  |
| entropy                 | 2.58     |
| episodes                | 227600   |
| lives                   | 227600   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 1492014  |
| value_loss              | 5.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0399  |
| entropy                 | 2.58     |
| episodes                | 227700   |
| lives                   | 227700   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1492750  |
| value_loss              | 5.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.51     |
| episodes                | 227800   |
| lives                   | 227800   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1493514  |
| value_loss              | 5.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0391  |
| entropy                 | 2.56     |
| episodes                | 227900   |
| lives                   | 227900   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1494193  |
| value_loss              | 4.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0682  |
| entropy                 | 2.53     |
| episodes                | 228000   |
| lives                   | 228000   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 1494943  |
| value_loss              | 4.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0889  |
| entropy                 | 2.55     |
| episodes                | 228100   |
| lives                   | 228100   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0       |
| steps                   | 1495663  |
| value_loss              | 4.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0315  |
| entropy                 | 2.59     |
| episodes                | 228200   |
| lives                   | 228200   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 1496389  |
| value_loss              | 5.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0219  |
| entropy                 | 2.53     |
| episodes                | 228300   |
| lives                   | 228300   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 1497181  |
| value_loss              | 5.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0455  |
| entropy                 | 2.63     |
| episodes                | 228400   |
| lives                   | 228400   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 1497952  |
| value_loss              | 4.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0399  |
| entropy                 | 2.59     |
| episodes                | 228500   |
| lives                   | 228500   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 1498656  |
| value_loss              | 3.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0192  |
| entropy                 | 2.54     |
| episodes                | 228600   |
| lives                   | 228600   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1499404  |
| value_loss              | 3.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0382  |
| entropy                 | 2.44     |
| episodes                | 228700   |
| lives                   | 228700   |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1500078  |
| value_loss              | 4.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.53     |
| episodes                | 228800   |
| lives                   | 228800   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0186  |
| steps                   | 1500838  |
| value_loss              | 4.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0452  |
| entropy                 | 2.49     |
| episodes                | 228900   |
| lives                   | 228900   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0122   |
| steps                   | 1501548  |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0203  |
| entropy                 | 2.42     |
| episodes                | 229000   |
| lives                   | 229000   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1502296  |
| value_loss              | 4.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.01    |
| entropy                 | 2.36     |
| episodes                | 229100   |
| lives                   | 229100   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1503005  |
| value_loss              | 3.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0194  |
| entropy                 | 2.39     |
| episodes                | 229200   |
| lives                   | 229200   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1503712  |
| value_loss              | 3.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0151  |
| entropy                 | 2.42     |
| episodes                | 229300   |
| lives                   | 229300   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1504462  |
| value_loss              | 3.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0073  |
| entropy                 | 2.33     |
| episodes                | 229400   |
| lives                   | 229400   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1505189  |
| value_loss              | 3.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.37     |
| episodes                | 229500   |
| lives                   | 229500   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1505922  |
| value_loss              | 4.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0352  |
| entropy                 | 2.43     |
| episodes                | 229600   |
| lives                   | 229600   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1506664  |
| value_loss              | 3.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.016   |
| entropy                 | 2.43     |
| episodes                | 229700   |
| lives                   | 229700   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1507411  |
| value_loss              | 4.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.46     |
| episodes                | 229800   |
| lives                   | 229800   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0006   |
| steps                   | 1508189  |
| value_loss              | 4.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0185  |
| entropy                 | 2.52     |
| episodes                | 229900   |
| lives                   | 229900   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0133  |
| steps                   | 1508971  |
| value_loss              | 3.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.44     |
| episodes                | 230000   |
| lives                   | 230000   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 1509741  |
| value_loss              | 4.12     |
--------------------------------------
Saving model due to running mean reward increase: 6.0856 -> 6.8812
--------------------------------------
| approx_kl               | -0.0203  |
| entropy                 | 2.4      |
| episodes                | 230100   |
| lives                   | 230100   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 1510470  |
| value_loss              | 3.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0087  |
| entropy                 | 2.43     |
| episodes                | 230200   |
| lives                   | 230200   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1511224  |
| value_loss              | 3.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0209  |
| entropy                 | 2.39     |
| episodes                | 230300   |
| lives                   | 230300   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 1511964  |
| value_loss              | 3.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0362  |
| entropy                 | 2.53     |
| episodes                | 230400   |
| lives                   | 230400   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1512720  |
| value_loss              | 4.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0235  |
| entropy                 | 2.53     |
| episodes                | 230500   |
| lives                   | 230500   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1513489  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0323  |
| entropy                 | 2.54     |
| episodes                | 230600   |
| lives                   | 230600   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1514229  |
| value_loss              | 3.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0302  |
| entropy                 | 2.57     |
| episodes                | 230700   |
| lives                   | 230700   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0014   |
| steps                   | 1515001  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0251  |
| entropy                 | 2.54     |
| episodes                | 230800   |
| lives                   | 230800   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1515758  |
| value_loss              | 3.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.54     |
| episodes                | 230900   |
| lives                   | 230900   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 5.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1516459  |
| value_loss              | 3.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0139  |
| entropy                 | 2.5      |
| episodes                | 231000   |
| lives                   | 231000   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1517180  |
| value_loss              | 3.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0584  |
| entropy                 | 2.52     |
| episodes                | 231100   |
| lives                   | 231100   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1517923  |
| value_loss              | 3.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.03    |
| entropy                 | 2.4      |
| episodes                | 231200   |
| lives                   | 231200   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 1518656  |
| value_loss              | 3.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0212  |
| entropy                 | 2.44     |
| episodes                | 231300   |
| lives                   | 231300   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 1519427  |
| value_loss              | 3.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.027   |
| entropy                 | 2.4      |
| episodes                | 231400   |
| lives                   | 231400   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1520157  |
| value_loss              | 3.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0145  |
| entropy                 | 2.4      |
| episodes                | 231500   |
| lives                   | 231500   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1520857  |
| value_loss              | 3.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0301  |
| entropy                 | 2.45     |
| episodes                | 231600   |
| lives                   | 231600   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1521593  |
| value_loss              | 4.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0491  |
| entropy                 | 2.53     |
| episodes                | 231700   |
| lives                   | 231700   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1522260  |
| value_loss              | 3.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.45     |
| episodes                | 231800   |
| lives                   | 231800   |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 1522916  |
| value_loss              | 3.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0558  |
| entropy                 | 2.43     |
| episodes                | 231900   |
| lives                   | 231900   |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 1523534  |
| value_loss              | 4.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.44     |
| episodes                | 232000   |
| lives                   | 232000   |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1524174  |
| value_loss              | 3.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0718  |
| entropy                 | 2.57     |
| episodes                | 232100   |
| lives                   | 232100   |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 1524837  |
| value_loss              | 4.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0294  |
| entropy                 | 2.48     |
| episodes                | 232200   |
| lives                   | 232200   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 1525569  |
| value_loss              | 5.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0247  |
| entropy                 | 2.53     |
| episodes                | 232300   |
| lives                   | 232300   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 1526314  |
| value_loss              | 4.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.47     |
| episodes                | 232400   |
| lives                   | 232400   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1527056  |
| value_loss              | 4.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0325  |
| entropy                 | 2.52     |
| episodes                | 232500   |
| lives                   | 232500   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1527802  |
| value_loss              | 4.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0129  |
| entropy                 | 2.47     |
| episodes                | 232600   |
| lives                   | 232600   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1528555  |
| value_loss              | 3.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0088  |
| entropy                 | 2.48     |
| episodes                | 232700   |
| lives                   | 232700   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1529348  |
| value_loss              | 4.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0307  |
| entropy                 | 2.4      |
| episodes                | 232800   |
| lives                   | 232800   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1530083  |
| value_loss              | 4.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.44     |
| episodes                | 232900   |
| lives                   | 232900   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1530828  |
| value_loss              | 3.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.043   |
| entropy                 | 2.44     |
| episodes                | 233000   |
| lives                   | 233000   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0007   |
| steps                   | 1531567  |
| value_loss              | 3.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0253  |
| entropy                 | 2.38     |
| episodes                | 233100   |
| lives                   | 233100   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1532298  |
| value_loss              | 3.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0227  |
| entropy                 | 2.41     |
| episodes                | 233200   |
| lives                   | 233200   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1533001  |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0171  |
| entropy                 | 2.46     |
| episodes                | 233300   |
| lives                   | 233300   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1533743  |
| value_loss              | 3.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0107  |
| entropy                 | 2.31     |
| episodes                | 233400   |
| lives                   | 233400   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 1534461  |
| value_loss              | 3.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0154  |
| entropy                 | 2.34     |
| episodes                | 233500   |
| lives                   | 233500   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0141  |
| steps                   | 1535161  |
| value_loss              | 3.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0339  |
| entropy                 | 2.49     |
| episodes                | 233600   |
| lives                   | 233600   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1535872  |
| value_loss              | 3.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0381  |
| entropy                 | 2.56     |
| episodes                | 233700   |
| lives                   | 233700   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 1536588  |
| value_loss              | 4.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0419  |
| entropy                 | 2.51     |
| episodes                | 233800   |
| lives                   | 233800   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0123  |
| steps                   | 1537274  |
| value_loss              | 3.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0675  |
| entropy                 | 2.47     |
| episodes                | 233900   |
| lives                   | 233900   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 4.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0199  |
| steps                   | 1537889  |
| value_loss              | 4.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0349  |
| entropy                 | 2.55     |
| episodes                | 234000   |
| lives                   | 234000   |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 4.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 1538564  |
| value_loss              | 3.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0594  |
| entropy                 | 2.51     |
| episodes                | 234100   |
| lives                   | 234100   |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1539208  |
| value_loss              | 3.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0825  |
| entropy                 | 2.56     |
| episodes                | 234200   |
| lives                   | 234200   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1539879  |
| value_loss              | 3.65     |
--------------------------------------
Saving model due to running mean reward increase: 4.8437 -> 4.9393
--------------------------------------
| approx_kl               | -0.0931  |
| entropy                 | 2.54     |
| episodes                | 234300   |
| lives                   | 234300   |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 5        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1540530  |
| value_loss              | 4.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0766  |
| entropy                 | 2.54     |
| episodes                | 234400   |
| lives                   | 234400   |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1541185  |
| value_loss              | 4.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0759  |
| entropy                 | 2.52     |
| episodes                | 234500   |
| lives                   | 234500   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1541850  |
| value_loss              | 4.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0383  |
| entropy                 | 2.49     |
| episodes                | 234600   |
| lives                   | 234600   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1542517  |
| value_loss              | 4.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0661  |
| entropy                 | 2.41     |
| episodes                | 234700   |
| lives                   | 234700   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1543148  |
| value_loss              | 4.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0423  |
| entropy                 | 2.42     |
| episodes                | 234800   |
| lives                   | 234800   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1543855  |
| value_loss              | 4.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0346  |
| entropy                 | 2.4      |
| episodes                | 234900   |
| lives                   | 234900   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1544566  |
| value_loss              | 4.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0313  |
| entropy                 | 2.42     |
| episodes                | 235000   |
| lives                   | 235000   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 1545300  |
| value_loss              | 4.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0137  |
| entropy                 | 2.44     |
| episodes                | 235100   |
| lives                   | 235100   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1546053  |
| value_loss              | 3.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.44     |
| episodes                | 235200   |
| lives                   | 235200   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 1546792  |
| value_loss              | 3.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.014   |
| entropy                 | 2.44     |
| episodes                | 235300   |
| lives                   | 235300   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 1547510  |
| value_loss              | 3.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0217  |
| entropy                 | 2.44     |
| episodes                | 235400   |
| lives                   | 235400   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 1548243  |
| value_loss              | 3.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0138  |
| entropy                 | 2.48     |
| episodes                | 235500   |
| lives                   | 235500   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1548976  |
| value_loss              | 3.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0095  |
| entropy                 | 2.48     |
| episodes                | 235600   |
| lives                   | 235600   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1549741  |
| value_loss              | 3.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0135  |
| entropy                 | 2.48     |
| episodes                | 235700   |
| lives                   | 235700   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1550554  |
| value_loss              | 3.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0129  |
| entropy                 | 2.36     |
| episodes                | 235800   |
| lives                   | 235800   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1551293  |
| value_loss              | 3.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0097  |
| entropy                 | 2.41     |
| episodes                | 235900   |
| lives                   | 235900   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 1552042  |
| value_loss              | 3.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0265  |
| entropy                 | 2.46     |
| episodes                | 236000   |
| lives                   | 236000   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1552750  |
| value_loss              | 3.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0411  |
| entropy                 | 2.52     |
| episodes                | 236100   |
| lives                   | 236100   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0015   |
| steps                   | 1553497  |
| value_loss              | 3.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.027   |
| entropy                 | 2.48     |
| episodes                | 236200   |
| lives                   | 236200   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 1554217  |
| value_loss              | 3.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0248  |
| entropy                 | 2.48     |
| episodes                | 236300   |
| lives                   | 236300   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1554941  |
| value_loss              | 3.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0165  |
| entropy                 | 2.48     |
| episodes                | 236400   |
| lives                   | 236400   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 1555713  |
| value_loss              | 3.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0091  |
| entropy                 | 2.44     |
| episodes                | 236500   |
| lives                   | 236500   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 7.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1556492  |
| value_loss              | 4.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.42     |
| episodes                | 236600   |
| lives                   | 236600   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1557200  |
| value_loss              | 3.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.026   |
| entropy                 | 2.43     |
| episodes                | 236700   |
| lives                   | 236700   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1557916  |
| value_loss              | 3.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.49     |
| episodes                | 236800   |
| lives                   | 236800   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1558623  |
| value_loss              | 3.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0185  |
| entropy                 | 2.44     |
| episodes                | 236900   |
| lives                   | 236900   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1559339  |
| value_loss              | 3.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0196  |
| entropy                 | 2.44     |
| episodes                | 237000   |
| lives                   | 237000   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 1560080  |
| value_loss              | 3.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0204  |
| entropy                 | 2.45     |
| episodes                | 237100   |
| lives                   | 237100   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1560847  |
| value_loss              | 4        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0332  |
| entropy                 | 2.49     |
| episodes                | 237200   |
| lives                   | 237200   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1561578  |
| value_loss              | 3.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0148  |
| entropy                 | 2.4      |
| episodes                | 237300   |
| lives                   | 237300   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 1562321  |
| value_loss              | 3.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0415  |
| entropy                 | 2.39     |
| episodes                | 237400   |
| lives                   | 237400   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1563026  |
| value_loss              | 3.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.015   |
| entropy                 | 2.31     |
| episodes                | 237500   |
| lives                   | 237500   |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0179  |
| steps                   | 1563676  |
| value_loss              | 3.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0365  |
| entropy                 | 2.47     |
| episodes                | 237600   |
| lives                   | 237600   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1564392  |
| value_loss              | 3.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.5      |
| episodes                | 237700   |
| lives                   | 237700   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1565135  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0139  |
| entropy                 | 2.48     |
| episodes                | 237800   |
| lives                   | 237800   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1565886  |
| value_loss              | 3.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.033   |
| entropy                 | 2.53     |
| episodes                | 237900   |
| lives                   | 237900   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1566651  |
| value_loss              | 3.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0347  |
| entropy                 | 2.52     |
| episodes                | 238000   |
| lives                   | 238000   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0025   |
| steps                   | 1567397  |
| value_loss              | 3.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0238  |
| entropy                 | 2.43     |
| episodes                | 238100   |
| lives                   | 238100   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1568146  |
| value_loss              | 4        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0277  |
| entropy                 | 2.4      |
| episodes                | 238200   |
| lives                   | 238200   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1568881  |
| value_loss              | 3.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0215  |
| entropy                 | 2.42     |
| episodes                | 238300   |
| lives                   | 238300   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1569602  |
| value_loss              | 4        |
--------------------------------------
Saving model due to running mean reward increase: 6.3886 -> 6.7826
--------------------------------------
| approx_kl               | -0.0239  |
| entropy                 | 2.38     |
| episodes                | 238400   |
| lives                   | 238400   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1570297  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0379  |
| entropy                 | 2.38     |
| episodes                | 238500   |
| lives                   | 238500   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1570975  |
| value_loss              | 3.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0631  |
| entropy                 | 2.48     |
| episodes                | 238600   |
| lives                   | 238600   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1571670  |
| value_loss              | 3.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0254  |
| entropy                 | 2.47     |
| episodes                | 238700   |
| lives                   | 238700   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1572445  |
| value_loss              | 3.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0182  |
| entropy                 | 2.43     |
| episodes                | 238800   |
| lives                   | 238800   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1573155  |
| value_loss              | 3.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0208  |
| entropy                 | 2.48     |
| episodes                | 238900   |
| lives                   | 238900   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1573922  |
| value_loss              | 3.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0165  |
| entropy                 | 2.4      |
| episodes                | 239000   |
| lives                   | 239000   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1574657  |
| value_loss              | 3.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0246  |
| entropy                 | 2.48     |
| episodes                | 239100   |
| lives                   | 239100   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1575393  |
| value_loss              | 3.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0229  |
| entropy                 | 2.46     |
| episodes                | 239200   |
| lives                   | 239200   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 7.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1576151  |
| value_loss              | 3.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0193  |
| entropy                 | 2.43     |
| episodes                | 239300   |
| lives                   | 239300   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1576885  |
| value_loss              | 3.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0299  |
| entropy                 | 2.52     |
| episodes                | 239400   |
| lives                   | 239400   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 1577608  |
| value_loss              | 3.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.043   |
| entropy                 | 2.48     |
| episodes                | 239500   |
| lives                   | 239500   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0105  |
| steps                   | 1578292  |
| value_loss              | 3.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.43     |
| episodes                | 239600   |
| lives                   | 239600   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1579009  |
| value_loss              | 3.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.32     |
| episodes                | 239700   |
| lives                   | 239700   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1579748  |
| value_loss              | 4.02     |
--------------------------------------
Saving model due to running mean reward increase: 6.4317 -> 6.9178
--------------------------------------
| approx_kl               | -0.0259  |
| entropy                 | 2.35     |
| episodes                | 239800   |
| lives                   | 239800   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1580443  |
| value_loss              | 3.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0531  |
| entropy                 | 2.48     |
| episodes                | 239900   |
| lives                   | 239900   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1581123  |
| value_loss              | 3.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0916  |
| entropy                 | 2.47     |
| episodes                | 240000   |
| lives                   | 240000   |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0205  |
| steps                   | 1581759  |
| value_loss              | 3.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0837  |
| entropy                 | 2.47     |
| episodes                | 240100   |
| lives                   | 240100   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0182  |
| steps                   | 1582349  |
| value_loss              | 3.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.109   |
| entropy                 | 2.45     |
| episodes                | 240200   |
| lives                   | 240200   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1582930  |
| value_loss              | 3.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.099   |
| entropy                 | 2.49     |
| episodes                | 240300   |
| lives                   | 240300   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 1583542  |
| value_loss              | 3.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.11    |
| entropy                 | 2.51     |
| episodes                | 240400   |
| lives                   | 240400   |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 1584155  |
| value_loss              | 3.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.13    |
| entropy                 | 2.58     |
| episodes                | 240500   |
| lives                   | 240500   |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0137  |
| steps                   | 1584780  |
| value_loss              | 3.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0962  |
| entropy                 | 2.54     |
| episodes                | 240600   |
| lives                   | 240600   |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 4.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 1585408  |
| value_loss              | 3.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.57     |
| episodes                | 240700   |
| lives                   | 240700   |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0145  |
| steps                   | 1586095  |
| value_loss              | 3.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0681  |
| entropy                 | 2.48     |
| episodes                | 240800   |
| lives                   | 240800   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 1586763  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0194  |
| entropy                 | 2.54     |
| episodes                | 240900   |
| lives                   | 240900   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 1587549  |
| value_loss              | 3.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0207  |
| entropy                 | 2.45     |
| episodes                | 241000   |
| lives                   | 241000   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1588297  |
| value_loss              | 3.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.43     |
| episodes                | 241100   |
| lives                   | 241100   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1589038  |
| value_loss              | 3.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0124  |
| entropy                 | 2.47     |
| episodes                | 241200   |
| lives                   | 241200   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1589810  |
| value_loss              | 3.69     |
--------------------------------------
Saving model due to running mean reward increase: 6.4931 -> 6.5393
--------------------------------------
| approx_kl               | -0.019   |
| entropy                 | 2.41     |
| episodes                | 241300   |
| lives                   | 241300   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1590565  |
| value_loss              | 3.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.023   |
| entropy                 | 2.47     |
| episodes                | 241400   |
| lives                   | 241400   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1591346  |
| value_loss              | 3.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0181  |
| entropy                 | 2.38     |
| episodes                | 241500   |
| lives                   | 241500   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1592052  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0174  |
| entropy                 | 2.4      |
| episodes                | 241600   |
| lives                   | 241600   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1592740  |
| value_loss              | 3.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.011   |
| entropy                 | 2.45     |
| episodes                | 241700   |
| lives                   | 241700   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 7.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0128  |
| steps                   | 1593537  |
| value_loss              | 4.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0155  |
| entropy                 | 2.42     |
| episodes                | 241800   |
| lives                   | 241800   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 1594266  |
| value_loss              | 3.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0069  |
| entropy                 | 2.36     |
| episodes                | 241900   |
| lives                   | 241900   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 1594968  |
| value_loss              | 3.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0072  |
| entropy                 | 2.37     |
| episodes                | 242000   |
| lives                   | 242000   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 1595692  |
| value_loss              | 4.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0158  |
| entropy                 | 2.45     |
| episodes                | 242100   |
| lives                   | 242100   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1596432  |
| value_loss              | 3.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0264  |
| entropy                 | 2.4      |
| episodes                | 242200   |
| lives                   | 242200   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1597140  |
| value_loss              | 3.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0295  |
| entropy                 | 2.43     |
| episodes                | 242300   |
| lives                   | 242300   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0117  |
| steps                   | 1597876  |
| value_loss              | 3.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.034   |
| entropy                 | 2.44     |
| episodes                | 242400   |
| lives                   | 242400   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1598596  |
| value_loss              | 4.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0288  |
| entropy                 | 2.31     |
| episodes                | 242500   |
| lives                   | 242500   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1599276  |
| value_loss              | 3.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0249  |
| entropy                 | 2.39     |
| episodes                | 242600   |
| lives                   | 242600   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1600000  |
| value_loss              | 3.96     |
--------------------------------------
Saving model due to running mean reward increase: 6.1796 -> 6.8474
--------------------------------------
| approx_kl               | -0.0476  |
| entropy                 | 2.54     |
| episodes                | 242700   |
| lives                   | 242700   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 6.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1600720  |
| value_loss              | 3.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0232  |
| entropy                 | 2.52     |
| episodes                | 242800   |
| lives                   | 242800   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 1601433  |
| value_loss              | 3.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0281  |
| entropy                 | 2.45     |
| episodes                | 242900   |
| lives                   | 242900   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1602111  |
| value_loss              | 3.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0202  |
| entropy                 | 2.47     |
| episodes                | 243000   |
| lives                   | 243000   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 1602812  |
| value_loss              | 3.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0428  |
| entropy                 | 2.45     |
| episodes                | 243100   |
| lives                   | 243100   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1603495  |
| value_loss              | 4.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.041   |
| entropy                 | 2.47     |
| episodes                | 243200   |
| lives                   | 243200   |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 1604149  |
| value_loss              | 3.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0263  |
| entropy                 | 2.4      |
| episodes                | 243300   |
| lives                   | 243300   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 1604832  |
| value_loss              | 4.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.35     |
| episodes                | 243400   |
| lives                   | 243400   |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 1605502  |
| value_loss              | 3.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0136  |
| entropy                 | 2.32     |
| episodes                | 243500   |
| lives                   | 243500   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 1606207  |
| value_loss              | 3.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0122  |
| entropy                 | 2.32     |
| episodes                | 243600   |
| lives                   | 243600   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1606931  |
| value_loss              | 4.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0227  |
| entropy                 | 2.25     |
| episodes                | 243700   |
| lives                   | 243700   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1607562  |
| value_loss              | 3.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0676  |
| entropy                 | 2.29     |
| episodes                | 243800   |
| lives                   | 243800   |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0249  |
| steps                   | 1608169  |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.1     |
| entropy                 | 2.12     |
| episodes                | 243900   |
| lives                   | 243900   |
| mean 100 episode length | 5.47     |
| mean 100 episode reward | 3.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0314  |
| steps                   | 1608616  |
| value_loss              | 3.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.16    |
| entropy                 | 2.39     |
| episodes                | 244000   |
| lives                   | 244000   |
| mean 100 episode length | 6.33     |
| mean 100 episode reward | 4.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0268  |
| steps                   | 1609149  |
| value_loss              | 3.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0815  |
| entropy                 | 2.4      |
| episodes                | 244100   |
| lives                   | 244100   |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0233  |
| steps                   | 1609740  |
| value_loss              | 4.54     |
--------------------------------------
Saving model due to running mean reward increase: 4.7341 -> 4.7455
--------------------------------------
| approx_kl               | -0.143   |
| entropy                 | 2.49     |
| episodes                | 244200   |
| lives                   | 244200   |
| mean 100 episode length | 6.22     |
| mean 100 episode reward | 3.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.022   |
| steps                   | 1610262  |
| value_loss              | 3.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.104   |
| entropy                 | 2.49     |
| episodes                | 244300   |
| lives                   | 244300   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0227  |
| steps                   | 1610874  |
| value_loss              | 4.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0283  |
| entropy                 | 2.47     |
| episodes                | 244400   |
| lives                   | 244400   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0149  |
| steps                   | 1611563  |
| value_loss              | 4.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0405  |
| entropy                 | 2.52     |
| episodes                | 244500   |
| lives                   | 244500   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0161  |
| steps                   | 1612281  |
| value_loss              | 3.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0422  |
| entropy                 | 2.44     |
| episodes                | 244600   |
| lives                   | 244600   |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0231  |
| steps                   | 1612912  |
| value_loss              | 4.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.106   |
| entropy                 | 2.43     |
| episodes                | 244700   |
| lives                   | 244700   |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 4.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0191  |
| steps                   | 1613451  |
| value_loss              | 3.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.138   |
| entropy                 | 2.45     |
| episodes                | 244800   |
| lives                   | 244800   |
| mean 100 episode length | 6.33     |
| mean 100 episode reward | 4.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0267  |
| steps                   | 1613984  |
| value_loss              | 3.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.128   |
| entropy                 | 2.52     |
| episodes                | 244900   |
| lives                   | 244900   |
| mean 100 episode length | 6.25     |
| mean 100 episode reward | 3.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0259  |
| steps                   | 1614509  |
| value_loss              | 3.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.154   |
| entropy                 | 2.56     |
| episodes                | 245000   |
| lives                   | 245000   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.023   |
| steps                   | 1615080  |
| value_loss              | 3.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.15    |
| entropy                 | 2.47     |
| episodes                | 245100   |
| lives                   | 245100   |
| mean 100 episode length | 5.78     |
| mean 100 episode reward | 3.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0201  |
| steps                   | 1615558  |
| value_loss              | 3.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.137   |
| entropy                 | 2.54     |
| episodes                | 245200   |
| lives                   | 245200   |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 1616150  |
| value_loss              | 4.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0097  |
| entropy                 | 2.59     |
| episodes                | 245300   |
| lives                   | 245300   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1616914  |
| value_loss              | 4.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0158  |
| entropy                 | 2.5      |
| episodes                | 245400   |
| lives                   | 245400   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1617665  |
| value_loss              | 3.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0172  |
| entropy                 | 2.49     |
| episodes                | 245500   |
| lives                   | 245500   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.012   |
| steps                   | 1618398  |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.48     |
| episodes                | 245600   |
| lives                   | 245600   |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1619079  |
| value_loss              | 3.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.48     |
| episodes                | 245700   |
| lives                   | 245700   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1619822  |
| value_loss              | 4.05     |
--------------------------------------
Saving model due to running mean reward increase: 5.7936 -> 6.5733
--------------------------------------
| approx_kl               | -0.0182  |
| entropy                 | 2.49     |
| episodes                | 245800   |
| lives                   | 245800   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0135  |
| steps                   | 1620530  |
| value_loss              | 3.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0252  |
| entropy                 | 2.47     |
| episodes                | 245900   |
| lives                   | 245900   |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1621179  |
| value_loss              | 3.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0377  |
| entropy                 | 2.55     |
| episodes                | 246000   |
| lives                   | 246000   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 1621847  |
| value_loss              | 3.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0415  |
| entropy                 | 2.5      |
| episodes                | 246100   |
| lives                   | 246100   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 1622577  |
| value_loss              | 3.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0239  |
| entropy                 | 2.51     |
| episodes                | 246200   |
| lives                   | 246200   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1623253  |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0232  |
| entropy                 | 2.49     |
| episodes                | 246300   |
| lives                   | 246300   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 1623945  |
| value_loss              | 3.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0278  |
| entropy                 | 2.51     |
| episodes                | 246400   |
| lives                   | 246400   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 1624631  |
| value_loss              | 3.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0434  |
| entropy                 | 2.51     |
| episodes                | 246500   |
| lives                   | 246500   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1625360  |
| value_loss              | 3.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0418  |
| entropy                 | 2.52     |
| episodes                | 246600   |
| lives                   | 246600   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1626078  |
| value_loss              | 3.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0354  |
| entropy                 | 2.53     |
| episodes                | 246700   |
| lives                   | 246700   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1626781  |
| value_loss              | 3.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.061   |
| entropy                 | 2.51     |
| episodes                | 246800   |
| lives                   | 246800   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1627467  |
| value_loss              | 3.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0515  |
| entropy                 | 2.54     |
| episodes                | 246900   |
| lives                   | 246900   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 1628153  |
| value_loss              | 3.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0375  |
| entropy                 | 2.49     |
| episodes                | 247000   |
| lives                   | 247000   |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1628801  |
| value_loss              | 3.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0357  |
| entropy                 | 2.54     |
| episodes                | 247100   |
| lives                   | 247100   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1629494  |
| value_loss              | 3.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0237  |
| entropy                 | 2.46     |
| episodes                | 247200   |
| lives                   | 247200   |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1630157  |
| value_loss              | 3.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0262  |
| entropy                 | 2.45     |
| episodes                | 247300   |
| lives                   | 247300   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0132  |
| steps                   | 1630842  |
| value_loss              | 3.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0282  |
| entropy                 | 2.49     |
| episodes                | 247400   |
| lives                   | 247400   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1631520  |
| value_loss              | 3.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0174  |
| entropy                 | 2.47     |
| episodes                | 247500   |
| lives                   | 247500   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 6.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1632254  |
| value_loss              | 3.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0176  |
| entropy                 | 2.49     |
| episodes                | 247600   |
| lives                   | 247600   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1632997  |
| value_loss              | 3.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0304  |
| entropy                 | 2.47     |
| episodes                | 247700   |
| lives                   | 247700   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1633730  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.022   |
| entropy                 | 2.46     |
| episodes                | 247800   |
| lives                   | 247800   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1634468  |
| value_loss              | 4.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.024   |
| entropy                 | 2.49     |
| episodes                | 247900   |
| lives                   | 247900   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 6.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 1635237  |
| value_loss              | 4.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0313  |
| entropy                 | 2.53     |
| episodes                | 248000   |
| lives                   | 248000   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1635961  |
| value_loss              | 4.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0258  |
| entropy                 | 2.54     |
| episodes                | 248100   |
| lives                   | 248100   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 7.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 1636753  |
| value_loss              | 4.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0199  |
| entropy                 | 2.56     |
| episodes                | 248200   |
| lives                   | 248200   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 1637530  |
| value_loss              | 3.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0375  |
| entropy                 | 2.46     |
| episodes                | 248300   |
| lives                   | 248300   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 1638243  |
| value_loss              | 3.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0276  |
| entropy                 | 2.49     |
| episodes                | 248400   |
| lives                   | 248400   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 6.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1638976  |
| value_loss              | 3.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0256  |
| entropy                 | 2.56     |
| episodes                | 248500   |
| lives                   | 248500   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 1639694  |
| value_loss              | 3.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0185  |
| entropy                 | 2.56     |
| episodes                | 248600   |
| lives                   | 248600   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1640446  |
| value_loss              | 3.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0234  |
| entropy                 | 2.53     |
| episodes                | 248700   |
| lives                   | 248700   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 1641178  |
| value_loss              | 3.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.035   |
| entropy                 | 2.53     |
| episodes                | 248800   |
| lives                   | 248800   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1641934  |
| value_loss              | 3.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0273  |
| entropy                 | 2.56     |
| episodes                | 248900   |
| lives                   | 248900   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1642694  |
| value_loss              | 3.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0222  |
| entropy                 | 2.5      |
| episodes                | 249000   |
| lives                   | 249000   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1643435  |
| value_loss              | 3.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0357  |
| entropy                 | 2.39     |
| episodes                | 249100   |
| lives                   | 249100   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1644129  |
| value_loss              | 3.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0223  |
| entropy                 | 2.48     |
| episodes                | 249200   |
| lives                   | 249200   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 1644837  |
| value_loss              | 4.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0862  |
| entropy                 | 2.51     |
| episodes                | 249300   |
| lives                   | 249300   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1645489  |
| value_loss              | 3.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0391  |
| entropy                 | 2.56     |
| episodes                | 249400   |
| lives                   | 249400   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 1646254  |
| value_loss              | 4.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0269  |
| entropy                 | 2.48     |
| episodes                | 249500   |
| lives                   | 249500   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 7.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0136  |
| steps                   | 1647019  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0164  |
| entropy                 | 2.45     |
| episodes                | 249600   |
| lives                   | 249600   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0124  |
| steps                   | 1647755  |
| value_loss              | 4.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.032   |
| entropy                 | 2.37     |
| episodes                | 249700   |
| lives                   | 249700   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0144  |
| steps                   | 1648416  |
| value_loss              | 3.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0714  |
| entropy                 | 2.41     |
| episodes                | 249800   |
| lives                   | 249800   |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 1649043  |
| value_loss              | 4.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0665  |
| entropy                 | 2.45     |
| episodes                | 249900   |
| lives                   | 249900   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0126  |
| steps                   | 1649702  |
| value_loss              | 3.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.102   |
| entropy                 | 2.48     |
| episodes                | 250000   |
| lives                   | 250000   |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1650308  |
| value_loss              | 3.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0417  |
| entropy                 | 2.55     |
| episodes                | 250100   |
| lives                   | 250100   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 1651004  |
| value_loss              | 4.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.132   |
| entropy                 | 2.38     |
| episodes                | 250200   |
| lives                   | 250200   |
| mean 100 episode length | 6.09     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0282  |
| steps                   | 1651513  |
| value_loss              | 3.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.185   |
| entropy                 | 2.44     |
| episodes                | 250300   |
| lives                   | 250300   |
| mean 100 episode length | 6.05     |
| mean 100 episode reward | 4.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.023   |
| steps                   | 1652018  |
| value_loss              | 3.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.217   |
| entropy                 | 2.47     |
| episodes                | 250400   |
| lives                   | 250400   |
| mean 100 episode length | 6.31     |
| mean 100 episode reward | 3.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0254  |
| steps                   | 1652549  |
| value_loss              | 3.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.168   |
| entropy                 | 2.52     |
| episodes                | 250500   |
| lives                   | 250500   |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 4.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0172  |
| steps                   | 1653103  |
| value_loss              | 4.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.182   |
| entropy                 | 2.54     |
| episodes                | 250600   |
| lives                   | 250600   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0233  |
| steps                   | 1653688  |
| value_loss              | 4.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0702  |
| entropy                 | 2.52     |
| episodes                | 250700   |
| lives                   | 250700   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 1654353  |
| value_loss              | 4.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0244  |
| entropy                 | 2.44     |
| episodes                | 250800   |
| lives                   | 250800   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1655026  |
| value_loss              | 4.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0894  |
| entropy                 | 2.37     |
| episodes                | 250900   |
| lives                   | 250900   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0271  |
| steps                   | 1655638  |
| value_loss              | 3.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.101   |
| entropy                 | 2.34     |
| episodes                | 251000   |
| lives                   | 251000   |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0266  |
| steps                   | 1656165  |
| value_loss              | 4.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.201   |
| entropy                 | 2.4      |
| episodes                | 251100   |
| lives                   | 251100   |
| mean 100 episode length | 6.31     |
| mean 100 episode reward | 4.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0299  |
| steps                   | 1656696  |
| value_loss              | 4.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.181   |
| entropy                 | 2.55     |
| episodes                | 251200   |
| lives                   | 251200   |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 4.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0197  |
| steps                   | 1657259  |
| value_loss              | 4.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.177   |
| entropy                 | 2.48     |
| episodes                | 251300   |
| lives                   | 251300   |
| mean 100 episode length | 6.7      |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0233  |
| steps                   | 1657829  |
| value_loss              | 4.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.147   |
| entropy                 | 2.57     |
| episodes                | 251400   |
| lives                   | 251400   |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0147  |
| steps                   | 1658451  |
| value_loss              | 4.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.212   |
| entropy                 | 2.47     |
| episodes                | 251500   |
| lives                   | 251500   |
| mean 100 episode length | 6.17     |
| mean 100 episode reward | 4        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.026   |
| steps                   | 1658968  |
| value_loss              | 3.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.204   |
| entropy                 | 2.46     |
| episodes                | 251600   |
| lives                   | 251600   |
| mean 100 episode length | 5.87     |
| mean 100 episode reward | 3.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 1659455  |
| value_loss              | 4.3      |
--------------------------------------
Saving model due to running mean reward increase: 3.651 -> 4.1439
--------------------------------------
| approx_kl               | -0.223   |
| entropy                 | 2.51     |
| episodes                | 251700   |
| lives                   | 251700   |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 4.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 1660014  |
| value_loss              | 3.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.159   |
| entropy                 | 2.54     |
| episodes                | 251800   |
| lives                   | 251800   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.015   |
| steps                   | 1660609  |
| value_loss              | 4.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.143   |
| entropy                 | 2.55     |
| episodes                | 251900   |
| lives                   | 251900   |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 4.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0163  |
| steps                   | 1661204  |
| value_loss              | 4.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.179   |
| entropy                 | 2.49     |
| episodes                | 252000   |
| lives                   | 252000   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 1661785  |
| value_loss              | 4.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.179   |
| entropy                 | 2.41     |
| episodes                | 252100   |
| lives                   | 252100   |
| mean 100 episode length | 6.1      |
| mean 100 episode reward | 3.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0221  |
| steps                   | 1662295  |
| value_loss              | 4.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.174   |
| entropy                 | 2.43     |
| episodes                | 252200   |
| lives                   | 252200   |
| mean 100 episode length | 6.24     |
| mean 100 episode reward | 4.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 1662819  |
| value_loss              | 4.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.193   |
| entropy                 | 2.42     |
| episodes                | 252300   |
| lives                   | 252300   |
| mean 100 episode length | 6.01     |
| mean 100 episode reward | 4.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.02    |
| steps                   | 1663320  |
| value_loss              | 4.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.154   |
| entropy                 | 2.5      |
| episodes                | 252400   |
| lives                   | 252400   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 4.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0206  |
| steps                   | 1663891  |
| value_loss              | 4.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.114   |
| entropy                 | 2.45     |
| episodes                | 252500   |
| lives                   | 252500   |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 4.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0176  |
| steps                   | 1664450  |
| value_loss              | 3.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.162   |
| entropy                 | 2.45     |
| episodes                | 252600   |
| lives                   | 252600   |
| mean 100 episode length | 6.31     |
| mean 100 episode reward | 4.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0223  |
| steps                   | 1664981  |
| value_loss              | 4.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.142   |
| entropy                 | 2.38     |
| episodes                | 252700   |
| lives                   | 252700   |
| mean 100 episode length | 5.79     |
| mean 100 episode reward | 3.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0206  |
| steps                   | 1665460  |
| value_loss              | 4.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.199   |
| entropy                 | 2.44     |
| episodes                | 252800   |
| lives                   | 252800   |
| mean 100 episode length | 5.73     |
| mean 100 episode reward | 3.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0263  |
| steps                   | 1665933  |
| value_loss              | 4.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.218   |
| entropy                 | 2.37     |
| episodes                | 252900   |
| lives                   | 252900   |
| mean 100 episode length | 5.75     |
| mean 100 episode reward | 3.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0225  |
| steps                   | 1666408  |
| value_loss              | 4.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.172   |
| entropy                 | 2.46     |
| episodes                | 253000   |
| lives                   | 253000   |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 4        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0182  |
| steps                   | 1666929  |
| value_loss              | 4.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.157   |
| entropy                 | 2.47     |
| episodes                | 253100   |
| lives                   | 253100   |
| mean 100 episode length | 6.07     |
| mean 100 episode reward | 3.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1667436  |
| value_loss              | 4.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.23    |
| entropy                 | 2.52     |
| episodes                | 253200   |
| lives                   | 253200   |
| mean 100 episode length | 6.45     |
| mean 100 episode reward | 3.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1667981  |
| value_loss              | 4.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.154   |
| entropy                 | 2.52     |
| episodes                | 253300   |
| lives                   | 253300   |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0171  |
| steps                   | 1668566  |
| value_loss              | 5.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.179   |
| entropy                 | 2.44     |
| episodes                | 253400   |
| lives                   | 253400   |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 1669124  |
| value_loss              | 4.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.176   |
| entropy                 | 2.41     |
| episodes                | 253500   |
| lives                   | 253500   |
| mean 100 episode length | 6.23     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0221  |
| steps                   | 1669647  |
| value_loss              | 4.59     |
--------------------------------------
Saving model due to running mean reward increase: 4.1304 -> 4.3645
--------------------------------------
| approx_kl               | -0.174   |
| entropy                 | 2.51     |
| episodes                | 253600   |
| lives                   | 253600   |
| mean 100 episode length | 6.17     |
| mean 100 episode reward | 3.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0182  |
| steps                   | 1670164  |
| value_loss              | 4.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.171   |
| entropy                 | 2.48     |
| episodes                | 253700   |
| lives                   | 253700   |
| mean 100 episode length | 6.37     |
| mean 100 episode reward | 3.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0188  |
| steps                   | 1670701  |
| value_loss              | 4.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.171   |
| entropy                 | 2.56     |
| episodes                | 253800   |
| lives                   | 253800   |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 4.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 1671278  |
| value_loss              | 4.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.206   |
| entropy                 | 2.54     |
| episodes                | 253900   |
| lives                   | 253900   |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 4.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0185  |
| steps                   | 1671843  |
| value_loss              | 4.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.204   |
| entropy                 | 2.59     |
| episodes                | 254000   |
| lives                   | 254000   |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 3.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0151  |
| steps                   | 1672433  |
| value_loss              | 3.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.168   |
| entropy                 | 2.57     |
| episodes                | 254100   |
| lives                   | 254100   |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0193  |
| steps                   | 1673022  |
| value_loss              | 4.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.165   |
| entropy                 | 2.59     |
| episodes                | 254200   |
| lives                   | 254200   |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 4.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 1673616  |
| value_loss              | 4.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.181   |
| entropy                 | 2.54     |
| episodes                | 254300   |
| lives                   | 254300   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 4.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 1674183  |
| value_loss              | 4        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.194   |
| entropy                 | 2.45     |
| episodes                | 254400   |
| lives                   | 254400   |
| mean 100 episode length | 6.4      |
| mean 100 episode reward | 4.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0193  |
| steps                   | 1674723  |
| value_loss              | 4.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.185   |
| entropy                 | 2.44     |
| episodes                | 254500   |
| lives                   | 254500   |
| mean 100 episode length | 6.13     |
| mean 100 episode reward | 3.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0254  |
| steps                   | 1675236  |
| value_loss              | 3.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.157   |
| entropy                 | 2.47     |
| episodes                | 254600   |
| lives                   | 254600   |
| mean 100 episode length | 6.15     |
| mean 100 episode reward | 4.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0254  |
| steps                   | 1675751  |
| value_loss              | 4.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.177   |
| entropy                 | 2.47     |
| episodes                | 254700   |
| lives                   | 254700   |
| mean 100 episode length | 6.14     |
| mean 100 episode reward | 3.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0177  |
| steps                   | 1676265  |
| value_loss              | 4.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.185   |
| entropy                 | 2.51     |
| episodes                | 254800   |
| lives                   | 254800   |
| mean 100 episode length | 6.24     |
| mean 100 episode reward | 3.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0214  |
| steps                   | 1676789  |
| value_loss              | 4.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0951  |
| entropy                 | 2.61     |
| episodes                | 254900   |
| lives                   | 254900   |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 4.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 1677409  |
| value_loss              | 4.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.149   |
| entropy                 | 2.51     |
| episodes                | 255000   |
| lives                   | 255000   |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 4.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.018   |
| steps                   | 1677970  |
| value_loss              | 4.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.151   |
| entropy                 | 2.57     |
| episodes                | 255100   |
| lives                   | 255100   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 1678608  |
| value_loss              | 4.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.115   |
| entropy                 | 2.55     |
| episodes                | 255200   |
| lives                   | 255200   |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1679175  |
| value_loss              | 4.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.131   |
| entropy                 | 2.57     |
| episodes                | 255300   |
| lives                   | 255300   |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 4.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0221  |
| steps                   | 1679773  |
| value_loss              | 4.19     |
--------------------------------------
Saving model due to running mean reward increase: 3.8192 -> 3.9321
--------------------------------------
| approx_kl               | -0.164   |
| entropy                 | 2.46     |
| episodes                | 255400   |
| lives                   | 255400   |
| mean 100 episode length | 6        |
| mean 100 episode reward | 3.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0168  |
| steps                   | 1680273  |
| value_loss              | 3.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.177   |
| entropy                 | 2.44     |
| episodes                | 255500   |
| lives                   | 255500   |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0074   |
| steps                   | 1680846  |
| value_loss              | 4.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.145   |
| entropy                 | 2.44     |
| episodes                | 255600   |
| lives                   | 255600   |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 4.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0178  |
| steps                   | 1681426  |
| value_loss              | 4.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.156   |
| entropy                 | 2.43     |
| episodes                | 255700   |
| lives                   | 255700   |
| mean 100 episode length | 6.33     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0194  |
| steps                   | 1681959  |
| value_loss              | 4.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.181   |
| entropy                 | 2.32     |
| episodes                | 255800   |
| lives                   | 255800   |
| mean 100 episode length | 5.35     |
| mean 100 episode reward | 3.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 1682394  |
| value_loss              | 4.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.178   |
| entropy                 | 2.45     |
| episodes                | 255900   |
| lives                   | 255900   |
| mean 100 episode length | 6.2      |
| mean 100 episode reward | 3.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0248  |
| steps                   | 1682914  |
| value_loss              | 4.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.188   |
| entropy                 | 2.5      |
| episodes                | 256000   |
| lives                   | 256000   |
| mean 100 episode length | 6.32     |
| mean 100 episode reward | 4.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0231  |
| steps                   | 1683446  |
| value_loss              | 4.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.17    |
| entropy                 | 2.49     |
| episodes                | 256100   |
| lives                   | 256100   |
| mean 100 episode length | 6.08     |
| mean 100 episode reward | 3.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0203  |
| steps                   | 1683954  |
| value_loss              | 4.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.175   |
| entropy                 | 2.51     |
| episodes                | 256200   |
| lives                   | 256200   |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 4.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0159  |
| steps                   | 1684525  |
| value_loss              | 3.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.146   |
| entropy                 | 2.52     |
| episodes                | 256300   |
| lives                   | 256300   |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 4.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0167  |
| steps                   | 1685140  |
| value_loss              | 4        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.148   |
| entropy                 | 2.52     |
| episodes                | 256400   |
| lives                   | 256400   |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 4.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0164  |
| steps                   | 1685752  |
| value_loss              | 4.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.128   |
| entropy                 | 2.48     |
| episodes                | 256500   |
| lives                   | 256500   |
| mean 100 episode length | 6.49     |
| mean 100 episode reward | 4.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0154  |
| steps                   | 1686301  |
| value_loss              | 4.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.124   |
| entropy                 | 2.43     |
| episodes                | 256600   |
| lives                   | 256600   |
| mean 100 episode length | 6.46     |
| mean 100 episode reward | 4.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0169  |
| steps                   | 1686847  |
| value_loss              | 4.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.16    |
| entropy                 | 2.45     |
| episodes                | 256700   |
| lives                   | 256700   |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 4.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0184  |
| steps                   | 1687407  |
| value_loss              | 4.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.165   |
| entropy                 | 2.48     |
| episodes                | 256800   |
| lives                   | 256800   |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0148  |
| steps                   | 1687998  |
| value_loss              | 4.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.13    |
| entropy                 | 2.42     |
| episodes                | 256900   |
| lives                   | 256900   |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 4.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0163  |
| steps                   | 1688580  |
| value_loss              | 3.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.139   |
| entropy                 | 2.36     |
| episodes                | 257000   |
| lives                   | 257000   |
| mean 100 episode length | 6.11     |
| mean 100 episode reward | 3.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.021   |
| steps                   | 1689091  |
| value_loss              | 4.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.184   |
| entropy                 | 2.37     |
| episodes                | 257100   |
| lives                   | 257100   |
| mean 100 episode length | 6.21     |
| mean 100 episode reward | 3.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0203  |
| steps                   | 1689612  |
| value_loss              | 4.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.166   |
| entropy                 | 2.47     |
| episodes                | 257200   |
| lives                   | 257200   |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 4.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0175  |
| steps                   | 1690170  |
| value_loss              | 4.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.144   |
| entropy                 | 2.43     |
| episodes                | 257300   |
| lives                   | 257300   |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 4.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0224  |
| steps                   | 1690704  |
| value_loss              | 4.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.138   |
| entropy                 | 2.4      |
| episodes                | 257400   |
| lives                   | 257400   |
| mean 100 episode length | 6.02     |
| mean 100 episode reward | 4.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0239  |
| steps                   | 1691206  |
| value_loss              | 4.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.176   |
| entropy                 | 2.45     |
| episodes                | 257500   |
| lives                   | 257500   |
| mean 100 episode length | 6.13     |
| mean 100 episode reward | 3.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0158  |
| steps                   | 1691719  |
| value_loss              | 4.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.159   |
| entropy                 | 2.41     |
| episodes                | 257600   |
| lives                   | 257600   |
| mean 100 episode length | 5.93     |
| mean 100 episode reward | 3.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.014   |
| steps                   | 1692212  |
| value_loss              | 4.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.193   |
| entropy                 | 2.42     |
| episodes                | 257700   |
| lives                   | 257700   |
| mean 100 episode length | 6.35     |
| mean 100 episode reward | 4.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.016   |
| steps                   | 1692747  |
| value_loss              | 4.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.171   |
| entropy                 | 2.55     |
| episodes                | 257800   |
| lives                   | 257800   |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 4.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0125  |
| steps                   | 1693390  |
| value_loss              | 4.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.133   |
| entropy                 | 2.58     |
| episodes                | 257900   |
| lives                   | 257900   |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 1694035  |
| value_loss              | 4.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.152   |
| entropy                 | 2.54     |
| episodes                | 258000   |
| lives                   | 258000   |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0181  |
| steps                   | 1694651  |
| value_loss              | 4.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.162   |
| entropy                 | 2.53     |
| episodes                | 258100   |
| lives                   | 258100   |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0134  |
| steps                   | 1695281  |
| value_loss              | 4.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.165   |
| entropy                 | 2.57     |
| episodes                | 258200   |
| lives                   | 258200   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0142  |
| steps                   | 1695933  |
| value_loss              | 3.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.139   |
| entropy                 | 2.53     |
| episodes                | 258300   |
| lives                   | 258300   |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0153  |
| steps                   | 1696586  |
| value_loss              | 4.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.154   |
| entropy                 | 2.48     |
| episodes                | 258400   |
| lives                   | 258400   |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 5.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0131  |
| steps                   | 1697216  |
| value_loss              | 4.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.142   |
| entropy                 | 2.48     |
| episodes                | 258500   |
| lives                   | 258500   |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 4.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 1697802  |
| value_loss              | 3.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.117   |
| entropy                 | 2.52     |
| episodes                | 258600   |
| lives                   | 258600   |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0155  |
| steps                   | 1698440  |
| value_loss              | 4.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.127   |
| entropy                 | 2.51     |
| episodes                | 258700   |
| lives                   | 258700   |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0213  |
| steps                   | 1699048  |
| value_loss              | 4.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.126   |
| entropy                 | 2.48     |
| episodes                | 258800   |
| lives                   | 258800   |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 4.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.017   |
| steps                   | 1699629  |
| value_loss              | 3.73     |
--------------------------------------
Restored model with mean reward: 7.1166
