Logging to /Users/janrichtr/Desktop/RL-HPO/batch_size-128/buffer_size-2048/cell-32/checkpoint_freq-10000/clip_ratio-0.3/entropy_coef-0.1/env_id-nnMeta-v40/gamma-0.99/lam-0.95/log_interval-1/lr-0.0001/nhidden-128/num_timesteps-3000000/seed-0/train_epochs-10/train_iters-4/
--------------------------------------
| approx_kl               | 0        |
| entropy                 | 0        |
| episodes                | 100      |
| lives                   | 100      |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 0.0704   |
| most_used_dataset       | 0        |
| number_of_used          | 20       |
| policy_loss             | 0        |
| steps                   | 872      |
| value_loss              | 0        |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 3.73     |
| episodes                | 200      |
| lives                   | 200      |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | -0.0973  |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0116  |
| steps                   | 1701     |
| value_loss              | 4.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 3.73     |
| episodes                | 300      |
| lives                   | 300      |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 0.019    |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2631     |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 3.73     |
| episodes                | 400      |
| lives                   | 400      |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 0.154    |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 3459     |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 3.72     |
| episodes                | 500      |
| lives                   | 500      |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 0.451    |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 4438     |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 3.67     |
| episodes                | 600      |
| lives                   | 600      |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 1.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 5293     |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 3.58     |
| episodes                | 700      |
| lives                   | 700      |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 2.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 6088     |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 3.38     |
| episodes                | 800      |
| lives                   | 800      |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 2.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 6810     |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0077  |
| entropy                 | 3.32     |
| episodes                | 900      |
| lives                   | 900      |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 2.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 7476     |
| value_loss              | 2.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 3.35     |
| episodes                | 1000     |
| lives                   | 1000     |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 3.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 8139     |
| value_loss              | 2.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.33     |
| episodes                | 1100     |
| lives                   | 1100     |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 2.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 8766     |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 3.32     |
| episodes                | 1200     |
| lives                   | 1200     |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 2.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 9396     |
| value_loss              | 2.03     |
--------------------------------------
Saving model due to mean reward increase: None -> 3.4849
Saving model due to running mean reward increase: 2.9916 -> 3.4849
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 3.21     |
| episodes                | 1300     |
| lives                   | 1300     |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 3.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 10014    |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 3.25     |
| episodes                | 1400     |
| lives                   | 1400     |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 3.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 10663    |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 3.28     |
| episodes                | 1500     |
| lives                   | 1500     |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 3.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 11354    |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 3.18     |
| episodes                | 1600     |
| lives                   | 1600     |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 3.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 11958    |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0023   |
| entropy                 | 3.24     |
| episodes                | 1700     |
| lives                   | 1700     |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 3.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 12622    |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.22     |
| episodes                | 1800     |
| lives                   | 1800     |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 3.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 13246    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0003   |
| entropy                 | 3.13     |
| episodes                | 1900     |
| lives                   | 1900     |
| mean 100 episode length | 6.39     |
| mean 100 episode reward | 3.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 13785    |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 3.11     |
| episodes                | 2000     |
| lives                   | 2000     |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 3.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 14363    |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 3.1      |
| episodes                | 2100     |
| lives                   | 2100     |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 14959    |
| value_loss              | 2.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 3.2      |
| episodes                | 2200     |
| lives                   | 2200     |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 3.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 15603    |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 3.22     |
| episodes                | 2300     |
| lives                   | 2300     |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 4.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 16254    |
| value_loss              | 2.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 3.16     |
| episodes                | 2400     |
| lives                   | 2400     |
| mean 100 episode length | 7        |
| mean 100 episode reward | 3.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 16854    |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3.09     |
| episodes                | 2500     |
| lives                   | 2500     |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 4.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 17461    |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 3.09     |
| episodes                | 2600     |
| lives                   | 2600     |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 3.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 18033    |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 3.08     |
| episodes                | 2700     |
| lives                   | 2700     |
| mean 100 episode length | 6.5      |
| mean 100 episode reward | 3.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 18583    |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 3.13     |
| episodes                | 2800     |
| lives                   | 2800     |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 4.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 19219    |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.19     |
| episodes                | 2900     |
| lives                   | 2900     |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 3.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 19848    |
| value_loss              | 1.8      |
--------------------------------------
Saving model due to mean reward increase: 3.4849 -> 3.9783
Saving model due to running mean reward increase: 3.8808 -> 3.9783
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3.23     |
| episodes                | 3000     |
| lives                   | 3000     |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 4.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 20473    |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 3.22     |
| episodes                | 3100     |
| lives                   | 3100     |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 3.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 21117    |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.22     |
| episodes                | 3200     |
| lives                   | 3200     |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 4.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 21815    |
| value_loss              | 2.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.19     |
| episodes                | 3300     |
| lives                   | 3300     |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 4.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 22445    |
| value_loss              | 2.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 3.18     |
| episodes                | 3400     |
| lives                   | 3400     |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 4.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 23079    |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 3.21     |
| episodes                | 3500     |
| lives                   | 3500     |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 23711    |
| value_loss              | 2.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.25     |
| episodes                | 3600     |
| lives                   | 3600     |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 4.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 24316    |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.22     |
| episodes                | 3700     |
| lives                   | 3700     |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 3.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 24931    |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.2      |
| episodes                | 3800     |
| lives                   | 3800     |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 4.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 25596    |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 3.18     |
| episodes                | 3900     |
| lives                   | 3900     |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 3.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 26239    |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.17     |
| episodes                | 4000     |
| lives                   | 4000     |
| mean 100 episode length | 7        |
| mean 100 episode reward | 3.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 26839    |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 3.14     |
| episodes                | 4100     |
| lives                   | 4100     |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 3.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 27418    |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.12     |
| episodes                | 4200     |
| lives                   | 4200     |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 4.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 28048    |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 3.09     |
| episodes                | 4300     |
| lives                   | 4300     |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 28641    |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.13     |
| episodes                | 4400     |
| lives                   | 4400     |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 4.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 29305    |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.16     |
| episodes                | 4500     |
| lives                   | 4500     |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 4.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 29970    |
| value_loss              | 2.15     |
--------------------------------------
Saving model due to mean reward increase: 3.9783 -> 4.3676
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3.15     |
| episodes                | 4600     |
| lives                   | 4600     |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 30578    |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 3.14     |
| episodes                | 4700     |
| lives                   | 4700     |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 31191    |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0008   |
| entropy                 | 3.18     |
| episodes                | 4800     |
| lives                   | 4800     |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 4.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 31840    |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 3.2      |
| episodes                | 4900     |
| lives                   | 4900     |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 3.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 32458    |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 3.17     |
| episodes                | 5000     |
| lives                   | 5000     |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 4.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 33084    |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.15     |
| episodes                | 5100     |
| lives                   | 5100     |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 33716    |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3.11     |
| episodes                | 5200     |
| lives                   | 5200     |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 34324    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 3.14     |
| episodes                | 5300     |
| lives                   | 5300     |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 34962    |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3.14     |
| episodes                | 5400     |
| lives                   | 5400     |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 35607    |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0002   |
| entropy                 | 3.14     |
| episodes                | 5500     |
| lives                   | 5500     |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 4.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 36186    |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 3.17     |
| episodes                | 5600     |
| lives                   | 5600     |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 36804    |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.16     |
| episodes                | 5700     |
| lives                   | 5700     |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 37400    |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.14     |
| episodes                | 5800     |
| lives                   | 5800     |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 4.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 38056    |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.15     |
| episodes                | 5900     |
| lives                   | 5900     |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 4.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 38712    |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 3.15     |
| episodes                | 6000     |
| lives                   | 6000     |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 4.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 39295    |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.14     |
| episodes                | 6100     |
| lives                   | 6100     |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 4.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 39938    |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 3.11     |
| episodes                | 6200     |
| lives                   | 6200     |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 40534    |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.12     |
| episodes                | 6300     |
| lives                   | 6300     |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 4.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 41155    |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 3.17     |
| episodes                | 6400     |
| lives                   | 6400     |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 4.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 41797    |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 3.14     |
| episodes                | 6500     |
| lives                   | 6500     |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 4.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 42445    |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 3.16     |
| episodes                | 6600     |
| lives                   | 6600     |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 4.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 43069    |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 3.15     |
| episodes                | 6700     |
| lives                   | 6700     |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 3.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 43681    |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 3.14     |
| episodes                | 6800     |
| lives                   | 6800     |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 3.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 44253    |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3.13     |
| episodes                | 6900     |
| lives                   | 6900     |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 4.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 44900    |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.14     |
| episodes                | 7000     |
| lives                   | 7000     |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 3.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 45502    |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.15     |
| episodes                | 7100     |
| lives                   | 7100     |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 3.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 46161    |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 3.14     |
| episodes                | 7200     |
| lives                   | 7200     |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 4.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 46787    |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 3.17     |
| episodes                | 7300     |
| lives                   | 7300     |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 4.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 47392    |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 3.18     |
| episodes                | 7400     |
| lives                   | 7400     |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 3.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 47981    |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 3.16     |
| episodes                | 7500     |
| lives                   | 7500     |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 4.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 48642    |
| value_loss              | 2.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.13     |
| episodes                | 7600     |
| lives                   | 7600     |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 4.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 49237    |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.16     |
| episodes                | 7700     |
| lives                   | 7700     |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 4.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 49896    |
| value_loss              | 2.01     |
--------------------------------------
Saving model due to mean reward increase: 4.3676 -> 4.5806
Saving model due to running mean reward increase: 4.2477 -> 4.5806
--------------------------------------
| approx_kl               | 0.0005   |
| entropy                 | 3.14     |
| episodes                | 7800     |
| lives                   | 7800     |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 50485    |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.11     |
| episodes                | 7900     |
| lives                   | 7900     |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 51123    |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.14     |
| episodes                | 8000     |
| lives                   | 8000     |
| mean 100 episode length | 7        |
| mean 100 episode reward | 3.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 51723    |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 3.11     |
| episodes                | 8100     |
| lives                   | 8100     |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 4.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 52364    |
| value_loss              | 2.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.11     |
| episodes                | 8200     |
| lives                   | 8200     |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 52953    |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.11     |
| episodes                | 8300     |
| lives                   | 8300     |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 4.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 53589    |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 3.13     |
| episodes                | 8400     |
| lives                   | 8400     |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 54236    |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.09     |
| episodes                | 8500     |
| lives                   | 8500     |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 4.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 54792    |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.09     |
| episodes                | 8600     |
| lives                   | 8600     |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 3.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 55379    |
| value_loss              | 2.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 3.1      |
| episodes                | 8700     |
| lives                   | 8700     |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 4.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 55989    |
| value_loss              | 2.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 3.08     |
| episodes                | 8800     |
| lives                   | 8800     |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 4.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 56630    |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 3.08     |
| episodes                | 8900     |
| lives                   | 8900     |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 4.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 57267    |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 3.07     |
| episodes                | 9000     |
| lives                   | 9000     |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 57852    |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 3.01     |
| episodes                | 9100     |
| lives                   | 9100     |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 58430    |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 3.01     |
| episodes                | 9200     |
| lives                   | 9200     |
| mean 100 episode length | 6.46     |
| mean 100 episode reward | 4.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 58976    |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 3.05     |
| episodes                | 9300     |
| lives                   | 9300     |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 4.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 59582    |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 3.03     |
| episodes                | 9400     |
| lives                   | 9400     |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 60170    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 3.04     |
| episodes                | 9500     |
| lives                   | 9500     |
| mean 100 episode length | 7        |
| mean 100 episode reward | 4.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 60770    |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 3.02     |
| episodes                | 9600     |
| lives                   | 9600     |
| mean 100 episode length | 6.54     |
| mean 100 episode reward | 4.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 61324    |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.97     |
| episodes                | 9700     |
| lives                   | 9700     |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 61902    |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 3        |
| episodes                | 9800     |
| lives                   | 9800     |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 3.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 62470    |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 3.07     |
| episodes                | 9900     |
| lives                   | 9900     |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 4.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 63042    |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 3.09     |
| episodes                | 10000    |
| lives                   | 10000    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 4.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 63653    |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 3.06     |
| episodes                | 10100    |
| lives                   | 10100    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 64219    |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 3.08     |
| episodes                | 10200    |
| lives                   | 10200    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 4.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 64820    |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0001   |
| entropy                 | 3.06     |
| episodes                | 10300    |
| lives                   | 10300    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 4.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 65428    |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.07     |
| episodes                | 10400    |
| lives                   | 10400    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 4.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 66059    |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.09     |
| episodes                | 10500    |
| lives                   | 10500    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 66699    |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 3.05     |
| episodes                | 10600    |
| lives                   | 10600    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 4.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 67320    |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 3.03     |
| episodes                | 10700    |
| lives                   | 10700    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 67916    |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 3.08     |
| episodes                | 10800    |
| lives                   | 10800    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 4.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 68503    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 3.09     |
| episodes                | 10900    |
| lives                   | 10900    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 69143    |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 3.09     |
| episodes                | 11000    |
| lives                   | 11000    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 69734    |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.13     |
| episodes                | 11100    |
| lives                   | 11100    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 70352    |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.15     |
| episodes                | 11200    |
| lives                   | 11200    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 4.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 70983    |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 3.1      |
| episodes                | 11300    |
| lives                   | 11300    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 4.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 71644    |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.09     |
| episodes                | 11400    |
| lives                   | 11400    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 72274    |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.08     |
| episodes                | 11500    |
| lives                   | 11500    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 72892    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 3.06     |
| episodes                | 11600    |
| lives                   | 11600    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 73524    |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 3.07     |
| episodes                | 11700    |
| lives                   | 11700    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 74162    |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.07     |
| episodes                | 11800    |
| lives                   | 11800    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 74817    |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 3.08     |
| episodes                | 11900    |
| lives                   | 11900    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 4.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 75449    |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.09     |
| episodes                | 12000    |
| lives                   | 12000    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 76060    |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 3.08     |
| episodes                | 12100    |
| lives                   | 12100    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 4.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 76658    |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 3.03     |
| episodes                | 12200    |
| lives                   | 12200    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 4.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 77277    |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 3.03     |
| episodes                | 12300    |
| lives                   | 12300    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 77905    |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 3.03     |
| episodes                | 12400    |
| lives                   | 12400    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 78526    |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 3.04     |
| episodes                | 12500    |
| lives                   | 12500    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 4.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 79121    |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 3.08     |
| episodes                | 12600    |
| lives                   | 12600    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 4.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 79728    |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 3.1      |
| episodes                | 12700    |
| lives                   | 12700    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 4.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 80355    |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3.05     |
| episodes                | 12800    |
| lives                   | 12800    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 80993    |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 3.02     |
| episodes                | 12900    |
| lives                   | 12900    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 81566    |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0058  |
| entropy                 | 3.05     |
| episodes                | 13000    |
| lives                   | 13000    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 82199    |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 3.01     |
| episodes                | 13100    |
| lives                   | 13100    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 82796    |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3.01     |
| episodes                | 13200    |
| lives                   | 13200    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 4.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 83421    |
| value_loss              | 2.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 3.04     |
| episodes                | 13300    |
| lives                   | 13300    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 84010    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 3.05     |
| episodes                | 13400    |
| lives                   | 13400    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 84614    |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 3.06     |
| episodes                | 13500    |
| lives                   | 13500    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 85259    |
| value_loss              | 2.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.01     |
| episodes                | 13600    |
| lives                   | 13600    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 85884    |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3.01     |
| episodes                | 13700    |
| lives                   | 13700    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 86493    |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 3.03     |
| episodes                | 13800    |
| lives                   | 13800    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 87121    |
| value_loss              | 2.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 3.01     |
| episodes                | 13900    |
| lives                   | 13900    |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 87715    |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.02     |
| episodes                | 14000    |
| lives                   | 14000    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 88356    |
| value_loss              | 2.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 3.01     |
| episodes                | 14100    |
| lives                   | 14100    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 88966    |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0067  |
| entropy                 | 2.97     |
| episodes                | 14200    |
| lives                   | 14200    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 89594    |
| value_loss              | 2.17     |
--------------------------------------
Saving model due to mean reward increase: 4.5806 -> 5.1005
Saving model due to running mean reward increase: 4.8897 -> 5.1005
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 2.98     |
| episodes                | 14300    |
| lives                   | 14300    |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 4.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 90156    |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 3.01     |
| episodes                | 14400    |
| lives                   | 14400    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 90803    |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 3.01     |
| episodes                | 14500    |
| lives                   | 14500    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 91408    |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 2.99     |
| episodes                | 14600    |
| lives                   | 14600    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 4.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 92005    |
| value_loss              | 2.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.98     |
| episodes                | 14700    |
| lives                   | 14700    |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 4.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 92579    |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.01     |
| episodes                | 14800    |
| lives                   | 14800    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 5.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 93216    |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3        |
| episodes                | 14900    |
| lives                   | 14900    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 93835    |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.96     |
| episodes                | 15000    |
| lives                   | 15000    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 94413    |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.99     |
| episodes                | 15100    |
| lives                   | 15100    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 95031    |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0011   |
| entropy                 | 3.01     |
| episodes                | 15200    |
| lives                   | 15200    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 95631    |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.02     |
| episodes                | 15300    |
| lives                   | 15300    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 4.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 96229    |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 2.99     |
| episodes                | 15400    |
| lives                   | 15400    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 4.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 96816    |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.98     |
| episodes                | 15500    |
| lives                   | 15500    |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 4.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 97396    |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.99     |
| episodes                | 15600    |
| lives                   | 15600    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 97981    |
| value_loss              | 2.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.95     |
| episodes                | 15700    |
| lives                   | 15700    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 98617    |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.98     |
| episodes                | 15800    |
| lives                   | 15800    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 99262    |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.98     |
| episodes                | 15900    |
| lives                   | 15900    |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 4.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 99834    |
| value_loss              | 2.08     |
--------------------------------------
Saving model due to running mean reward increase: 5.0479 -> 5.096
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.91     |
| episodes                | 16000    |
| lives                   | 16000    |
| mean 100 episode length | 6.7      |
| mean 100 episode reward | 4.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 100404   |
| value_loss              | 2.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.94     |
| episodes                | 16100    |
| lives                   | 16100    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 101023   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.98     |
| episodes                | 16200    |
| lives                   | 16200    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 101651   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.97     |
| episodes                | 16300    |
| lives                   | 16300    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 102289   |
| value_loss              | 2.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 3.01     |
| episodes                | 16400    |
| lives                   | 16400    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 4.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 102881   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3        |
| episodes                | 16500    |
| lives                   | 16500    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 103472   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3        |
| episodes                | 16600    |
| lives                   | 16600    |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 4.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 104041   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.97     |
| episodes                | 16700    |
| lives                   | 16700    |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 104612   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0129  |
| entropy                 | 2.99     |
| episodes                | 16800    |
| lives                   | 16800    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 105231   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.94     |
| episodes                | 16900    |
| lives                   | 16900    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 4.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 105818   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.99     |
| episodes                | 17000    |
| lives                   | 17000    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 4.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 106403   |
| value_loss              | 2.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 3.03     |
| episodes                | 17100    |
| lives                   | 17100    |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 4.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 106986   |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 3.04     |
| episodes                | 17200    |
| lives                   | 17200    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 4.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 107632   |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 3.01     |
| episodes                | 17300    |
| lives                   | 17300    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 108248   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.94     |
| episodes                | 17400    |
| lives                   | 17400    |
| mean 100 episode length | 6.52     |
| mean 100 episode reward | 4.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 108800   |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.93     |
| episodes                | 17500    |
| lives                   | 17500    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 109393   |
| value_loss              | 1.45     |
--------------------------------------
Saving model due to mean reward increase: 5.1005 -> 5.3382
Saving model due to running mean reward increase: 5.1165 -> 5.3382
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.96     |
| episodes                | 17600    |
| lives                   | 17600    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 110017   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.91     |
| episodes                | 17700    |
| lives                   | 17700    |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 4.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 110575   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.94     |
| episodes                | 17800    |
| lives                   | 17800    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 111199   |
| value_loss              | 2.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.91     |
| episodes                | 17900    |
| lives                   | 17900    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 111841   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.02     |
| episodes                | 18000    |
| lives                   | 18000    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 4.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 112437   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.97     |
| episodes                | 18100    |
| lives                   | 18100    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 113026   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.94     |
| episodes                | 18200    |
| lives                   | 18200    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 4.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 113615   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.96     |
| episodes                | 18300    |
| lives                   | 18300    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 114193   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.98     |
| episodes                | 18400    |
| lives                   | 18400    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 114805   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.98     |
| episodes                | 18500    |
| lives                   | 18500    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 115426   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.98     |
| episodes                | 18600    |
| lives                   | 18600    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 116017   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.96     |
| episodes                | 18700    |
| lives                   | 18700    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 5.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 116641   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.89     |
| episodes                | 18800    |
| lives                   | 18800    |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 117199   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.9      |
| episodes                | 18900    |
| lives                   | 18900    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 117772   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.86     |
| episodes                | 19000    |
| lives                   | 19000    |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 4.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 118328   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.89     |
| episodes                | 19100    |
| lives                   | 19100    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 118937   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0003   |
| entropy                 | 2.96     |
| episodes                | 19200    |
| lives                   | 19200    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 119524   |
| value_loss              | 2.21     |
--------------------------------------
Saving model due to mean reward increase: 5.3382 -> 5.5738
Saving model due to running mean reward increase: 5.2319 -> 5.5738
--------------------------------------
| approx_kl               | 0.0008   |
| entropy                 | 2.89     |
| episodes                | 19300    |
| lives                   | 19300    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 120128   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.9      |
| episodes                | 19400    |
| lives                   | 19400    |
| mean 100 episode length | 6.6      |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 120688   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.95     |
| episodes                | 19500    |
| lives                   | 19500    |
| mean 100 episode length | 6.5      |
| mean 100 episode reward | 4.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 121238   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.88     |
| episodes                | 19600    |
| lives                   | 19600    |
| mean 100 episode length | 6.71     |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 121809   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.89     |
| episodes                | 19700    |
| lives                   | 19700    |
| mean 100 episode length | 6.41     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 122350   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.95     |
| episodes                | 19800    |
| lives                   | 19800    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 122992   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.95     |
| episodes                | 19900    |
| lives                   | 19900    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 123588   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.95     |
| episodes                | 20000    |
| lives                   | 20000    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 124191   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.95     |
| episodes                | 20100    |
| lives                   | 20100    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 124838   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.97     |
| episodes                | 20200    |
| lives                   | 20200    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 125444   |
| value_loss              | 2.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.91     |
| episodes                | 20300    |
| lives                   | 20300    |
| mean 100 episode length | 6.47     |
| mean 100 episode reward | 4.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 125991   |
| value_loss              | 2.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.94     |
| episodes                | 20400    |
| lives                   | 20400    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 126564   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.98     |
| episodes                | 20500    |
| lives                   | 20500    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 4.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 127182   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.93     |
| episodes                | 20600    |
| lives                   | 20600    |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 127749   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.97     |
| episodes                | 20700    |
| lives                   | 20700    |
| mean 100 episode length | 6.27     |
| mean 100 episode reward | 4.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 128276   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 3.03     |
| episodes                | 20800    |
| lives                   | 20800    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 128933   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.97     |
| episodes                | 20900    |
| lives                   | 20900    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 129521   |
| value_loss              | 2.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.94     |
| episodes                | 21000    |
| lives                   | 21000    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 130122   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.99     |
| episodes                | 21100    |
| lives                   | 21100    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 4.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 130700   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.98     |
| episodes                | 21200    |
| lives                   | 21200    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 131293   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.95     |
| episodes                | 21300    |
| lives                   | 21300    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 5.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 131871   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.96     |
| episodes                | 21400    |
| lives                   | 21400    |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 132430   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.89     |
| episodes                | 21500    |
| lives                   | 21500    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 133035   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.91     |
| episodes                | 21600    |
| lives                   | 21600    |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 133629   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.92     |
| episodes                | 21700    |
| lives                   | 21700    |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 4.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 134198   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.92     |
| episodes                | 21800    |
| lives                   | 21800    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 134787   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.92     |
| episodes                | 21900    |
| lives                   | 21900    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 135353   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.9      |
| episodes                | 22000    |
| lives                   | 22000    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 135950   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.91     |
| episodes                | 22100    |
| lives                   | 22100    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 136531   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.92     |
| episodes                | 22200    |
| lives                   | 22200    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 137118   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.95     |
| episodes                | 22300    |
| lives                   | 22300    |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 4.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 137712   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.96     |
| episodes                | 22400    |
| lives                   | 22400    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 4.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 138285   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.92     |
| episodes                | 22500    |
| lives                   | 22500    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 138898   |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.94     |
| episodes                | 22600    |
| lives                   | 22600    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 139523   |
| value_loss              | 2.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.98     |
| episodes                | 22700    |
| lives                   | 22700    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 140137   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.99     |
| episodes                | 22800    |
| lives                   | 22800    |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 140717   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 3.02     |
| episodes                | 22900    |
| lives                   | 22900    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 141317   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 3.02     |
| episodes                | 23000    |
| lives                   | 23000    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 141929   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 3.01     |
| episodes                | 23100    |
| lives                   | 23100    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 142555   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.99     |
| episodes                | 23200    |
| lives                   | 23200    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 143157   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.99     |
| episodes                | 23300    |
| lives                   | 23300    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 143752   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 2.97     |
| episodes                | 23400    |
| lives                   | 23400    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 4.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 144347   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.99     |
| episodes                | 23500    |
| lives                   | 23500    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 144955   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.95     |
| episodes                | 23600    |
| lives                   | 23600    |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 5        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 145529   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.99     |
| episodes                | 23700    |
| lives                   | 23700    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 146131   |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 3        |
| episodes                | 23800    |
| lives                   | 23800    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 146728   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.96     |
| episodes                | 23900    |
| lives                   | 23900    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 147336   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.96     |
| episodes                | 24000    |
| lives                   | 24000    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 147978   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.96     |
| episodes                | 24100    |
| lives                   | 24100    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 148609   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.96     |
| episodes                | 24200    |
| lives                   | 24200    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 149220   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.95     |
| episodes                | 24300    |
| lives                   | 24300    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 149839   |
| value_loss              | 2.19     |
--------------------------------------
Saving model due to running mean reward increase: 5.2562 -> 5.3053
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.94     |
| episodes                | 24400    |
| lives                   | 24400    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 150429   |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.95     |
| episodes                | 24500    |
| lives                   | 24500    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 151039   |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.95     |
| episodes                | 24600    |
| lives                   | 24600    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 151655   |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.95     |
| episodes                | 24700    |
| lives                   | 24700    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 152272   |
| value_loss              | 2.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.95     |
| episodes                | 24800    |
| lives                   | 24800    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 152848   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 2.95     |
| episodes                | 24900    |
| lives                   | 24900    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 153452   |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.95     |
| episodes                | 25000    |
| lives                   | 25000    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 154086   |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.92     |
| episodes                | 25100    |
| lives                   | 25100    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 154667   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 2.94     |
| episodes                | 25200    |
| lives                   | 25200    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 155288   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.98     |
| episodes                | 25300    |
| lives                   | 25300    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 155899   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 3        |
| episodes                | 25400    |
| lives                   | 25400    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 156490   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.99     |
| episodes                | 25500    |
| lives                   | 25500    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 4.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 157114   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.97     |
| episodes                | 25600    |
| lives                   | 25600    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 157742   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.93     |
| episodes                | 25700    |
| lives                   | 25700    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 158345   |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0063  |
| entropy                 | 2.95     |
| episodes                | 25800    |
| lives                   | 25800    |
| mean 100 episode length | 6.83     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 158928   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.98     |
| episodes                | 25900    |
| lives                   | 25900    |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 5.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 159502   |
| value_loss              | 2.54     |
--------------------------------------
Saving model due to running mean reward increase: 5.0641 -> 5.4335
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.99     |
| episodes                | 26000    |
| lives                   | 26000    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 5.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 160104   |
| value_loss              | 2.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 3.02     |
| episodes                | 26100    |
| lives                   | 26100    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 5.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 160748   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 3.01     |
| episodes                | 26200    |
| lives                   | 26200    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 161336   |
| value_loss              | 2.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 3        |
| episodes                | 26300    |
| lives                   | 26300    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 161932   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.97     |
| episodes                | 26400    |
| lives                   | 26400    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 4.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 162523   |
| value_loss              | 2.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.99     |
| episodes                | 26500    |
| lives                   | 26500    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 163161   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 3        |
| episodes                | 26600    |
| lives                   | 26600    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 163777   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 3.02     |
| episodes                | 26700    |
| lives                   | 26700    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 164370   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 3.03     |
| episodes                | 26800    |
| lives                   | 26800    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 165022   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.94     |
| episodes                | 26900    |
| lives                   | 26900    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 165598   |
| value_loss              | 2.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.94     |
| episodes                | 27000    |
| lives                   | 27000    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 166213   |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.98     |
| episodes                | 27100    |
| lives                   | 27100    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 5.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 166822   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.91     |
| episodes                | 27200    |
| lives                   | 27200    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 167454   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.92     |
| episodes                | 27300    |
| lives                   | 27300    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 5.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 168027   |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.92     |
| episodes                | 27400    |
| lives                   | 27400    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 168619   |
| value_loss              | 2.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.95     |
| episodes                | 27500    |
| lives                   | 27500    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 169208   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.009   |
| entropy                 | 2.91     |
| episodes                | 27600    |
| lives                   | 27600    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0121  |
| steps                   | 169849   |
| value_loss              | 1.74     |
--------------------------------------
Saving model due to mean reward increase: 5.5738 -> 5.7527
Saving model due to running mean reward increase: 5.2066 -> 5.7527
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.89     |
| episodes                | 27700    |
| lives                   | 27700    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 170450   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.93     |
| episodes                | 27800    |
| lives                   | 27800    |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 4.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 171019   |
| value_loss              | 2.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.87     |
| episodes                | 27900    |
| lives                   | 27900    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 171637   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.9      |
| episodes                | 28000    |
| lives                   | 28000    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 172266   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.94     |
| episodes                | 28100    |
| lives                   | 28100    |
| mean 100 episode length | 6.19     |
| mean 100 episode reward | 4.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 172785   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.99     |
| episodes                | 28200    |
| lives                   | 28200    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 173397   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.99     |
| episodes                | 28300    |
| lives                   | 28300    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 174038   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.96     |
| episodes                | 28400    |
| lives                   | 28400    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 174636   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.97     |
| episodes                | 28500    |
| lives                   | 28500    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 5.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 175264   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.95     |
| episodes                | 28600    |
| lives                   | 28600    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 175868   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.95     |
| episodes                | 28700    |
| lives                   | 28700    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 176434   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.95     |
| episodes                | 28800    |
| lives                   | 28800    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 177057   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.92     |
| episodes                | 28900    |
| lives                   | 28900    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 177668   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.91     |
| episodes                | 29000    |
| lives                   | 29000    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 178274   |
| value_loss              | 2.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.95     |
| episodes                | 29100    |
| lives                   | 29100    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 178855   |
| value_loss              | 2.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.94     |
| episodes                | 29200    |
| lives                   | 29200    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 179489   |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.92     |
| episodes                | 29300    |
| lives                   | 29300    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 180107   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.93     |
| episodes                | 29400    |
| lives                   | 29400    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 180725   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.92     |
| episodes                | 29500    |
| lives                   | 29500    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 181374   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.9      |
| episodes                | 29600    |
| lives                   | 29600    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 181971   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.9      |
| episodes                | 29700    |
| lives                   | 29700    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 182572   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0001   |
| entropy                 | 2.96     |
| episodes                | 29800    |
| lives                   | 29800    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 183159   |
| value_loss              | 2.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.98     |
| episodes                | 29900    |
| lives                   | 29900    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 183789   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.95     |
| episodes                | 30000    |
| lives                   | 30000    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 184405   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 3        |
| episodes                | 30100    |
| lives                   | 30100    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 185018   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.95     |
| episodes                | 30200    |
| lives                   | 30200    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 185604   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.91     |
| episodes                | 30300    |
| lives                   | 30300    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 186202   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.94     |
| episodes                | 30400    |
| lives                   | 30400    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 186815   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.94     |
| episodes                | 30500    |
| lives                   | 30500    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 5.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 187394   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.93     |
| episodes                | 30600    |
| lives                   | 30600    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 188034   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.93     |
| episodes                | 30700    |
| lives                   | 30700    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 188639   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.92     |
| episodes                | 30800    |
| lives                   | 30800    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 189245   |
| value_loss              | 2.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.89     |
| episodes                | 30900    |
| lives                   | 30900    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 189868   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.87     |
| episodes                | 31000    |
| lives                   | 31000    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 190481   |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.9      |
| episodes                | 31100    |
| lives                   | 31100    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 191097   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.85     |
| episodes                | 31200    |
| lives                   | 31200    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 191712   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 2.84     |
| episodes                | 31300    |
| lives                   | 31300    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 192301   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.82     |
| episodes                | 31400    |
| lives                   | 31400    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 192900   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0005   |
| entropy                 | 2.89     |
| episodes                | 31500    |
| lives                   | 31500    |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 4.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 193475   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.91     |
| episodes                | 31600    |
| lives                   | 31600    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 194097   |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.86     |
| episodes                | 31700    |
| lives                   | 31700    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 194692   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.86     |
| episodes                | 31800    |
| lives                   | 31800    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 195304   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.87     |
| episodes                | 31900    |
| lives                   | 31900    |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 5.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 195884   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.86     |
| episodes                | 32000    |
| lives                   | 32000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 196523   |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.84     |
| episodes                | 32100    |
| lives                   | 32100    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 197156   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.85     |
| episodes                | 32200    |
| lives                   | 32200    |
| mean 100 episode length | 6.56     |
| mean 100 episode reward | 4.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 197712   |
| value_loss              | 2.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.86     |
| episodes                | 32300    |
| lives                   | 32300    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 198305   |
| value_loss              | 2.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.88     |
| episodes                | 32400    |
| lives                   | 32400    |
| mean 100 episode length | 6.53     |
| mean 100 episode reward | 5.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 198858   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.85     |
| episodes                | 32500    |
| lives                   | 32500    |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 199422   |
| value_loss              | 1.99     |
--------------------------------------
Saving model due to mean reward increase: 5.7527 -> 5.8513
Saving model due to running mean reward increase: 5.2309 -> 5.8513
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.85     |
| episodes                | 32600    |
| lives                   | 32600    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 5.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 200047   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.91     |
| episodes                | 32700    |
| lives                   | 32700    |
| mean 100 episode length | 6.64     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 200611   |
| value_loss              | 2.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.89     |
| episodes                | 32800    |
| lives                   | 32800    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 201217   |
| value_loss              | 2.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.94     |
| episodes                | 32900    |
| lives                   | 32900    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 201822   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.83     |
| episodes                | 33000    |
| lives                   | 33000    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 202417   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.82     |
| episodes                | 33100    |
| lives                   | 33100    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 202996   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.84     |
| episodes                | 33200    |
| lives                   | 33200    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 203585   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.89     |
| episodes                | 33300    |
| lives                   | 33300    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 204186   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.87     |
| episodes                | 33400    |
| lives                   | 33400    |
| mean 100 episode length | 6.34     |
| mean 100 episode reward | 4.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 204720   |
| value_loss              | 2.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.9      |
| episodes                | 33500    |
| lives                   | 33500    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 5.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 205335   |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.89     |
| episodes                | 33600    |
| lives                   | 33600    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 205916   |
| value_loss              | 2.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 2.9      |
| episodes                | 33700    |
| lives                   | 33700    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 206524   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.93     |
| episodes                | 33800    |
| lives                   | 33800    |
| mean 100 episode length | 6.65     |
| mean 100 episode reward | 4.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 207089   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.93     |
| episodes                | 33900    |
| lives                   | 33900    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 207708   |
| value_loss              | 3.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.94     |
| episodes                | 34000    |
| lives                   | 34000    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 208289   |
| value_loss              | 2.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.91     |
| episodes                | 34100    |
| lives                   | 34100    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 208900   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0005   |
| entropy                 | 2.88     |
| episodes                | 34200    |
| lives                   | 34200    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 209516   |
| value_loss              | 2.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.92     |
| episodes                | 34300    |
| lives                   | 34300    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 210145   |
| value_loss              | 2.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.88     |
| episodes                | 34400    |
| lives                   | 34400    |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 210688   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.96     |
| episodes                | 34500    |
| lives                   | 34500    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 211320   |
| value_loss              | 2.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.93     |
| episodes                | 34600    |
| lives                   | 34600    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 211906   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.89     |
| episodes                | 34700    |
| lives                   | 34700    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 212504   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.88     |
| episodes                | 34800    |
| lives                   | 34800    |
| mean 100 episode length | 6.67     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 213071   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.91     |
| episodes                | 34900    |
| lives                   | 34900    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 213703   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0001   |
| entropy                 | 2.94     |
| episodes                | 35000    |
| lives                   | 35000    |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 5.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 214278   |
| value_loss              | 2.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.95     |
| episodes                | 35100    |
| lives                   | 35100    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 214909   |
| value_loss              | 2.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.94     |
| episodes                | 35200    |
| lives                   | 35200    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 215497   |
| value_loss              | 2.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.96     |
| episodes                | 35300    |
| lives                   | 35300    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 216132   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.93     |
| episodes                | 35400    |
| lives                   | 35400    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 216739   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.93     |
| episodes                | 35500    |
| lives                   | 35500    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 217356   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.9      |
| episodes                | 35600    |
| lives                   | 35600    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 217975   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.89     |
| episodes                | 35700    |
| lives                   | 35700    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 218598   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.89     |
| episodes                | 35800    |
| lives                   | 35800    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 219215   |
| value_loss              | 2.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.85     |
| episodes                | 35900    |
| lives                   | 35900    |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 219783   |
| value_loss              | 2.05     |
--------------------------------------
Saving model due to running mean reward increase: 5.4122 -> 5.6211
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.84     |
| episodes                | 36000    |
| lives                   | 36000    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 220395   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.83     |
| episodes                | 36100    |
| lives                   | 36100    |
| mean 100 episode length | 6.51     |
| mean 100 episode reward | 5.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 220946   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.83     |
| episodes                | 36200    |
| lives                   | 36200    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 221550   |
| value_loss              | 2.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.84     |
| episodes                | 36300    |
| lives                   | 36300    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 5.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 222129   |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.8      |
| episodes                | 36400    |
| lives                   | 36400    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 222728   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.85     |
| episodes                | 36500    |
| lives                   | 36500    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 5.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 223336   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.81     |
| episodes                | 36600    |
| lives                   | 36600    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 223953   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.79     |
| episodes                | 36700    |
| lives                   | 36700    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 224548   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.78     |
| episodes                | 36800    |
| lives                   | 36800    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 225136   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.83     |
| episodes                | 36900    |
| lives                   | 36900    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 225712   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.87     |
| episodes                | 37000    |
| lives                   | 37000    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 226301   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.85     |
| episodes                | 37100    |
| lives                   | 37100    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 226949   |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.83     |
| episodes                | 37200    |
| lives                   | 37200    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 227562   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.006   |
| entropy                 | 2.82     |
| episodes                | 37300    |
| lives                   | 37300    |
| mean 100 episode length | 6.7      |
| mean 100 episode reward | 5.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 228132   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.78     |
| episodes                | 37400    |
| lives                   | 37400    |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 5.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 228693   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.76     |
| episodes                | 37500    |
| lives                   | 37500    |
| mean 100 episode length | 6.73     |
| mean 100 episode reward | 5.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 229266   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.82     |
| episodes                | 37600    |
| lives                   | 37600    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 229857   |
| value_loss              | 1.84     |
--------------------------------------
Saving model due to running mean reward increase: 5.6213 -> 5.6494
--------------------------------------
| approx_kl               | -0.0284  |
| entropy                 | 2.81     |
| episodes                | 37700    |
| lives                   | 37700    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0047   |
| steps                   | 230462   |
| value_loss              | 2.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 2.89     |
| episodes                | 37800    |
| lives                   | 37800    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 231048   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0001   |
| entropy                 | 2.88     |
| episodes                | 37900    |
| lives                   | 37900    |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 5.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 231606   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.87     |
| episodes                | 38000    |
| lives                   | 38000    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 232216   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0091  |
| entropy                 | 2.87     |
| episodes                | 38100    |
| lives                   | 38100    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 232820   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.82     |
| episodes                | 38200    |
| lives                   | 38200    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 233440   |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.88     |
| episodes                | 38300    |
| lives                   | 38300    |
| mean 100 episode length | 6.58     |
| mean 100 episode reward | 5.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 233998   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.82     |
| episodes                | 38400    |
| lives                   | 38400    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 234605   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.83     |
| episodes                | 38500    |
| lives                   | 38500    |
| mean 100 episode length | 6.81     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 235186   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.83     |
| episodes                | 38600    |
| lives                   | 38600    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 235845   |
| value_loss              | 2.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.8      |
| episodes                | 38700    |
| lives                   | 38700    |
| mean 100 episode length | 6.62     |
| mean 100 episode reward | 5.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 236407   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.82     |
| episodes                | 38800    |
| lives                   | 38800    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 236993   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.77     |
| episodes                | 38900    |
| lives                   | 38900    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 5.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 237582   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.81     |
| episodes                | 39000    |
| lives                   | 39000    |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 238164   |
| value_loss              | 2.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.77     |
| episodes                | 39100    |
| lives                   | 39100    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 238768   |
| value_loss              | 2.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.8      |
| episodes                | 39200    |
| lives                   | 39200    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 239345   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.83     |
| episodes                | 39300    |
| lives                   | 39300    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 239955   |
| value_loss              | 2.09     |
--------------------------------------
Saving model due to mean reward increase: 5.8513 -> 5.8527
Saving model due to running mean reward increase: 5.8476 -> 5.8527
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.8      |
| episodes                | 39400    |
| lives                   | 39400    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 240554   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.81     |
| episodes                | 39500    |
| lives                   | 39500    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 241144   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.78     |
| episodes                | 39600    |
| lives                   | 39600    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 241732   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.79     |
| episodes                | 39700    |
| lives                   | 39700    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 242331   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.77     |
| episodes                | 39800    |
| lives                   | 39800    |
| mean 100 episode length | 6.43     |
| mean 100 episode reward | 5.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 242874   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.79     |
| episodes                | 39900    |
| lives                   | 39900    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 243481   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.82     |
| episodes                | 40000    |
| lives                   | 40000    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 244124   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.83     |
| episodes                | 40100    |
| lives                   | 40100    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 244737   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.81     |
| episodes                | 40200    |
| lives                   | 40200    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 245324   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.81     |
| episodes                | 40300    |
| lives                   | 40300    |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 5.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 245887   |
| value_loss              | 2.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.76     |
| episodes                | 40400    |
| lives                   | 40400    |
| mean 100 episode length | 6.84     |
| mean 100 episode reward | 5.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 246471   |
| value_loss              | 2.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.72     |
| episodes                | 40500    |
| lives                   | 40500    |
| mean 100 episode length | 6.91     |
| mean 100 episode reward | 5.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 247062   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 2.84     |
| episodes                | 40600    |
| lives                   | 40600    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 247685   |
| value_loss              | 2.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.006   |
| entropy                 | 2.83     |
| episodes                | 40700    |
| lives                   | 40700    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 248285   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.85     |
| episodes                | 40800    |
| lives                   | 40800    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 248916   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.82     |
| episodes                | 40900    |
| lives                   | 40900    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 249536   |
| value_loss              | 2.33     |
--------------------------------------
Saving model due to mean reward increase: 5.8527 -> 6.1632
Saving model due to running mean reward increase: 5.498 -> 6.1632
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.8      |
| episodes                | 41000    |
| lives                   | 41000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 250175   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 2.81     |
| episodes                | 41100    |
| lives                   | 41100    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 250771   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.83     |
| episodes                | 41200    |
| lives                   | 41200    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 251377   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.82     |
| episodes                | 41300    |
| lives                   | 41300    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 251982   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.79     |
| episodes                | 41400    |
| lives                   | 41400    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 6.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 252600   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.82     |
| episodes                | 41500    |
| lives                   | 41500    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 253239   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.77     |
| episodes                | 41600    |
| lives                   | 41600    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 5.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 253832   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0065  |
| entropy                 | 2.7      |
| episodes                | 41700    |
| lives                   | 41700    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 5.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 254408   |
| value_loss              | 2.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.74     |
| episodes                | 41800    |
| lives                   | 41800    |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 5.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 255002   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.78     |
| episodes                | 41900    |
| lives                   | 41900    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 255605   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.78     |
| episodes                | 42000    |
| lives                   | 42000    |
| mean 100 episode length | 6.68     |
| mean 100 episode reward | 5.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 256173   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.82     |
| episodes                | 42100    |
| lives                   | 42100    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 256739   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.8      |
| episodes                | 42200    |
| lives                   | 42200    |
| mean 100 episode length | 6.75     |
| mean 100 episode reward | 5.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 257314   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.8      |
| episodes                | 42300    |
| lives                   | 42300    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 257940   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.8      |
| episodes                | 42400    |
| lives                   | 42400    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 258574   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.82     |
| episodes                | 42500    |
| lives                   | 42500    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 6.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 259181   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0011   |
| entropy                 | 2.8      |
| episodes                | 42600    |
| lives                   | 42600    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 259813   |
| value_loss              | 2.1      |
--------------------------------------
Saving model due to running mean reward increase: 5.8776 -> 5.8877
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.75     |
| episodes                | 42700    |
| lives                   | 42700    |
| mean 100 episode length | 6.5      |
| mean 100 episode reward | 5.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 260363   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.74     |
| episodes                | 42800    |
| lives                   | 42800    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 260953   |
| value_loss              | 2.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.74     |
| episodes                | 42900    |
| lives                   | 42900    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 261587   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.75     |
| episodes                | 43000    |
| lives                   | 43000    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 262186   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.7      |
| episodes                | 43100    |
| lives                   | 43100    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 262778   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.72     |
| episodes                | 43200    |
| lives                   | 43200    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 263413   |
| value_loss              | 2.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.72     |
| episodes                | 43300    |
| lives                   | 43300    |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 263974   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.81     |
| episodes                | 43400    |
| lives                   | 43400    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 264604   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.81     |
| episodes                | 43500    |
| lives                   | 43500    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 5.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 265202   |
| value_loss              | 2.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.84     |
| episodes                | 43600    |
| lives                   | 43600    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 5.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 265831   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.017   |
| entropy                 | 2.88     |
| episodes                | 43700    |
| lives                   | 43700    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 266462   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.83     |
| episodes                | 43800    |
| lives                   | 43800    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 5.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 267115   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.78     |
| episodes                | 43900    |
| lives                   | 43900    |
| mean 100 episode length | 6.74     |
| mean 100 episode reward | 5.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 267689   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.73     |
| episodes                | 44000    |
| lives                   | 44000    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 268266   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.8      |
| episodes                | 44100    |
| lives                   | 44100    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 268906   |
| value_loss              | 2.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.76     |
| episodes                | 44200    |
| lives                   | 44200    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 5.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 269536   |
| value_loss              | 2.04     |
--------------------------------------
Saving model due to mean reward increase: 6.1632 -> 6.194
Saving model due to running mean reward increase: 5.9092 -> 6.194
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.78     |
| episodes                | 44300    |
| lives                   | 44300    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 5.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 270142   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.87     |
| episodes                | 44400    |
| lives                   | 44400    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 270817   |
| value_loss              | 2.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.82     |
| episodes                | 44500    |
| lives                   | 44500    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 271434   |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.74     |
| episodes                | 44600    |
| lives                   | 44600    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 272057   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.69     |
| episodes                | 44700    |
| lives                   | 44700    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 5.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 272634   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0056  |
| entropy                 | 2.67     |
| episodes                | 44800    |
| lives                   | 44800    |
| mean 100 episode length | 6.66     |
| mean 100 episode reward | 5.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0003   |
| steps                   | 273200   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.68     |
| episodes                | 44900    |
| lives                   | 44900    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 273800   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.69     |
| episodes                | 45000    |
| lives                   | 45000    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 6.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 274393   |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.72     |
| episodes                | 45100    |
| lives                   | 45100    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 275000   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.76     |
| episodes                | 45200    |
| lives                   | 45200    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 5.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 275602   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.76     |
| episodes                | 45300    |
| lives                   | 45300    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 5.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 276232   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.67     |
| episodes                | 45400    |
| lives                   | 45400    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 6.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 276837   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.68     |
| episodes                | 45500    |
| lives                   | 45500    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 277453   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.68     |
| episodes                | 45600    |
| lives                   | 45600    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 278081   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.69     |
| episodes                | 45700    |
| lives                   | 45700    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 278692   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.73     |
| episodes                | 45800    |
| lives                   | 45800    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 5.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 279280   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.69     |
| episodes                | 45900    |
| lives                   | 45900    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 279895   |
| value_loss              | 2.32     |
--------------------------------------
Saving model due to running mean reward increase: 5.7513 -> 6.1827
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.7      |
| episodes                | 46000    |
| lives                   | 46000    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 6.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 280498   |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0002   |
| entropy                 | 2.72     |
| episodes                | 46100    |
| lives                   | 46100    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 281108   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.72     |
| episodes                | 46200    |
| lives                   | 46200    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 281709   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.75     |
| episodes                | 46300    |
| lives                   | 46300    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 5.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 282321   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.69     |
| episodes                | 46400    |
| lives                   | 46400    |
| mean 100 episode length | 6.82     |
| mean 100 episode reward | 6        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 282903   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.68     |
| episodes                | 46500    |
| lives                   | 46500    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 6.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 283504   |
| value_loss              | 2.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.69     |
| episodes                | 46600    |
| lives                   | 46600    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 284101   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.7      |
| episodes                | 46700    |
| lives                   | 46700    |
| mean 100 episode length | 6.87     |
| mean 100 episode reward | 5.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 284688   |
| value_loss              | 2.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0061  |
| entropy                 | 2.67     |
| episodes                | 46800    |
| lives                   | 46800    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 5.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 285291   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 2.7      |
| episodes                | 46900    |
| lives                   | 46900    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 5.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 285904   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0065  |
| entropy                 | 2.62     |
| episodes                | 47000    |
| lives                   | 47000    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 6.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 286502   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.68     |
| episodes                | 47100    |
| lives                   | 47100    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 287121   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.66     |
| episodes                | 47200    |
| lives                   | 47200    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 287739   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 2.63     |
| episodes                | 47300    |
| lives                   | 47300    |
| mean 100 episode length | 6.72     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 288311   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.62     |
| episodes                | 47400    |
| lives                   | 47400    |
| mean 100 episode length | 6.78     |
| mean 100 episode reward | 5.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 288889   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.62     |
| episodes                | 47500    |
| lives                   | 47500    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 6.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 289485   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0107  |
| entropy                 | 2.6      |
| episodes                | 47600    |
| lives                   | 47600    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 290094   |
| value_loss              | 2.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.66     |
| episodes                | 47700    |
| lives                   | 47700    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 290701   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.66     |
| episodes                | 47800    |
| lives                   | 47800    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 291329   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.67     |
| episodes                | 47900    |
| lives                   | 47900    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 291995   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.65     |
| episodes                | 48000    |
| lives                   | 48000    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 6.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 292604   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.64     |
| episodes                | 48100    |
| lives                   | 48100    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 293244   |
| value_loss              | 2.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 2.63     |
| episodes                | 48200    |
| lives                   | 48200    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 293864   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.64     |
| episodes                | 48300    |
| lives                   | 48300    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 294440   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.66     |
| episodes                | 48400    |
| lives                   | 48400    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 295090   |
| value_loss              | 2.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.59     |
| episodes                | 48500    |
| lives                   | 48500    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 295706   |
| value_loss              | 3.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.66     |
| episodes                | 48600    |
| lives                   | 48600    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 296359   |
| value_loss              | 2.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 2.65     |
| episodes                | 48700    |
| lives                   | 48700    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 297008   |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.74     |
| episodes                | 48800    |
| lives                   | 48800    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 297647   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0064  |
| entropy                 | 2.7      |
| episodes                | 48900    |
| lives                   | 48900    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 298233   |
| value_loss              | 2.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.71     |
| episodes                | 49000    |
| lives                   | 49000    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 6.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 298841   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0052  |
| entropy                 | 2.65     |
| episodes                | 49100    |
| lives                   | 49100    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 6.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.011   |
| steps                   | 299482   |
| value_loss              | 1.97     |
--------------------------------------
Saving model due to mean reward increase: 6.194 -> 6.4185
Saving model due to running mean reward increase: 6.3841 -> 6.4185
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 2.65     |
| episodes                | 49200    |
| lives                   | 49200    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 300098   |
| value_loss              | 2.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.65     |
| episodes                | 49300    |
| lives                   | 49300    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 300730   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.62     |
| episodes                | 49400    |
| lives                   | 49400    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 301345   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.65     |
| episodes                | 49500    |
| lives                   | 49500    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 301991   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.65     |
| episodes                | 49600    |
| lives                   | 49600    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 5.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 302587   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.62     |
| episodes                | 49700    |
| lives                   | 49700    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 6.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 303182   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.65     |
| episodes                | 49800    |
| lives                   | 49800    |
| mean 100 episode length | 7.07     |
| mean 100 episode reward | 6.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 303789   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.67     |
| episodes                | 49900    |
| lives                   | 49900    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 304437   |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.73     |
| episodes                | 50000    |
| lives                   | 50000    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 305045   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.7      |
| episodes                | 50100    |
| lives                   | 50100    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 305685   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.66     |
| episodes                | 50200    |
| lives                   | 50200    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 6.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 306331   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.64     |
| episodes                | 50300    |
| lives                   | 50300    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 5.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 306917   |
| value_loss              | 2.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.64     |
| episodes                | 50400    |
| lives                   | 50400    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 307528   |
| value_loss              | 2.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.66     |
| episodes                | 50500    |
| lives                   | 50500    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 5.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 308113   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0065  |
| entropy                 | 2.63     |
| episodes                | 50600    |
| lives                   | 50600    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 6.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 308727   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.59     |
| episodes                | 50700    |
| lives                   | 50700    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 309329   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.56     |
| episodes                | 50800    |
| lives                   | 50800    |
| mean 100 episode length | 6.8      |
| mean 100 episode reward | 6.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 309909   |
| value_loss              | 2.14     |
--------------------------------------
Saving model due to running mean reward increase: 6.2605 -> 6.4027
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.54     |
| episodes                | 50900    |
| lives                   | 50900    |
| mean 100 episode length | 6.63     |
| mean 100 episode reward | 5.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 310472   |
| value_loss              | 2.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.57     |
| episodes                | 51000    |
| lives                   | 51000    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 311091   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.63     |
| episodes                | 51100    |
| lives                   | 51100    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 311743   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.53     |
| episodes                | 51200    |
| lives                   | 51200    |
| mean 100 episode length | 6.77     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 312320   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.52     |
| episodes                | 51300    |
| lives                   | 51300    |
| mean 100 episode length | 6.61     |
| mean 100 episode reward | 5.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 312881   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.59     |
| episodes                | 51400    |
| lives                   | 51400    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 313483   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.59     |
| episodes                | 51500    |
| lives                   | 51500    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 314089   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.57     |
| episodes                | 51600    |
| lives                   | 51600    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 314693   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.58     |
| episodes                | 51700    |
| lives                   | 51700    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 315319   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.55     |
| episodes                | 51800    |
| lives                   | 51800    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 6.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 315921   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0065  |
| entropy                 | 2.54     |
| episodes                | 51900    |
| lives                   | 51900    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 6.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 316513   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.54     |
| episodes                | 52000    |
| lives                   | 52000    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 317144   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.53     |
| episodes                | 52100    |
| lives                   | 52100    |
| mean 100 episode length | 6.94     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 317738   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.48     |
| episodes                | 52200    |
| lives                   | 52200    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 318350   |
| value_loss              | 2.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.51     |
| episodes                | 52300    |
| lives                   | 52300    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 318974   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.51     |
| episodes                | 52400    |
| lives                   | 52400    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 6.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 319588   |
| value_loss              | 2.22     |
--------------------------------------
Saving model due to mean reward increase: 6.4185 -> 6.6687
Saving model due to running mean reward increase: 6.2438 -> 6.6687
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.52     |
| episodes                | 52500    |
| lives                   | 52500    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 320213   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.52     |
| episodes                | 52600    |
| lives                   | 52600    |
| mean 100 episode length | 6.69     |
| mean 100 episode reward | 6.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 320782   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0069  |
| entropy                 | 2.57     |
| episodes                | 52700    |
| lives                   | 52700    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 321420   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.57     |
| episodes                | 52800    |
| lives                   | 52800    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 322038   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.56     |
| episodes                | 52900    |
| lives                   | 52900    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 322637   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.52     |
| episodes                | 53000    |
| lives                   | 53000    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 6.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 323227   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.51     |
| episodes                | 53100    |
| lives                   | 53100    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 323856   |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.56     |
| episodes                | 53200    |
| lives                   | 53200    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 324465   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.52     |
| episodes                | 53300    |
| lives                   | 53300    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 325051   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.55     |
| episodes                | 53400    |
| lives                   | 53400    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 325679   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.52     |
| episodes                | 53500    |
| lives                   | 53500    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 326303   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.51     |
| episodes                | 53600    |
| lives                   | 53600    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 326937   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.5      |
| episodes                | 53700    |
| lives                   | 53700    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 327603   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.54     |
| episodes                | 53800    |
| lives                   | 53800    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 328253   |
| value_loss              | 2.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.47     |
| episodes                | 53900    |
| lives                   | 53900    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 328886   |
| value_loss              | 2.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.48     |
| episodes                | 54000    |
| lives                   | 54000    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 329508   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.5      |
| episodes                | 54100    |
| lives                   | 54100    |
| mean 100 episode length | 6.97     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 330105   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.53     |
| episodes                | 54200    |
| lives                   | 54200    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 330745   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.54     |
| episodes                | 54300    |
| lives                   | 54300    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 331375   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.53     |
| episodes                | 54400    |
| lives                   | 54400    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 6.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 331995   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.54     |
| episodes                | 54500    |
| lives                   | 54500    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 332610   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.54     |
| episodes                | 54600    |
| lives                   | 54600    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 333246   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.52     |
| episodes                | 54700    |
| lives                   | 54700    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 6.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 333832   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.53     |
| episodes                | 54800    |
| lives                   | 54800    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 334485   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.52     |
| episodes                | 54900    |
| lives                   | 54900    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 335115   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.52     |
| episodes                | 55000    |
| lives                   | 55000    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 335769   |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 2.54     |
| episodes                | 55100    |
| lives                   | 55100    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0115  |
| steps                   | 336370   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.54     |
| episodes                | 55200    |
| lives                   | 55200    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 336970   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.54     |
| episodes                | 55300    |
| lives                   | 55300    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 337608   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.59     |
| episodes                | 55400    |
| lives                   | 55400    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 6.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 338258   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.57     |
| episodes                | 55500    |
| lives                   | 55500    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 338893   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.52     |
| episodes                | 55600    |
| lives                   | 55600    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 339519   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.49     |
| episodes                | 55700    |
| lives                   | 55700    |
| mean 100 episode length | 6.59     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 340078   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.55     |
| episodes                | 55800    |
| lives                   | 55800    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 340723   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 2.52     |
| episodes                | 55900    |
| lives                   | 55900    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 341342   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.54     |
| episodes                | 56000    |
| lives                   | 56000    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 341955   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.53     |
| episodes                | 56100    |
| lives                   | 56100    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 342636   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.48     |
| episodes                | 56200    |
| lives                   | 56200    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 343237   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.54     |
| episodes                | 56300    |
| lives                   | 56300    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 343868   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.46     |
| episodes                | 56400    |
| lives                   | 56400    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 344495   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.54     |
| episodes                | 56500    |
| lives                   | 56500    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 345139   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.5      |
| episodes                | 56600    |
| lives                   | 56600    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 6.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 345728   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.54     |
| episodes                | 56700    |
| lives                   | 56700    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 6.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 346350   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.5      |
| episodes                | 56800    |
| lives                   | 56800    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 346969   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.52     |
| episodes                | 56900    |
| lives                   | 56900    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 6.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 347606   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.54     |
| episodes                | 57000    |
| lives                   | 57000    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 348204   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.51     |
| episodes                | 57100    |
| lives                   | 57100    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 348812   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.53     |
| episodes                | 57200    |
| lives                   | 57200    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 349452   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.5      |
| episodes                | 57300    |
| lives                   | 57300    |
| mean 100 episode length | 6.88     |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 350040   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.49     |
| episodes                | 57400    |
| lives                   | 57400    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 350632   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.5      |
| episodes                | 57500    |
| lives                   | 57500    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 6.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 351255   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.51     |
| episodes                | 57600    |
| lives                   | 57600    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 351853   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.52     |
| episodes                | 57700    |
| lives                   | 57700    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 352488   |
| value_loss              | 2.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.52     |
| episodes                | 57800    |
| lives                   | 57800    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 353073   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.47     |
| episodes                | 57900    |
| lives                   | 57900    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 353672   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.48     |
| episodes                | 58000    |
| lives                   | 58000    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 354304   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.43     |
| episodes                | 58100    |
| lives                   | 58100    |
| mean 100 episode length | 6.9      |
| mean 100 episode reward | 6.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 354894   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.46     |
| episodes                | 58200    |
| lives                   | 58200    |
| mean 100 episode length | 7.17     |
| mean 100 episode reward | 6.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 355511   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.45     |
| episodes                | 58300    |
| lives                   | 58300    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 7.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 356136   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.5      |
| episodes                | 58400    |
| lives                   | 58400    |
| mean 100 episode length | 6.93     |
| mean 100 episode reward | 6.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 356729   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.5      |
| episodes                | 58500    |
| lives                   | 58500    |
| mean 100 episode length | 7.22     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 357351   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0068  |
| entropy                 | 2.59     |
| episodes                | 58600    |
| lives                   | 58600    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 7.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 357995   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.57     |
| episodes                | 58700    |
| lives                   | 58700    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 358651   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0081  |
| entropy                 | 2.59     |
| episodes                | 58800    |
| lives                   | 58800    |
| mean 100 episode length | 6.98     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 359249   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.67     |
| episodes                | 58900    |
| lives                   | 58900    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 6.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 359902   |
| value_loss              | 2.43     |
--------------------------------------
Saving model due to running mean reward increase: 6.2518 -> 6.6663
--------------------------------------
| approx_kl               | -0.0332  |
| entropy                 | 2.73     |
| episodes                | 59000    |
| lives                   | 59000    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 5.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0085   |
| steps                   | 360528   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.74     |
| episodes                | 59100    |
| lives                   | 59100    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 361174   |
| value_loss              | 2.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.7      |
| episodes                | 59200    |
| lives                   | 59200    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 361843   |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.72     |
| episodes                | 59300    |
| lives                   | 59300    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 362503   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.66     |
| episodes                | 59400    |
| lives                   | 59400    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 363165   |
| value_loss              | 2.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.61     |
| episodes                | 59500    |
| lives                   | 59500    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 363760   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.014   |
| entropy                 | 2.65     |
| episodes                | 59600    |
| lives                   | 59600    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0119  |
| steps                   | 364395   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.6      |
| episodes                | 59700    |
| lives                   | 59700    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 6.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 365060   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.59     |
| episodes                | 59800    |
| lives                   | 59800    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 365712   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.56     |
| episodes                | 59900    |
| lives                   | 59900    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 6.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 366317   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.58     |
| episodes                | 60000    |
| lives                   | 60000    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 6.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 366937   |
| value_loss              | 2.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.62     |
| episodes                | 60100    |
| lives                   | 60100    |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 367613   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 2.65     |
| episodes                | 60200    |
| lives                   | 60200    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 368251   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 2.61     |
| episodes                | 60300    |
| lives                   | 60300    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 368900   |
| value_loss              | 2.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.64     |
| episodes                | 60400    |
| lives                   | 60400    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 6.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 369543   |
| value_loss              | 1.93     |
--------------------------------------
Saving model due to running mean reward increase: 6.2392 -> 6.5616
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.58     |
| episodes                | 60500    |
| lives                   | 60500    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 6.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 370176   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.6      |
| episodes                | 60600    |
| lives                   | 60600    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 6.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 370800   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.57     |
| episodes                | 60700    |
| lives                   | 60700    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 6.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 371404   |
| value_loss              | 2.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.51     |
| episodes                | 60800    |
| lives                   | 60800    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 372013   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.5      |
| episodes                | 60900    |
| lives                   | 60900    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 372605   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.54     |
| episodes                | 61000    |
| lives                   | 61000    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 373236   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.54     |
| episodes                | 61100    |
| lives                   | 61100    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 373878   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0066  |
| entropy                 | 2.59     |
| episodes                | 61200    |
| lives                   | 61200    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 374514   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.59     |
| episodes                | 61300    |
| lives                   | 61300    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0113  |
| steps                   | 375155   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.52     |
| episodes                | 61400    |
| lives                   | 61400    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 375779   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.52     |
| episodes                | 61500    |
| lives                   | 61500    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 376390   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.52     |
| episodes                | 61600    |
| lives                   | 61600    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 377030   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.61     |
| episodes                | 61700    |
| lives                   | 61700    |
| mean 100 episode length | 7.12     |
| mean 100 episode reward | 6.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 377642   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.56     |
| episodes                | 61800    |
| lives                   | 61800    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 6.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 378250   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.54     |
| episodes                | 61900    |
| lives                   | 61900    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 6.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 378865   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.5      |
| episodes                | 62000    |
| lives                   | 62000    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 6.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 379469   |
| value_loss              | 2.34     |
--------------------------------------
Saving model due to running mean reward increase: 6.462 -> 6.5503
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.47     |
| episodes                | 62100    |
| lives                   | 62100    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0109  |
| steps                   | 380090   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.48     |
| episodes                | 62200    |
| lives                   | 62200    |
| mean 100 episode length | 7.44     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 380734   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0067  |
| entropy                 | 2.48     |
| episodes                | 62300    |
| lives                   | 62300    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 7.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 381387   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.49     |
| episodes                | 62400    |
| lives                   | 62400    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 6.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 382025   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0079  |
| entropy                 | 2.5      |
| episodes                | 62500    |
| lives                   | 62500    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 382652   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.52     |
| episodes                | 62600    |
| lives                   | 62600    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 383279   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.51     |
| episodes                | 62700    |
| lives                   | 62700    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 6.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 383908   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0003   |
| entropy                 | 2.47     |
| episodes                | 62800    |
| lives                   | 62800    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 384542   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.46     |
| episodes                | 62900    |
| lives                   | 62900    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 385142   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.43     |
| episodes                | 63000    |
| lives                   | 63000    |
| mean 100 episode length | 7.01     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 385743   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.42     |
| episodes                | 63100    |
| lives                   | 63100    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 386353   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.47     |
| episodes                | 63200    |
| lives                   | 63200    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 7.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 386988   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.46     |
| episodes                | 63300    |
| lives                   | 63300    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 6.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 387573   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.44     |
| episodes                | 63400    |
| lives                   | 63400    |
| mean 100 episode length | 6.86     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 388159   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.47     |
| episodes                | 63500    |
| lives                   | 63500    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 7.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 388816   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.46     |
| episodes                | 63600    |
| lives                   | 63600    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 7.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 389456   |
| value_loss              | 2.24     |
--------------------------------------
Saving model due to mean reward increase: 6.6687 -> 6.9133
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.49     |
| episodes                | 63700    |
| lives                   | 63700    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 390091   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.5      |
| episodes                | 63800    |
| lives                   | 63800    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 6.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 390686   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.007   |
| entropy                 | 2.53     |
| episodes                | 63900    |
| lives                   | 63900    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 391346   |
| value_loss              | 2.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.51     |
| episodes                | 64000    |
| lives                   | 64000    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 391976   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.47     |
| episodes                | 64100    |
| lives                   | 64100    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 6.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 392586   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.51     |
| episodes                | 64200    |
| lives                   | 64200    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 393253   |
| value_loss              | 2.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.49     |
| episodes                | 64300    |
| lives                   | 64300    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 6.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 393859   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 2.53     |
| episodes                | 64400    |
| lives                   | 64400    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 394510   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0057  |
| entropy                 | 2.53     |
| episodes                | 64500    |
| lives                   | 64500    |
| mean 100 episode length | 7.32     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 395142   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0059  |
| entropy                 | 2.53     |
| episodes                | 64600    |
| lives                   | 64600    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 7.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0127  |
| steps                   | 395794   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.5      |
| episodes                | 64700    |
| lives                   | 64700    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 6.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 396370   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.52     |
| episodes                | 64800    |
| lives                   | 64800    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 396994   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.55     |
| episodes                | 64900    |
| lives                   | 64900    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 397651   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.55     |
| episodes                | 65000    |
| lives                   | 65000    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 398293   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.48     |
| episodes                | 65100    |
| lives                   | 65100    |
| mean 100 episode length | 6.85     |
| mean 100 episode reward | 6.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 398878   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.5      |
| episodes                | 65200    |
| lives                   | 65200    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 399506   |
| value_loss              | 2.23     |
--------------------------------------
Saving model due to mean reward increase: 6.9133 -> 7.0762
Saving model due to running mean reward increase: 6.8058 -> 7.0762
--------------------------------------
| approx_kl               | -0.0054  |
| entropy                 | 2.48     |
| episodes                | 65300    |
| lives                   | 65300    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 400142   |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.49     |
| episodes                | 65400    |
| lives                   | 65400    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 400734   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.48     |
| episodes                | 65500    |
| lives                   | 65500    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 401385   |
| value_loss              | 2.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.48     |
| episodes                | 65600    |
| lives                   | 65600    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 402001   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 2.49     |
| episodes                | 65700    |
| lives                   | 65700    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 6.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 402615   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 2.47     |
| episodes                | 65800    |
| lives                   | 65800    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 6.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 403228   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.41     |
| episodes                | 65900    |
| lives                   | 65900    |
| mean 100 episode length | 6.95     |
| mean 100 episode reward | 6.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 403823   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.41     |
| episodes                | 66000    |
| lives                   | 66000    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 404444   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.41     |
| episodes                | 66100    |
| lives                   | 66100    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 405065   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.42     |
| episodes                | 66200    |
| lives                   | 66200    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 6.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 405675   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.4      |
| episodes                | 66300    |
| lives                   | 66300    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 406303   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.39     |
| episodes                | 66400    |
| lives                   | 66400    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 7.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 406916   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 2.35     |
| episodes                | 66500    |
| lives                   | 66500    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 407515   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.43     |
| episodes                | 66600    |
| lives                   | 66600    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 7.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 408151   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.42     |
| episodes                | 66700    |
| lives                   | 66700    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 7.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 408777   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0006   |
| entropy                 | 2.39     |
| episodes                | 66800    |
| lives                   | 66800    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 7.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 409413   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.39     |
| episodes                | 66900    |
| lives                   | 66900    |
| mean 100 episode length | 7.15     |
| mean 100 episode reward | 6.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 410028   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0002   |
| entropy                 | 2.43     |
| episodes                | 67000    |
| lives                   | 67000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 7.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 410667   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.43     |
| episodes                | 67100    |
| lives                   | 67100    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 411285   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.38     |
| episodes                | 67200    |
| lives                   | 67200    |
| mean 100 episode length | 7.09     |
| mean 100 episode reward | 6.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 411894   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0076  |
| entropy                 | 2.38     |
| episodes                | 67300    |
| lives                   | 67300    |
| mean 100 episode length | 7.05     |
| mean 100 episode reward | 6.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0129  |
| steps                   | 412499   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.37     |
| episodes                | 67400    |
| lives                   | 67400    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 7.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 413109   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0061  |
| entropy                 | 2.43     |
| episodes                | 67500    |
| lives                   | 67500    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 7.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 413793   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.47     |
| episodes                | 67600    |
| lives                   | 67600    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 414461   |
| value_loss              | 2.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.38     |
| episodes                | 67700    |
| lives                   | 67700    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 7.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 415091   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.39     |
| episodes                | 67800    |
| lives                   | 67800    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 7.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 415710   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.41     |
| episodes                | 67900    |
| lives                   | 67900    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 416331   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.44     |
| episodes                | 68000    |
| lives                   | 68000    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 416981   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.52     |
| episodes                | 68100    |
| lives                   | 68100    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 7.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 417634   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.5      |
| episodes                | 68200    |
| lives                   | 68200    |
| mean 100 episode length | 6.76     |
| mean 100 episode reward | 6.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 418210   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.5      |
| episodes                | 68300    |
| lives                   | 68300    |
| mean 100 episode length | 7.1      |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 418820   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.51     |
| episodes                | 68400    |
| lives                   | 68400    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 419463   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.55     |
| episodes                | 68500    |
| lives                   | 68500    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 420099   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.49     |
| episodes                | 68600    |
| lives                   | 68600    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 6.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 420701   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.48     |
| episodes                | 68700    |
| lives                   | 68700    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 6.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 421305   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.5      |
| episodes                | 68800    |
| lives                   | 68800    |
| mean 100 episode length | 7.08     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 421913   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.45     |
| episodes                | 68900    |
| lives                   | 68900    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 6.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 422538   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.43     |
| episodes                | 69000    |
| lives                   | 69000    |
| mean 100 episode length | 7        |
| mean 100 episode reward | 6.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 423138   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.41     |
| episodes                | 69100    |
| lives                   | 69100    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 423744   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 2.41     |
| episodes                | 69200    |
| lives                   | 69200    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 424398   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.41     |
| episodes                | 69300    |
| lives                   | 69300    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 6.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 425021   |
| value_loss              | 2.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.41     |
| episodes                | 69400    |
| lives                   | 69400    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 7.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 425650   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.41     |
| episodes                | 69500    |
| lives                   | 69500    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 426286   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.41     |
| episodes                | 69600    |
| lives                   | 69600    |
| mean 100 episode length | 7.25     |
| mean 100 episode reward | 7.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 426911   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.41     |
| episodes                | 69700    |
| lives                   | 69700    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 427539   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.41     |
| episodes                | 69800    |
| lives                   | 69800    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 428193   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.37     |
| episodes                | 69900    |
| lives                   | 69900    |
| mean 100 episode length | 6.99     |
| mean 100 episode reward | 6.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 428792   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.38     |
| episodes                | 70000    |
| lives                   | 70000    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 7.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 429428   |
| value_loss              | 1.92     |
--------------------------------------
Saving model due to mean reward increase: 7.0762 -> 7.1361
Saving model due to running mean reward increase: 6.9687 -> 7.1361
--------------------------------------
| approx_kl               | -0.0052  |
| entropy                 | 2.4      |
| episodes                | 70100    |
| lives                   | 70100    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 430063   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.44     |
| episodes                | 70200    |
| lives                   | 70200    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 430704   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.44     |
| episodes                | 70300    |
| lives                   | 70300    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 6.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 431315   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.45     |
| episodes                | 70400    |
| lives                   | 70400    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0096  |
| steps                   | 431955   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.48     |
| episodes                | 70500    |
| lives                   | 70500    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 432594   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.4      |
| episodes                | 70600    |
| lives                   | 70600    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 433213   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.47     |
| episodes                | 70700    |
| lives                   | 70700    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 433836   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.46     |
| episodes                | 70800    |
| lives                   | 70800    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 434507   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.41     |
| episodes                | 70900    |
| lives                   | 70900    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 435138   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.38     |
| episodes                | 71000    |
| lives                   | 71000    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 435769   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.43     |
| episodes                | 71100    |
| lives                   | 71100    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 7.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 436412   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.43     |
| episodes                | 71200    |
| lives                   | 71200    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 437033   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.45     |
| episodes                | 71300    |
| lives                   | 71300    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 437663   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.44     |
| episodes                | 71400    |
| lives                   | 71400    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 7.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 438298   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.42     |
| episodes                | 71500    |
| lives                   | 71500    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 7.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 438947   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.4      |
| episodes                | 71600    |
| lives                   | 71600    |
| mean 100 episode length | 7.11     |
| mean 100 episode reward | 6.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 439558   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.39     |
| episodes                | 71700    |
| lives                   | 71700    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 440171   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.39     |
| episodes                | 71800    |
| lives                   | 71800    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 7.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 440828   |
| value_loss              | 2.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.41     |
| episodes                | 71900    |
| lives                   | 71900    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 441486   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.48     |
| episodes                | 72000    |
| lives                   | 72000    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 7.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 442175   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.45     |
| episodes                | 72100    |
| lives                   | 72100    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 7.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 442871   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.37     |
| episodes                | 72200    |
| lives                   | 72200    |
| mean 100 episode length | 7.16     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 443487   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.37     |
| episodes                | 72300    |
| lives                   | 72300    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 7.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 444129   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.41     |
| episodes                | 72400    |
| lives                   | 72400    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 7.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 444767   |
| value_loss              | 2.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.41     |
| episodes                | 72500    |
| lives                   | 72500    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 7.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 445401   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0087  |
| entropy                 | 2.45     |
| episodes                | 72600    |
| lives                   | 72600    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 446029   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.42     |
| episodes                | 72700    |
| lives                   | 72700    |
| mean 100 episode length | 6.96     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 446625   |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.47     |
| episodes                | 72800    |
| lives                   | 72800    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 447306   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.46     |
| episodes                | 72900    |
| lives                   | 72900    |
| mean 100 episode length | 7.2      |
| mean 100 episode reward | 6.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 447926   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.34     |
| episodes                | 73000    |
| lives                   | 73000    |
| mean 100 episode length | 6.89     |
| mean 100 episode reward | 6.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 448515   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.36     |
| episodes                | 73100    |
| lives                   | 73100    |
| mean 100 episode length | 7.26     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 449141   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.38     |
| episodes                | 73200    |
| lives                   | 73200    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 449782   |
| value_loss              | 1.84     |
--------------------------------------
Saving model due to mean reward increase: 7.1361 -> 7.1971
Saving model due to running mean reward increase: 6.8046 -> 7.1971
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.34     |
| episodes                | 73300    |
| lives                   | 73300    |
| mean 100 episode length | 7.03     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 450385   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.35     |
| episodes                | 73400    |
| lives                   | 73400    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 451012   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.35     |
| episodes                | 73500    |
| lives                   | 73500    |
| mean 100 episode length | 7.06     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 451618   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.35     |
| episodes                | 73600    |
| lives                   | 73600    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 6.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 452220   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.43     |
| episodes                | 73700    |
| lives                   | 73700    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 452872   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.43     |
| episodes                | 73800    |
| lives                   | 73800    |
| mean 100 episode length | 6.79     |
| mean 100 episode reward | 6.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 453451   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.5      |
| episodes                | 73900    |
| lives                   | 73900    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 6.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 454112   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 2.43     |
| episodes                | 74000    |
| lives                   | 74000    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 454754   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0063  |
| entropy                 | 2.38     |
| episodes                | 74100    |
| lives                   | 74100    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 455381   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.34     |
| episodes                | 74200    |
| lives                   | 74200    |
| mean 100 episode length | 7.04     |
| mean 100 episode reward | 6.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 455985   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.31     |
| episodes                | 74300    |
| lives                   | 74300    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 456604   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.33     |
| episodes                | 74400    |
| lives                   | 74400    |
| mean 100 episode length | 7.14     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 457218   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.33     |
| episodes                | 74500    |
| lives                   | 74500    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 7        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 457852   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.29     |
| episodes                | 74600    |
| lives                   | 74600    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 7.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 458497   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.34     |
| episodes                | 74700    |
| lives                   | 74700    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 7.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 459127   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.31     |
| episodes                | 74800    |
| lives                   | 74800    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 459755   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.29     |
| episodes                | 74900    |
| lives                   | 74900    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 7.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 460388   |
| value_loss              | 2.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 2.32     |
| episodes                | 75000    |
| lives                   | 75000    |
| mean 100 episode length | 7.18     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 461006   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.28     |
| episodes                | 75100    |
| lives                   | 75100    |
| mean 100 episode length | 7.13     |
| mean 100 episode reward | 6.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 461619   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.36     |
| episodes                | 75200    |
| lives                   | 75200    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 7.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 462293   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.36     |
| episodes                | 75300    |
| lives                   | 75300    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 7.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 462939   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0082  |
| entropy                 | 2.3      |
| episodes                | 75400    |
| lives                   | 75400    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 6.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0108  |
| steps                   | 463576   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.29     |
| episodes                | 75500    |
| lives                   | 75500    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 7.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 464210   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0005   |
| entropy                 | 2.28     |
| episodes                | 75600    |
| lives                   | 75600    |
| mean 100 episode length | 7.35     |
| mean 100 episode reward | 7.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 464845   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.25     |
| episodes                | 75700    |
| lives                   | 75700    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 7.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 465474   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.3      |
| episodes                | 75800    |
| lives                   | 75800    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 7.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 466129   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.27     |
| episodes                | 75900    |
| lives                   | 75900    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 466771   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.33     |
| episodes                | 76000    |
| lives                   | 76000    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 467430   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.35     |
| episodes                | 76100    |
| lives                   | 76100    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 6.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 468087   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.36     |
| episodes                | 76200    |
| lives                   | 76200    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 7.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 468759   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.35     |
| episodes                | 76300    |
| lives                   | 76300    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 469429   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.35     |
| episodes                | 76400    |
| lives                   | 76400    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 470100   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.27     |
| episodes                | 76500    |
| lives                   | 76500    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 470755   |
| value_loss              | 2.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.25     |
| episodes                | 76600    |
| lives                   | 76600    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 7.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 471413   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.28     |
| episodes                | 76700    |
| lives                   | 76700    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 472049   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.33     |
| episodes                | 76800    |
| lives                   | 76800    |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 7.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 472746   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.3      |
| episodes                | 76900    |
| lives                   | 76900    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 7.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 473403   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.33     |
| episodes                | 77000    |
| lives                   | 77000    |
| mean 100 episode length | 7.5      |
| mean 100 episode reward | 7.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 474053   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.28     |
| episodes                | 77100    |
| lives                   | 77100    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 7.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 474691   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.3      |
| episodes                | 77200    |
| lives                   | 77200    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 475337   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 2.29     |
| episodes                | 77300    |
| lives                   | 77300    |
| mean 100 episode length | 7.27     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 475964   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.29     |
| episodes                | 77400    |
| lives                   | 77400    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 476629   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.31     |
| episodes                | 77500    |
| lives                   | 77500    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 7.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 477276   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.34     |
| episodes                | 77600    |
| lives                   | 77600    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 7.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 477959   |
| value_loss              | 2.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.36     |
| episodes                | 77700    |
| lives                   | 77700    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 7.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 478628   |
| value_loss              | 2.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.37     |
| episodes                | 77800    |
| lives                   | 77800    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 7.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 479289   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.35     |
| episodes                | 77900    |
| lives                   | 77900    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 6.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 479929   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.29     |
| episodes                | 78000    |
| lives                   | 78000    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 7.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 480601   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.27     |
| episodes                | 78100    |
| lives                   | 78100    |
| mean 100 episode length | 7.28     |
| mean 100 episode reward | 7.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 481229   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.22     |
| episodes                | 78200    |
| lives                   | 78200    |
| mean 100 episode length | 6.92     |
| mean 100 episode reward | 6.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 481821   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.31     |
| episodes                | 78300    |
| lives                   | 78300    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 482457   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.32     |
| episodes                | 78400    |
| lives                   | 78400    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 7.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 483095   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.31     |
| episodes                | 78500    |
| lives                   | 78500    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 7.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 483741   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.29     |
| episodes                | 78600    |
| lives                   | 78600    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 7.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 484370   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.32     |
| episodes                | 78700    |
| lives                   | 78700    |
| mean 100 episode length | 7.23     |
| mean 100 episode reward | 7.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 484993   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0109  |
| entropy                 | 2.31     |
| episodes                | 78800    |
| lives                   | 78800    |
| mean 100 episode length | 7.24     |
| mean 100 episode reward | 6.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0106  |
| steps                   | 485617   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.38     |
| episodes                | 78900    |
| lives                   | 78900    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 486279   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.3      |
| episodes                | 79000    |
| lives                   | 79000    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 6.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 486924   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.006   |
| entropy                 | 2.17     |
| episodes                | 79100    |
| lives                   | 79100    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 7.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 487555   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.18     |
| episodes                | 79200    |
| lives                   | 79200    |
| mean 100 episode length | 7.3      |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 488185   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0091  |
| entropy                 | 2.23     |
| episodes                | 79300    |
| lives                   | 79300    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 7.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 488819   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.35     |
| episodes                | 79400    |
| lives                   | 79400    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 489502   |
| value_loss              | 2.13     |
--------------------------------------
Saving model due to mean reward increase: 7.1971 -> 7.4174
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.29     |
| episodes                | 79500    |
| lives                   | 79500    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 490149   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.31     |
| episodes                | 79600    |
| lives                   | 79600    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 7.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 490792   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.32     |
| episodes                | 79700    |
| lives                   | 79700    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 7.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 491477   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.31     |
| episodes                | 79800    |
| lives                   | 79800    |
| mean 100 episode length | 7.37     |
| mean 100 episode reward | 7.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 492114   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.33     |
| episodes                | 79900    |
| lives                   | 79900    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 8.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 492829   |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 2.28     |
| episodes                | 80000    |
| lives                   | 80000    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 7.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 493474   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.27     |
| episodes                | 80100    |
| lives                   | 80100    |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 494134   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.28     |
| episodes                | 80200    |
| lives                   | 80200    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 7.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 494785   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.26     |
| episodes                | 80300    |
| lives                   | 80300    |
| mean 100 episode length | 7.02     |
| mean 100 episode reward | 6.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 495387   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.23     |
| episodes                | 80400    |
| lives                   | 80400    |
| mean 100 episode length | 7.4      |
| mean 100 episode reward | 7.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 496027   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.25     |
| episodes                | 80500    |
| lives                   | 80500    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 496681   |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.26     |
| episodes                | 80600    |
| lives                   | 80600    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 7.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 497363   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.24     |
| episodes                | 80700    |
| lives                   | 80700    |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 7.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 498069   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 2.23     |
| episodes                | 80800    |
| lives                   | 80800    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 7.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 498752   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.2      |
| episodes                | 80900    |
| lives                   | 80900    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 499408   |
| value_loss              | 1.71     |
--------------------------------------
Saving model due to mean reward increase: 7.4174 -> 7.5416
Saving model due to running mean reward increase: 7.3602 -> 7.5416
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.23     |
| episodes                | 81000    |
| lives                   | 81000    |
| mean 100 episode length | 7.39     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 500047   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.27     |
| episodes                | 81100    |
| lives                   | 81100    |
| mean 100 episode length | 7.29     |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 500676   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.29     |
| episodes                | 81200    |
| lives                   | 81200    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 7.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 501338   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.23     |
| episodes                | 81300    |
| lives                   | 81300    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 7.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 501993   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.24     |
| episodes                | 81400    |
| lives                   | 81400    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 502649   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.26     |
| episodes                | 81500    |
| lives                   | 81500    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 503316   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.28     |
| episodes                | 81600    |
| lives                   | 81600    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 7.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 503984   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.29     |
| episodes                | 81700    |
| lives                   | 81700    |
| mean 100 episode length | 7.21     |
| mean 100 episode reward | 6.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 504605   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.32     |
| episodes                | 81800    |
| lives                   | 81800    |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 7.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 505293   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.31     |
| episodes                | 81900    |
| lives                   | 81900    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 505949   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.28     |
| episodes                | 82000    |
| lives                   | 82000    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 7.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 506568   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.28     |
| episodes                | 82100    |
| lives                   | 82100    |
| mean 100 episode length | 7.42     |
| mean 100 episode reward | 7.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0103  |
| steps                   | 507210   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.27     |
| episodes                | 82200    |
| lives                   | 82200    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 507866   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.27     |
| episodes                | 82300    |
| lives                   | 82300    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 508525   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.27     |
| episodes                | 82400    |
| lives                   | 82400    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 7.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 509163   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.26     |
| episodes                | 82500    |
| lives                   | 82500    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 7.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 509811   |
| value_loss              | 1.83     |
--------------------------------------
Saving model due to running mean reward increase: 7.0738 -> 7.3386
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.28     |
| episodes                | 82600    |
| lives                   | 82600    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 510481   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.32     |
| episodes                | 82700    |
| lives                   | 82700    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 511137   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.3      |
| episodes                | 82800    |
| lives                   | 82800    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 7.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 511795   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 2.28     |
| episodes                | 82900    |
| lives                   | 82900    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 7.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 512484   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.24     |
| episodes                | 83000    |
| lives                   | 83000    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 513164   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.21     |
| episodes                | 83100    |
| lives                   | 83100    |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 7.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 513829   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.16     |
| episodes                | 83200    |
| lives                   | 83200    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 7.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 514478   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.2      |
| episodes                | 83300    |
| lives                   | 83300    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 7.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 515135   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.23     |
| episodes                | 83400    |
| lives                   | 83400    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 515816   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.18     |
| episodes                | 83500    |
| lives                   | 83500    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 516468   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.19     |
| episodes                | 83600    |
| lives                   | 83600    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 7.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 517136   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.26     |
| episodes                | 83700    |
| lives                   | 83700    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 517816   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.19     |
| episodes                | 83800    |
| lives                   | 83800    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 7.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 518465   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.12     |
| episodes                | 83900    |
| lives                   | 83900    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 519135   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.12     |
| episodes                | 84000    |
| lives                   | 84000    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 519805   |
| value_loss              | 1.91     |
--------------------------------------
Saving model due to running mean reward increase: 7.2829 -> 7.4733
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.11     |
| episodes                | 84100    |
| lives                   | 84100    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 520472   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.17     |
| episodes                | 84200    |
| lives                   | 84200    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 7.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 521150   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.15     |
| episodes                | 84300    |
| lives                   | 84300    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 7.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 521834   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 2.13     |
| episodes                | 84400    |
| lives                   | 84400    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 7.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 522502   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.11     |
| episodes                | 84500    |
| lives                   | 84500    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 7.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 523189   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.07     |
| episodes                | 84600    |
| lives                   | 84600    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 523869   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.108   |
| entropy                 | 2.05     |
| episodes                | 84700    |
| lives                   | 84700    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 6.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0052   |
| steps                   | 524500   |
| value_loss              | 2.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0085  |
| entropy                 | 2.19     |
| episodes                | 84800    |
| lives                   | 84800    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 6.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 525159   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0219  |
| entropy                 | 2.19     |
| episodes                | 84900    |
| lives                   | 84900    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 7.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 525843   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0521  |
| entropy                 | 2.16     |
| episodes                | 85000    |
| lives                   | 85000    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 526528   |
| value_loss              | 2.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.14     |
| episodes                | 85100    |
| lives                   | 85100    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 7.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 527181   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0332  |
| entropy                 | 2.15     |
| episodes                | 85200    |
| lives                   | 85200    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 7.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 527842   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0095  |
| entropy                 | 2.19     |
| episodes                | 85300    |
| lives                   | 85300    |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0087   |
| steps                   | 528528   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0002   |
| entropy                 | 2.22     |
| episodes                | 85400    |
| lives                   | 85400    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 529195   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.2      |
| episodes                | 85500    |
| lives                   | 85500    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 7.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 529844   |
| value_loss              | 1.99     |
--------------------------------------
Saving model due to running mean reward increase: 7.303 -> 7.4708
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.25     |
| episodes                | 85600    |
| lives                   | 85600    |
| mean 100 episode length | 7.57     |
| mean 100 episode reward | 7.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 530501   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.23     |
| episodes                | 85700    |
| lives                   | 85700    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 7.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 531194   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.26     |
| episodes                | 85800    |
| lives                   | 85800    |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 7.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 531926   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.14     |
| episodes                | 85900    |
| lives                   | 85900    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 532582   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.16     |
| episodes                | 86000    |
| lives                   | 86000    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 7.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 533271   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.14     |
| episodes                | 86100    |
| lives                   | 86100    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 7.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 533978   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 2.17     |
| episodes                | 86200    |
| lives                   | 86200    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 7.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 534655   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 2.18     |
| episodes                | 86300    |
| lives                   | 86300    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 7.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 535310   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.14     |
| episodes                | 86400    |
| lives                   | 86400    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 535984   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.11     |
| episodes                | 86500    |
| lives                   | 86500    |
| mean 100 episode length | 7.19     |
| mean 100 episode reward | 7.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 536603   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.17     |
| episodes                | 86600    |
| lives                   | 86600    |
| mean 100 episode length | 7.55     |
| mean 100 episode reward | 7.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0107  |
| steps                   | 537258   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.2      |
| episodes                | 86700    |
| lives                   | 86700    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 7.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 537899   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.22     |
| episodes                | 86800    |
| lives                   | 86800    |
| mean 100 episode length | 7.36     |
| mean 100 episode reward | 7.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 538535   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 2.24     |
| episodes                | 86900    |
| lives                   | 86900    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 7.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 539209   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.21     |
| episodes                | 87000    |
| lives                   | 87000    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 539882   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 2.17     |
| episodes                | 87100    |
| lives                   | 87100    |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 7.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 540551   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.17     |
| episodes                | 87200    |
| lives                   | 87200    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0084  |
| steps                   | 541223   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.17     |
| episodes                | 87300    |
| lives                   | 87300    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 7.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 541876   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.21     |
| episodes                | 87400    |
| lives                   | 87400    |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 8.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 542590   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.2      |
| episodes                | 87500    |
| lives                   | 87500    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 7.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 543291   |
| value_loss              | 2.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 2.19     |
| episodes                | 87600    |
| lives                   | 87600    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 543993   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 2.14     |
| episodes                | 87700    |
| lives                   | 87700    |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 7.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 544699   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.17     |
| episodes                | 87800    |
| lives                   | 87800    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 545394   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.09     |
| episodes                | 87900    |
| lives                   | 87900    |
| mean 100 episode length | 7.49     |
| mean 100 episode reward | 7.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 546043   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.1      |
| episodes                | 88000    |
| lives                   | 88000    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 546697   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 2.09     |
| episodes                | 88100    |
| lives                   | 88100    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 7.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 547363   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 2.15     |
| episodes                | 88200    |
| lives                   | 88200    |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 7.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 548079   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.11     |
| episodes                | 88300    |
| lives                   | 88300    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 8.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 548771   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.06     |
| episodes                | 88400    |
| lives                   | 88400    |
| mean 100 episode length | 8        |
| mean 100 episode reward | 8.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 549471   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.05     |
| episodes                | 88500    |
| lives                   | 88500    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 550125   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0063  |
| entropy                 | 2.03     |
| episodes                | 88600    |
| lives                   | 88600    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 7.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 550804   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.02     |
| episodes                | 88700    |
| lives                   | 88700    |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 7.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 551465   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.09     |
| episodes                | 88800    |
| lives                   | 88800    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 7.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 552161   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.02     |
| episodes                | 88900    |
| lives                   | 88900    |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 552815   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.08     |
| episodes                | 89000    |
| lives                   | 89000    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 553471   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.006   |
| entropy                 | 2.06     |
| episodes                | 89100    |
| lives                   | 89100    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 7.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 554135   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.07     |
| episodes                | 89200    |
| lives                   | 89200    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 7.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 554787   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.09     |
| episodes                | 89300    |
| lives                   | 89300    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 7.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 555464   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.09     |
| episodes                | 89400    |
| lives                   | 89400    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 556166   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.15     |
| episodes                | 89500    |
| lives                   | 89500    |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 7.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 556881   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.23     |
| episodes                | 89600    |
| lives                   | 89600    |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 7.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 557603   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.17     |
| episodes                | 89700    |
| lives                   | 89700    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 558259   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.13     |
| episodes                | 89800    |
| lives                   | 89800    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 7.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0104  |
| steps                   | 558912   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.06     |
| episodes                | 89900    |
| lives                   | 89900    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 7.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 559557   |
| value_loss              | 1.88     |
--------------------------------------
Saving model due to mean reward increase: 7.5416 -> 7.5992
Saving model due to running mean reward increase: 7.3541 -> 7.5992
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.07     |
| episodes                | 90000    |
| lives                   | 90000    |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 7.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 560203   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.07     |
| episodes                | 90100    |
| lives                   | 90100    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 560866   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.09     |
| episodes                | 90200    |
| lives                   | 90200    |
| mean 100 episode length | 7.43     |
| mean 100 episode reward | 7.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 561509   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 2.09     |
| episodes                | 90300    |
| lives                   | 90300    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 562168   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.05     |
| episodes                | 90400    |
| lives                   | 90400    |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 7.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0095  |
| steps                   | 562840   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.12     |
| episodes                | 90500    |
| lives                   | 90500    |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 8.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 563546   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.05     |
| episodes                | 90600    |
| lives                   | 90600    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 564216   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.03     |
| episodes                | 90700    |
| lives                   | 90700    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 564886   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.1      |
| episodes                | 90800    |
| lives                   | 90800    |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 8.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 565596   |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.12     |
| episodes                | 90900    |
| lives                   | 90900    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 566260   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.11     |
| episodes                | 91000    |
| lives                   | 91000    |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 8.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 566967   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.06     |
| episodes                | 91100    |
| lives                   | 91100    |
| mean 100 episode length | 7.41     |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 567608   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.11     |
| episodes                | 91200    |
| lives                   | 91200    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 7.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 568279   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.12     |
| episodes                | 91300    |
| lives                   | 91300    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 568952   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.15     |
| episodes                | 91400    |
| lives                   | 91400    |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 7.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 569670   |
| value_loss              | 1.78     |
--------------------------------------
Saving model due to mean reward increase: 7.5992 -> 8.0971
Saving model due to running mean reward increase: 7.569 -> 8.0971
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.07     |
| episodes                | 91500    |
| lives                   | 91500    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 570347   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.02     |
| episodes                | 91600    |
| lives                   | 91600    |
| mean 100 episode length | 7.34     |
| mean 100 episode reward | 7.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 570981   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.02     |
| episodes                | 91700    |
| lives                   | 91700    |
| mean 100 episode length | 7.31     |
| mean 100 episode reward | 7.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 571612   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.07     |
| episodes                | 91800    |
| lives                   | 91800    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 572271   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.1      |
| episodes                | 91900    |
| lives                   | 91900    |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 7.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 572960   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.99     |
| episodes                | 92000    |
| lives                   | 92000    |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 7.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 573613   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.09     |
| episodes                | 92100    |
| lives                   | 92100    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 7.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 574291   |
| value_loss              | 2.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.08     |
| episodes                | 92200    |
| lives                   | 92200    |
| mean 100 episode length | 8        |
| mean 100 episode reward | 7.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 574991   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.09     |
| episodes                | 92300    |
| lives                   | 92300    |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 8.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 575714   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.08     |
| episodes                | 92400    |
| lives                   | 92400    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 576381   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.07     |
| episodes                | 92500    |
| lives                   | 92500    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 7.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 577060   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.04     |
| episodes                | 92600    |
| lives                   | 92600    |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 7.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 577724   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 2.1      |
| episodes                | 92700    |
| lives                   | 92700    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 578404   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.11     |
| episodes                | 92800    |
| lives                   | 92800    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 7.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 579096   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.13     |
| episodes                | 92900    |
| lives                   | 92900    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 579776   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 2.09     |
| episodes                | 93000    |
| lives                   | 93000    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 580456   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.06     |
| episodes                | 93100    |
| lives                   | 93100    |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 8.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 581151   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.09     |
| episodes                | 93200    |
| lives                   | 93200    |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 7.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 581842   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 2.14     |
| episodes                | 93300    |
| lives                   | 93300    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 7.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 582544   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.12     |
| episodes                | 93400    |
| lives                   | 93400    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 7.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0112  |
| steps                   | 583206   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2        |
| episodes                | 93500    |
| lives                   | 93500    |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 8.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 583869   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.09     |
| episodes                | 93600    |
| lives                   | 93600    |
| mean 100 episode length | 7.62     |
| mean 100 episode reward | 7.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 584531   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.03     |
| episodes                | 93700    |
| lives                   | 93700    |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 7.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 585202   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.14     |
| episodes                | 93800    |
| lives                   | 93800    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 585904   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2.11     |
| episodes                | 93900    |
| lives                   | 93900    |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 7.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 586597   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 2.13     |
| episodes                | 94000    |
| lives                   | 94000    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 587278   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.11     |
| episodes                | 94100    |
| lives                   | 94100    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 7.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 587953   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 2.2      |
| episodes                | 94200    |
| lives                   | 94200    |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 7.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 588695   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.12     |
| episodes                | 94300    |
| lives                   | 94300    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 7.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 589373   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.13     |
| episodes                | 94400    |
| lives                   | 94400    |
| mean 100 episode length | 7.7      |
| mean 100 episode reward | 7.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0114  |
| steps                   | 590043   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.05     |
| episodes                | 94500    |
| lives                   | 94500    |
| mean 100 episode length | 7.75     |
| mean 100 episode reward | 7.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 590718   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 2.09     |
| episodes                | 94600    |
| lives                   | 94600    |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 591399   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 2.07     |
| episodes                | 94700    |
| lives                   | 94700    |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 8.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 592104   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 2.12     |
| episodes                | 94800    |
| lives                   | 94800    |
| mean 100 episode length | 7.99     |
| mean 100 episode reward | 7.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 592803   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.05     |
| episodes                | 94900    |
| lives                   | 94900    |
| mean 100 episode length | 7.48     |
| mean 100 episode reward | 7.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 593451   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.05     |
| episodes                | 95000    |
| lives                   | 95000    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 7.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 594096   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.03     |
| episodes                | 95100    |
| lives                   | 95100    |
| mean 100 episode length | 7.38     |
| mean 100 episode reward | 7.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 594734   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 2.03     |
| episodes                | 95200    |
| lives                   | 95200    |
| mean 100 episode length | 7.56     |
| mean 100 episode reward | 7.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 595390   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.11     |
| episodes                | 95300    |
| lives                   | 95300    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 7.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 596084   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 2.08     |
| episodes                | 95400    |
| lives                   | 95400    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 7.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 596762   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.12     |
| episodes                | 95500    |
| lives                   | 95500    |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 597491   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0002   |
| entropy                 | 2.03     |
| episodes                | 95600    |
| lives                   | 95600    |
| mean 100 episode length | 7.66     |
| mean 100 episode reward | 8        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 598157   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.06     |
| episodes                | 95700    |
| lives                   | 95700    |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 7.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 598831   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0058  |
| entropy                 | 2.06     |
| episodes                | 95800    |
| lives                   | 95800    |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 7.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 599514   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.05     |
| episodes                | 95900    |
| lives                   | 95900    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 7.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 600210   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.06     |
| episodes                | 96000    |
| lives                   | 96000    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 600912   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.02     |
| episodes                | 96100    |
| lives                   | 96100    |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 8.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 601589   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.99     |
| episodes                | 96200    |
| lives                   | 96200    |
| mean 100 episode length | 7.51     |
| mean 100 episode reward | 7.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 602240   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.01     |
| episodes                | 96300    |
| lives                   | 96300    |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 7.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 602887   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 2.07     |
| episodes                | 96400    |
| lives                   | 96400    |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 7.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 603567   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 2.17     |
| episodes                | 96500    |
| lives                   | 96500    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 7.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 604254   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.08     |
| episodes                | 96600    |
| lives                   | 96600    |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 7.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 604906   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0078  |
| entropy                 | 2.14     |
| episodes                | 96700    |
| lives                   | 96700    |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 7.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 605602   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.05     |
| episodes                | 96800    |
| lives                   | 96800    |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 7.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 606286   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 2.11     |
| episodes                | 96900    |
| lives                   | 96900    |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 7.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 606978   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.11     |
| episodes                | 97000    |
| lives                   | 97000    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 7.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 607672   |
| value_loss              | 2.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 2.07     |
| episodes                | 97100    |
| lives                   | 97100    |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 7.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 608374   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.98     |
| episodes                | 97200    |
| lives                   | 97200    |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 7.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 609042   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.98     |
| episodes                | 97300    |
| lives                   | 97300    |
| mean 100 episode length | 7.58     |
| mean 100 episode reward | 7.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 609700   |
| value_loss              | 1.57     |
--------------------------------------
Saving model due to running mean reward increase: 7.6927 -> 7.919
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 2.06     |
| episodes                | 97400    |
| lives                   | 97400    |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 7.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 610373   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 2.07     |
| episodes                | 97500    |
| lives                   | 97500    |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 7.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 611059   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.1      |
| episodes                | 97600    |
| lives                   | 97600    |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 8.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 611757   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 2.14     |
| episodes                | 97700    |
| lives                   | 97700    |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 8.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 612489   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0008   |
| entropy                 | 2.13     |
| episodes                | 97800    |
| lives                   | 97800    |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 7.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 613209   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.09     |
| episodes                | 97900    |
| lives                   | 97900    |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 613933   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 2.07     |
| episodes                | 98000    |
| lives                   | 98000    |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 8.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 614650   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.06     |
| episodes                | 98100    |
| lives                   | 98100    |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 8.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 615344   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 2.07     |
| episodes                | 98200    |
| lives                   | 98200    |
| mean 100 episode length | 7.87     |
| mean 100 episode reward | 7.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 616031   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2        |
| episodes                | 98300    |
| lives                   | 98300    |
| mean 100 episode length | 7.45     |
| mean 100 episode reward | 7.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 616676   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.97     |
| episodes                | 98400    |
| lives                   | 98400    |
| mean 100 episode length | 7.33     |
| mean 100 episode reward | 7.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 617309   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 2.04     |
| episodes                | 98500    |
| lives                   | 98500    |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 8.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 618034   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.11     |
| episodes                | 98600    |
| lives                   | 98600    |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 7.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 618713   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.05     |
| episodes                | 98700    |
| lives                   | 98700    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 619380   |
| value_loss              | 1.95     |
--------------------------------------
Saving model due to running mean reward increase: 7.2058 -> 7.988
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.14     |
| episodes                | 98800    |
| lives                   | 98800    |
| mean 100 episode length | 8.12     |
| mean 100 episode reward | 7.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 620092   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 2.05     |
| episodes                | 98900    |
| lives                   | 98900    |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 7.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 620770   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2.04     |
| episodes                | 99000    |
| lives                   | 99000    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 8.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 621455   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 2.15     |
| episodes                | 99100    |
| lives                   | 99100    |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 8.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 622182   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 2.05     |
| episodes                | 99200    |
| lives                   | 99200    |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 7.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 622867   |
| value_loss              | 2.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 2.04     |
| episodes                | 99300    |
| lives                   | 99300    |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 8.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 623575   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0058  |
| entropy                 | 2.07     |
| episodes                | 99400    |
| lives                   | 99400    |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 8.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0093  |
| steps                   | 624276   |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 2.02     |
| episodes                | 99500    |
| lives                   | 99500    |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 624943   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.03     |
| episodes                | 99600    |
| lives                   | 99600    |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 625602   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 2.08     |
| episodes                | 99700    |
| lives                   | 99700    |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 7.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 626284   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 2.14     |
| episodes                | 99800    |
| lives                   | 99800    |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 627013   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 2.07     |
| episodes                | 99900    |
| lives                   | 99900    |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 7.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 627724   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.02     |
| episodes                | 100000   |
| lives                   | 100000   |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 7.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 628406   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.99     |
| episodes                | 100100   |
| lives                   | 100100   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 8.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 629098   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 2.01     |
| episodes                | 100200   |
| lives                   | 100200   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 7.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 629804   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 2        |
| episodes                | 100300   |
| lives                   | 100300   |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 7.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 630486   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.99     |
| episodes                | 100400   |
| lives                   | 100400   |
| mean 100 episode length | 7.61     |
| mean 100 episode reward | 8.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 631147   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.98     |
| episodes                | 100500   |
| lives                   | 100500   |
| mean 100 episode length | 7.46     |
| mean 100 episode reward | 7.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 631793   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.99     |
| episodes                | 100600   |
| lives                   | 100600   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 632452   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.05     |
| episodes                | 100700   |
| lives                   | 100700   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 633173   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.98     |
| episodes                | 100800   |
| lives                   | 100800   |
| mean 100 episode length | 7.65     |
| mean 100 episode reward | 7.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 633838   |
| value_loss              | 2.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 2.03     |
| episodes                | 100900   |
| lives                   | 100900   |
| mean 100 episode length | 7.74     |
| mean 100 episode reward | 7.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 634512   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 2.05     |
| episodes                | 101000   |
| lives                   | 101000   |
| mean 100 episode length | 7.77     |
| mean 100 episode reward | 7.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 635189   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0053  |
| entropy                 | 1.96     |
| episodes                | 101100   |
| lives                   | 101100   |
| mean 100 episode length | 7.54     |
| mean 100 episode reward | 7.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 635843   |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 2.03     |
| episodes                | 101200   |
| lives                   | 101200   |
| mean 100 episode length | 7.67     |
| mean 100 episode reward | 7.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 636510   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.01     |
| episodes                | 101300   |
| lives                   | 101300   |
| mean 100 episode length | 7.81     |
| mean 100 episode reward | 7.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 637191   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.92     |
| episodes                | 101400   |
| lives                   | 101400   |
| mean 100 episode length | 7.47     |
| mean 100 episode reward | 7.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 637838   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.92     |
| episodes                | 101500   |
| lives                   | 101500   |
| mean 100 episode length | 7.71     |
| mean 100 episode reward | 8.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 638509   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.88     |
| episodes                | 101600   |
| lives                   | 101600   |
| mean 100 episode length | 7.6      |
| mean 100 episode reward | 8        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 639169   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.91     |
| episodes                | 101700   |
| lives                   | 101700   |
| mean 100 episode length | 7.52     |
| mean 100 episode reward | 7.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 639821   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 2        |
| episodes                | 101800   |
| lives                   | 101800   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 7.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 640513   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 2.02     |
| episodes                | 101900   |
| lives                   | 101900   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 8.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 641238   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 2.02     |
| episodes                | 102000   |
| lives                   | 102000   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 7.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0098  |
| steps                   | 641942   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.89     |
| episodes                | 102100   |
| lives                   | 102100   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 8.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 642625   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.95     |
| episodes                | 102200   |
| lives                   | 102200   |
| mean 100 episode length | 7.92     |
| mean 100 episode reward | 7.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 643317   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 2.01     |
| episodes                | 102300   |
| lives                   | 102300   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 8.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 644039   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.96     |
| episodes                | 102400   |
| lives                   | 102400   |
| mean 100 episode length | 7.88     |
| mean 100 episode reward | 8.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 644727   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 2.02     |
| episodes                | 102500   |
| lives                   | 102500   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 7.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 645421   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 2.01     |
| episodes                | 102600   |
| lives                   | 102600   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 8.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 646128   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 2.02     |
| episodes                | 102700   |
| lives                   | 102700   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 8.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 646842   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 2.01     |
| episodes                | 102800   |
| lives                   | 102800   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 647573   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.99     |
| episodes                | 102900   |
| lives                   | 102900   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 8.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 648300   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.92     |
| episodes                | 103000   |
| lives                   | 103000   |
| mean 100 episode length | 7.78     |
| mean 100 episode reward | 8.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 648978   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.9      |
| episodes                | 103100   |
| lives                   | 103100   |
| mean 100 episode length | 7.64     |
| mean 100 episode reward | 8.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 649642   |
| value_loss              | 1.42     |
--------------------------------------
Saving model due to mean reward increase: 8.0971 -> 8.2355
Saving model due to running mean reward increase: 7.991 -> 8.2355
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 2.01     |
| episodes                | 103200   |
| lives                   | 103200   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 8.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 650357   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.91     |
| episodes                | 103300   |
| lives                   | 103300   |
| mean 100 episode length | 7.76     |
| mean 100 episode reward | 8.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 651033   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.94     |
| episodes                | 103400   |
| lives                   | 103400   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 8.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 651741   |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.94     |
| episodes                | 103500   |
| lives                   | 103500   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 8.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 652432   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 2.03     |
| episodes                | 103600   |
| lives                   | 103600   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 653134   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0001   |
| entropy                 | 1.93     |
| episodes                | 103700   |
| lives                   | 103700   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 7.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 653820   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0052  |
| entropy                 | 1.86     |
| episodes                | 103800   |
| lives                   | 103800   |
| mean 100 episode length | 7.63     |
| mean 100 episode reward | 8.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 654483   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.92     |
| episodes                | 103900   |
| lives                   | 103900   |
| mean 100 episode length | 7.8      |
| mean 100 episode reward | 8.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 655163   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.94     |
| episodes                | 104000   |
| lives                   | 104000   |
| mean 100 episode length | 7.83     |
| mean 100 episode reward | 8.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 655846   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0054  |
| entropy                 | 1.94     |
| episodes                | 104100   |
| lives                   | 104100   |
| mean 100 episode length | 7.73     |
| mean 100 episode reward | 8.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 656519   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.97     |
| episodes                | 104200   |
| lives                   | 104200   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 657230   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.92     |
| episodes                | 104300   |
| lives                   | 104300   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 8.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0094  |
| steps                   | 657924   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.82     |
| episodes                | 104400   |
| lives                   | 104400   |
| mean 100 episode length | 7.53     |
| mean 100 episode reward | 8.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 658577   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.89     |
| episodes                | 104500   |
| lives                   | 104500   |
| mean 100 episode length | 7.82     |
| mean 100 episode reward | 8.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 659259   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.83     |
| episodes                | 104600   |
| lives                   | 104600   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 8.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 659943   |
| value_loss              | 1.77     |
--------------------------------------
Saving model due to running mean reward increase: 7.9211 -> 8.1176
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.85     |
| episodes                | 104700   |
| lives                   | 104700   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 8.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 660628   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.92     |
| episodes                | 104800   |
| lives                   | 104800   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 8.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 661350   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.91     |
| episodes                | 104900   |
| lives                   | 104900   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 8.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 662083   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.95     |
| episodes                | 105000   |
| lives                   | 105000   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 662848   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.97     |
| episodes                | 105100   |
| lives                   | 105100   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 663611   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.96     |
| episodes                | 105200   |
| lives                   | 105200   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 664345   |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.9      |
| episodes                | 105300   |
| lives                   | 105300   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 8.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 665048   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.92     |
| episodes                | 105400   |
| lives                   | 105400   |
| mean 100 episode length | 7.95     |
| mean 100 episode reward | 7.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 665743   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.93     |
| episodes                | 105500   |
| lives                   | 105500   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 8.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 666463   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.91     |
| episodes                | 105600   |
| lives                   | 105600   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 8.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 667190   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.85     |
| episodes                | 105700   |
| lives                   | 105700   |
| mean 100 episode length | 7.79     |
| mean 100 episode reward | 7.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 667869   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.9      |
| episodes                | 105800   |
| lives                   | 105800   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 8.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 668584   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.9      |
| episodes                | 105900   |
| lives                   | 105900   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 669295   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.83     |
| episodes                | 106000   |
| lives                   | 106000   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 8.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 669986   |
| value_loss              | 1.5      |
--------------------------------------
Saving model due to mean reward increase: 8.2355 -> 8.5514
Saving model due to running mean reward increase: 8.2506 -> 8.5514
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.81     |
| episodes                | 106100   |
| lives                   | 106100   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 8.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 670676   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 1.92     |
| episodes                | 106200   |
| lives                   | 106200   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 8.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 671394   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0068  |
| entropy                 | 1.91     |
| episodes                | 106300   |
| lives                   | 106300   |
| mean 100 episode length | 7.94     |
| mean 100 episode reward | 8.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 672088   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.9      |
| episodes                | 106400   |
| lives                   | 106400   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 672824   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.92     |
| episodes                | 106500   |
| lives                   | 106500   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 8.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 673546   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.92     |
| episodes                | 106600   |
| lives                   | 106600   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 7.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 674250   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.89     |
| episodes                | 106700   |
| lives                   | 106700   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 8.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 674963   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0001   |
| entropy                 | 1.88     |
| episodes                | 106800   |
| lives                   | 106800   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 8.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 675679   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.93     |
| episodes                | 106900   |
| lives                   | 106900   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 8.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 676407   |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.88     |
| episodes                | 107000   |
| lives                   | 107000   |
| mean 100 episode length | 7.86     |
| mean 100 episode reward | 8.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 677093   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.82     |
| episodes                | 107100   |
| lives                   | 107100   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 8.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 677783   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.83     |
| episodes                | 107200   |
| lives                   | 107200   |
| mean 100 episode length | 7.72     |
| mean 100 episode reward | 8.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 678455   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.83     |
| episodes                | 107300   |
| lives                   | 107300   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 8.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 679162   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.89     |
| episodes                | 107400   |
| lives                   | 107400   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 8.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 679890   |
| value_loss              | 1.87     |
--------------------------------------
Saving model due to mean reward increase: 8.5514 -> 8.7973
Saving model due to running mean reward increase: 8.5247 -> 8.7973
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.85     |
| episodes                | 107500   |
| lives                   | 107500   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 8.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 680579   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.77     |
| episodes                | 107600   |
| lives                   | 107600   |
| mean 100 episode length | 7.68     |
| mean 100 episode reward | 8.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 681247   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.83     |
| episodes                | 107700   |
| lives                   | 107700   |
| mean 100 episode length | 7.59     |
| mean 100 episode reward | 7.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 681906   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.97     |
| episodes                | 107800   |
| lives                   | 107800   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 8        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0118  |
| steps                   | 682606   |
| value_loss              | 2.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.93     |
| episodes                | 107900   |
| lives                   | 107900   |
| mean 100 episode length | 7.84     |
| mean 100 episode reward | 8.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 683290   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.92     |
| episodes                | 108000   |
| lives                   | 108000   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 8.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 683997   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.97     |
| episodes                | 108100   |
| lives                   | 108100   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 8.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 684741   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.99     |
| episodes                | 108200   |
| lives                   | 108200   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 8.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 685498   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.94     |
| episodes                | 108300   |
| lives                   | 108300   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 8.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 686221   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.97     |
| episodes                | 108400   |
| lives                   | 108400   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 8.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 686992   |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 2.02     |
| episodes                | 108500   |
| lives                   | 108500   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 8.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 687756   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.96     |
| episodes                | 108600   |
| lives                   | 108600   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 8.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 688500   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.89     |
| episodes                | 108700   |
| lives                   | 108700   |
| mean 100 episode length | 7.98     |
| mean 100 episode reward | 8.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 689198   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.91     |
| episodes                | 108800   |
| lives                   | 108800   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 8.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 689918   |
| value_loss              | 1.59     |
--------------------------------------
Saving model due to running mean reward increase: 8.1654 -> 8.6568
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.9      |
| episodes                | 108900   |
| lives                   | 108900   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 8.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 690670   |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.91     |
| episodes                | 109000   |
| lives                   | 109000   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 8.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 691398   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0057  |
| entropy                 | 2.01     |
| episodes                | 109100   |
| lives                   | 109100   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 8.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 692160   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.95     |
| episodes                | 109200   |
| lives                   | 109200   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 8.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 692900   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.87     |
| episodes                | 109300   |
| lives                   | 109300   |
| mean 100 episode length | 7.69     |
| mean 100 episode reward | 7.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 693569   |
| value_loss              | 2.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.97     |
| episodes                | 109400   |
| lives                   | 109400   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 8.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0088  |
| steps                   | 694307   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.95     |
| episodes                | 109500   |
| lives                   | 109500   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 8.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 695016   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.95     |
| episodes                | 109600   |
| lives                   | 109600   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 8.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 695730   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.89     |
| episodes                | 109700   |
| lives                   | 109700   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 8.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 696438   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.97     |
| episodes                | 109800   |
| lives                   | 109800   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 8.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 697190   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.89     |
| episodes                | 109900   |
| lives                   | 109900   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 697892   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.88     |
| episodes                | 110000   |
| lives                   | 110000   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 8.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 698592   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.85     |
| episodes                | 110100   |
| lives                   | 110100   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 8.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 699288   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.84     |
| episodes                | 110200   |
| lives                   | 110200   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 8.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 699991   |
| value_loss              | 1.65     |
--------------------------------------
Saving model due to running mean reward increase: 8.3319 -> 8.5485
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.94     |
| episodes                | 110300   |
| lives                   | 110300   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 700741   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.88     |
| episodes                | 110400   |
| lives                   | 110400   |
| mean 100 episode length | 7.93     |
| mean 100 episode reward | 8.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 701434   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.86     |
| episodes                | 110500   |
| lives                   | 110500   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 8.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 702124   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.86     |
| episodes                | 110600   |
| lives                   | 110600   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 8.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0102  |
| steps                   | 702833   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.88     |
| episodes                | 110700   |
| lives                   | 110700   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 8.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 703591   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.87     |
| episodes                | 110800   |
| lives                   | 110800   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 8.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 704323   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.84     |
| episodes                | 110900   |
| lives                   | 110900   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 8.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0091  |
| steps                   | 705055   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.81     |
| episodes                | 111000   |
| lives                   | 111000   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 705766   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.8      |
| episodes                | 111100   |
| lives                   | 111100   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 8.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 706480   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.82     |
| episodes                | 111200   |
| lives                   | 111200   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 8.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 707210   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.75     |
| episodes                | 111300   |
| lives                   | 111300   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 8.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 707911   |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.81     |
| episodes                | 111400   |
| lives                   | 111400   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 8.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 708650   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.8      |
| episodes                | 111500   |
| lives                   | 111500   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 8.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 709364   |
| value_loss              | 1.77     |
--------------------------------------
Saving model due to mean reward increase: 8.7973 -> 9.0485
Saving model due to running mean reward increase: 8.6975 -> 9.0485
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.89     |
| episodes                | 111600   |
| lives                   | 111600   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 710133   |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.9      |
| episodes                | 111700   |
| lives                   | 111700   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 710909   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.88     |
| episodes                | 111800   |
| lives                   | 111800   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 711661   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.87     |
| episodes                | 111900   |
| lives                   | 111900   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 8.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 712424   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.83     |
| episodes                | 112000   |
| lives                   | 112000   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 8.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 713141   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.86     |
| episodes                | 112100   |
| lives                   | 112100   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 8.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 713887   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.82     |
| episodes                | 112200   |
| lives                   | 112200   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 8.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 714629   |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.78     |
| episodes                | 112300   |
| lives                   | 112300   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 715358   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.77     |
| episodes                | 112400   |
| lives                   | 112400   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 716076   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.79     |
| episodes                | 112500   |
| lives                   | 112500   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 716787   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0066  |
| entropy                 | 1.77     |
| episodes                | 112600   |
| lives                   | 112600   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 8.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 717514   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.76     |
| episodes                | 112700   |
| lives                   | 112700   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 8.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 718233   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.8      |
| episodes                | 112800   |
| lives                   | 112800   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 718970   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.74     |
| episodes                | 112900   |
| lives                   | 112900   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 8.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 719675   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.75     |
| episodes                | 113000   |
| lives                   | 113000   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 8.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 720397   |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.77     |
| episodes                | 113100   |
| lives                   | 113100   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 721141   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.78     |
| episodes                | 113200   |
| lives                   | 113200   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 8.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 721882   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.82     |
| episodes                | 113300   |
| lives                   | 113300   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 8.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 722619   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.77     |
| episodes                | 113400   |
| lives                   | 113400   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 8.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 723355   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.006   |
| entropy                 | 1.87     |
| episodes                | 113500   |
| lives                   | 113500   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 724130   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.81     |
| episodes                | 113600   |
| lives                   | 113600   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 8.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 724862   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.84     |
| episodes                | 113700   |
| lives                   | 113700   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 8.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 725632   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0078  |
| entropy                 | 1.8      |
| episodes                | 113800   |
| lives                   | 113800   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 8.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 726378   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.7      |
| episodes                | 113900   |
| lives                   | 113900   |
| mean 100 episode length | 7.89     |
| mean 100 episode reward | 8.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 727067   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 1.79     |
| episodes                | 114000   |
| lives                   | 114000   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 8.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 727816   |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.82     |
| episodes                | 114100   |
| lives                   | 114100   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 728576   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.74     |
| episodes                | 114200   |
| lives                   | 114200   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 729301   |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.68     |
| episodes                | 114300   |
| lives                   | 114300   |
| mean 100 episode length | 7.97     |
| mean 100 episode reward | 8.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 729998   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.81     |
| episodes                | 114400   |
| lives                   | 114400   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 730749   |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.74     |
| episodes                | 114500   |
| lives                   | 114500   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 731497   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.77     |
| episodes                | 114600   |
| lives                   | 114600   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 732231   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.68     |
| episodes                | 114700   |
| lives                   | 114700   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 732952   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.79     |
| episodes                | 114800   |
| lives                   | 114800   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 733707   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.76     |
| episodes                | 114900   |
| lives                   | 114900   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 8.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 734445   |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.8      |
| episodes                | 115000   |
| lives                   | 115000   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 8.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 735184   |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.74     |
| episodes                | 115100   |
| lives                   | 115100   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 8.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 735929   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.74     |
| episodes                | 115200   |
| lives                   | 115200   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 8.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 736669   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.74     |
| episodes                | 115300   |
| lives                   | 115300   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 737393   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.74     |
| episodes                | 115400   |
| lives                   | 115400   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 9        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 738134   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.73     |
| episodes                | 115500   |
| lives                   | 115500   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 8.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 738851   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 1.74     |
| episodes                | 115600   |
| lives                   | 115600   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0087  |
| steps                   | 739604   |
| value_loss              | 1.43     |
--------------------------------------
Saving model due to running mean reward increase: 8.8401 -> 9.0364
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.72     |
| episodes                | 115700   |
| lives                   | 115700   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 740355   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.68     |
| episodes                | 115800   |
| lives                   | 115800   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 741094   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0052  |
| entropy                 | 1.66     |
| episodes                | 115900   |
| lives                   | 115900   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 8.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 741809   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0083  |
| entropy                 | 1.66     |
| episodes                | 116000   |
| lives                   | 116000   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 8.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 742518   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.64     |
| episodes                | 116100   |
| lives                   | 116100   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 8.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 743208   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.69     |
| episodes                | 116200   |
| lives                   | 116200   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 8.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 743934   |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.64     |
| episodes                | 116300   |
| lives                   | 116300   |
| mean 100 episode length | 8.04     |
| mean 100 episode reward | 8.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 744638   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.75     |
| episodes                | 116400   |
| lives                   | 116400   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 8.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 745397   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.71     |
| episodes                | 116500   |
| lives                   | 116500   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 746126   |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.79     |
| episodes                | 116600   |
| lives                   | 116600   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 8.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 746885   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0074  |
| entropy                 | 1.7      |
| episodes                | 116700   |
| lives                   | 116700   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 747612   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0101  |
| entropy                 | 1.76     |
| episodes                | 116800   |
| lives                   | 116800   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 748314   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0108  |
| entropy                 | 1.73     |
| episodes                | 116900   |
| lives                   | 116900   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 8.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0111  |
| steps                   | 749056   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.79     |
| episodes                | 117000   |
| lives                   | 117000   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 749794   |
| value_loss              | 1.62     |
--------------------------------------
Saving model due to running mean reward increase: 8.6873 -> 8.8122
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.76     |
| episodes                | 117100   |
| lives                   | 117100   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 8.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 750547   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.75     |
| episodes                | 117200   |
| lives                   | 117200   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 751330   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.72     |
| episodes                | 117300   |
| lives                   | 117300   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 752054   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.73     |
| episodes                | 117400   |
| lives                   | 117400   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 8.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 752787   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.76     |
| episodes                | 117500   |
| lives                   | 117500   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 753533   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.73     |
| episodes                | 117600   |
| lives                   | 117600   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 8.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 754256   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 1.84     |
| episodes                | 117700   |
| lives                   | 117700   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 8.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 755055   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.74     |
| episodes                | 117800   |
| lives                   | 117800   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 755819   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.71     |
| episodes                | 117900   |
| lives                   | 117900   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 756548   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.7      |
| episodes                | 118000   |
| lives                   | 118000   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 757308   |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.63     |
| episodes                | 118100   |
| lives                   | 118100   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 758027   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.75     |
| episodes                | 118200   |
| lives                   | 118200   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 758805   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.64     |
| episodes                | 118300   |
| lives                   | 118300   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 759545   |
| value_loss              | 1.64     |
--------------------------------------
Saving model due to mean reward increase: 9.0485 -> 9.2474
Saving model due to running mean reward increase: 8.8569 -> 9.2474
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.76     |
| episodes                | 118400   |
| lives                   | 118400   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 760321   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.62     |
| episodes                | 118500   |
| lives                   | 118500   |
| mean 100 episode length | 8.07     |
| mean 100 episode reward | 8.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 761028   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.68     |
| episodes                | 118600   |
| lives                   | 118600   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 761788   |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.7      |
| episodes                | 118700   |
| lives                   | 118700   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 8.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 762527   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.71     |
| episodes                | 118800   |
| lives                   | 118800   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 763305   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.67     |
| episodes                | 118900   |
| lives                   | 118900   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 8.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 764047   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.64     |
| episodes                | 119000   |
| lives                   | 119000   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 764771   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.65     |
| episodes                | 119100   |
| lives                   | 119100   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 765500   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.66     |
| episodes                | 119200   |
| lives                   | 119200   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 766246   |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.72     |
| episodes                | 119300   |
| lives                   | 119300   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 767010   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.71     |
| episodes                | 119400   |
| lives                   | 119400   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 767764   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.63     |
| episodes                | 119500   |
| lives                   | 119500   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 8.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 768473   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.6      |
| episodes                | 119600   |
| lives                   | 119600   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 9.14     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 769202   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0066  |
| entropy                 | 1.68     |
| episodes                | 119700   |
| lives                   | 119700   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 8.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0097  |
| steps                   | 769920   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.66     |
| episodes                | 119800   |
| lives                   | 119800   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 770657   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.74     |
| episodes                | 119900   |
| lives                   | 119900   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 771415   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.73     |
| episodes                | 120000   |
| lives                   | 120000   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 772177   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.74     |
| episodes                | 120100   |
| lives                   | 120100   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 772969   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.68     |
| episodes                | 120200   |
| lives                   | 120200   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 8.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 773723   |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.68     |
| episodes                | 120300   |
| lives                   | 120300   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 774454   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.73     |
| episodes                | 120400   |
| lives                   | 120400   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 775215   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.74     |
| episodes                | 120500   |
| lives                   | 120500   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 775978   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 1.76     |
| episodes                | 120600   |
| lives                   | 120600   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 9.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 776783   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.76     |
| episodes                | 120700   |
| lives                   | 120700   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 777561   |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.7      |
| episodes                | 120800   |
| lives                   | 120800   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 778330   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.73     |
| episodes                | 120900   |
| lives                   | 120900   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 779102   |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.74     |
| episodes                | 121000   |
| lives                   | 121000   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 779873   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.75     |
| episodes                | 121100   |
| lives                   | 121100   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 780677   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.69     |
| episodes                | 121200   |
| lives                   | 121200   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 781428   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.64     |
| episodes                | 121300   |
| lives                   | 121300   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 9.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 782161   |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.73     |
| episodes                | 121400   |
| lives                   | 121400   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0101  |
| steps                   | 782927   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.74     |
| episodes                | 121500   |
| lives                   | 121500   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 8.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 783683   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.72     |
| episodes                | 121600   |
| lives                   | 121600   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 784443   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.65     |
| episodes                | 121700   |
| lives                   | 121700   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 8.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 785168   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.74     |
| episodes                | 121800   |
| lives                   | 121800   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 9.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 785927   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.74     |
| episodes                | 121900   |
| lives                   | 121900   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 786680   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.75     |
| episodes                | 122000   |
| lives                   | 122000   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 787436   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.74     |
| episodes                | 122100   |
| lives                   | 122100   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 788176   |
| value_loss              | 2.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.78     |
| episodes                | 122200   |
| lives                   | 122200   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 788951   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.72     |
| episodes                | 122300   |
| lives                   | 122300   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 8.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 789694   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.61     |
| episodes                | 122400   |
| lives                   | 122400   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 790427   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.63     |
| episodes                | 122500   |
| lives                   | 122500   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 791129   |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.66     |
| episodes                | 122600   |
| lives                   | 122600   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 8.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 791862   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.72     |
| episodes                | 122700   |
| lives                   | 122700   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 9.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 792625   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.66     |
| episodes                | 122800   |
| lives                   | 122800   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 793359   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.67     |
| episodes                | 122900   |
| lives                   | 122900   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 8.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 794102   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.76     |
| episodes                | 123000   |
| lives                   | 123000   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0083  |
| steps                   | 794874   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.75     |
| episodes                | 123100   |
| lives                   | 123100   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 8.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 795624   |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.69     |
| episodes                | 123200   |
| lives                   | 123200   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 796363   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0061  |
| entropy                 | 1.72     |
| episodes                | 123300   |
| lives                   | 123300   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 797121   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.72     |
| episodes                | 123400   |
| lives                   | 123400   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 8.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 797866   |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.7      |
| episodes                | 123500   |
| lives                   | 123500   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 798587   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.74     |
| episodes                | 123600   |
| lives                   | 123600   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 799322   |
| value_loss              | 1.58     |
--------------------------------------
Saving model due to mean reward increase: 9.2474 -> 9.3959
Saving model due to running mean reward increase: 8.8504 -> 9.3959
--------------------------------------
| approx_kl               | -0.0052  |
| entropy                 | 1.77     |
| episodes                | 123700   |
| lives                   | 123700   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 800090   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.78     |
| episodes                | 123800   |
| lives                   | 123800   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 800832   |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 1.67     |
| episodes                | 123900   |
| lives                   | 123900   |
| mean 100 episode length | 8.23     |
| mean 100 episode reward | 8.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 801555   |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.69     |
| episodes                | 124000   |
| lives                   | 124000   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 802286   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.67     |
| episodes                | 124100   |
| lives                   | 124100   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 803023   |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.76     |
| episodes                | 124200   |
| lives                   | 124200   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 803770   |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.64     |
| episodes                | 124300   |
| lives                   | 124300   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 804492   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.72     |
| episodes                | 124400   |
| lives                   | 124400   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 805245   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.77     |
| episodes                | 124500   |
| lives                   | 124500   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 806008   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.79     |
| episodes                | 124600   |
| lives                   | 124600   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 9.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 806752   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.68     |
| episodes                | 124700   |
| lives                   | 124700   |
| mean 100 episode length | 7.85     |
| mean 100 episode reward | 8.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 807437   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.7      |
| episodes                | 124800   |
| lives                   | 124800   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 8.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 808162   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.7      |
| episodes                | 124900   |
| lives                   | 124900   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 8.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 808871   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.78     |
| episodes                | 125000   |
| lives                   | 125000   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 809630   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.75     |
| episodes                | 125100   |
| lives                   | 125100   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 810400   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.75     |
| episodes                | 125200   |
| lives                   | 125200   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 8.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0099  |
| steps                   | 811136   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.67     |
| episodes                | 125300   |
| lives                   | 125300   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 811864   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.72     |
| episodes                | 125400   |
| lives                   | 125400   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 812600   |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.71     |
| episodes                | 125500   |
| lives                   | 125500   |
| mean 100 episode length | 7.9      |
| mean 100 episode reward | 8.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 813290   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.82     |
| episodes                | 125600   |
| lives                   | 125600   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 814021   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.71     |
| episodes                | 125700   |
| lives                   | 125700   |
| mean 100 episode length | 8        |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 814721   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.75     |
| episodes                | 125800   |
| lives                   | 125800   |
| mean 100 episode length | 8.02     |
| mean 100 episode reward | 8.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 815423   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.75     |
| episodes                | 125900   |
| lives                   | 125900   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 8.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 816162   |
| value_loss              | 2.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.75     |
| episodes                | 126000   |
| lives                   | 126000   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 816898   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0075  |
| entropy                 | 1.74     |
| episodes                | 126100   |
| lives                   | 126100   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 817653   |
| value_loss              | 2.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 1.74     |
| episodes                | 126200   |
| lives                   | 126200   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 8.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 818399   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.76     |
| episodes                | 126300   |
| lives                   | 126300   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 819151   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.78     |
| episodes                | 126400   |
| lives                   | 126400   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 8.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 819914   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.82     |
| episodes                | 126500   |
| lives                   | 126500   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 820693   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.71     |
| episodes                | 126600   |
| lives                   | 126600   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 821404   |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.77     |
| episodes                | 126700   |
| lives                   | 126700   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 822138   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.82     |
| episodes                | 126800   |
| lives                   | 126800   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 8.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 822899   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0053  |
| entropy                 | 1.82     |
| episodes                | 126900   |
| lives                   | 126900   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 823669   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.81     |
| episodes                | 127000   |
| lives                   | 127000   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 824421   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.77     |
| episodes                | 127100   |
| lives                   | 127100   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 9.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 825180   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.79     |
| episodes                | 127200   |
| lives                   | 127200   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0085  |
| steps                   | 825945   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.77     |
| episodes                | 127300   |
| lives                   | 127300   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 826689   |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.76     |
| episodes                | 127400   |
| lives                   | 127400   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 8.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 827407   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.84     |
| episodes                | 127500   |
| lives                   | 127500   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 828155   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.78     |
| episodes                | 127600   |
| lives                   | 127600   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 828886   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0064  |
| entropy                 | 1.74     |
| episodes                | 127700   |
| lives                   | 127700   |
| mean 100 episode length | 8.09     |
| mean 100 episode reward | 8.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0092  |
| steps                   | 829595   |
| value_loss              | 1.88     |
--------------------------------------
Saving model due to running mean reward increase: 8.576 -> 8.6746
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.74     |
| episodes                | 127800   |
| lives                   | 127800   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 830326   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.83     |
| episodes                | 127900   |
| lives                   | 127900   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 831074   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0072  |
| entropy                 | 1.73     |
| episodes                | 128000   |
| lives                   | 128000   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 831805   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.78     |
| episodes                | 128100   |
| lives                   | 128100   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 832556   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0058  |
| entropy                 | 1.77     |
| episodes                | 128200   |
| lives                   | 128200   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 8.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 833291   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.73     |
| episodes                | 128300   |
| lives                   | 128300   |
| mean 100 episode length | 8.08     |
| mean 100 episode reward | 8.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 833999   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.8      |
| episodes                | 128400   |
| lives                   | 128400   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 834756   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.77     |
| episodes                | 128500   |
| lives                   | 128500   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 835480   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.77     |
| episodes                | 128600   |
| lives                   | 128600   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 836212   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.76     |
| episodes                | 128700   |
| lives                   | 128700   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 836947   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.79     |
| episodes                | 128800   |
| lives                   | 128800   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 837690   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.73     |
| episodes                | 128900   |
| lives                   | 128900   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 838401   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.74     |
| episodes                | 129000   |
| lives                   | 129000   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 8.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 839111   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.71     |
| episodes                | 129100   |
| lives                   | 129100   |
| mean 100 episode length | 8.11     |
| mean 100 episode reward | 8.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 839822   |
| value_loss              | 1.71     |
--------------------------------------
Saving model due to running mean reward increase: 8.5991 -> 8.9522
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.81     |
| episodes                | 129200   |
| lives                   | 129200   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 9.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 840589   |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.75     |
| episodes                | 129300   |
| lives                   | 129300   |
| mean 100 episode length | 8.22     |
| mean 100 episode reward | 8.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 841311   |
| value_loss              | 2.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0054  |
| entropy                 | 1.83     |
| episodes                | 129400   |
| lives                   | 129400   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 842075   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.82     |
| episodes                | 129500   |
| lives                   | 129500   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 842815   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.79     |
| episodes                | 129600   |
| lives                   | 129600   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 843562   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.86     |
| episodes                | 129700   |
| lives                   | 129700   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 844345   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.86     |
| episodes                | 129800   |
| lives                   | 129800   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 8.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 845106   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.84     |
| episodes                | 129900   |
| lives                   | 129900   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 845861   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.77     |
| episodes                | 130000   |
| lives                   | 130000   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 846613   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.73     |
| episodes                | 130100   |
| lives                   | 130100   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 847348   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.73     |
| episodes                | 130200   |
| lives                   | 130200   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 848110   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.72     |
| episodes                | 130300   |
| lives                   | 130300   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 848843   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.74     |
| episodes                | 130400   |
| lives                   | 130400   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 849626   |
| value_loss              | 1.77     |
--------------------------------------
Saving model due to mean reward increase: 9.3959 -> 9.5581
Saving model due to running mean reward increase: 9.1694 -> 9.5581
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.74     |
| episodes                | 130500   |
| lives                   | 130500   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 850410   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 1.73     |
| episodes                | 130600   |
| lives                   | 130600   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.009   |
| steps                   | 851168   |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.73     |
| episodes                | 130700   |
| lives                   | 130700   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 851916   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.72     |
| episodes                | 130800   |
| lives                   | 130800   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 852632   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.72     |
| episodes                | 130900   |
| lives                   | 130900   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 853402   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.72     |
| episodes                | 131000   |
| lives                   | 131000   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 854154   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0053  |
| entropy                 | 1.71     |
| episodes                | 131100   |
| lives                   | 131100   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 8.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 854894   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.68     |
| episodes                | 131200   |
| lives                   | 131200   |
| mean 100 episode length | 8.14     |
| mean 100 episode reward | 8.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 855608   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.66     |
| episodes                | 131300   |
| lives                   | 131300   |
| mean 100 episode length | 8.1      |
| mean 100 episode reward | 8.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 856318   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.71     |
| episodes                | 131400   |
| lives                   | 131400   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 857076   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.72     |
| episodes                | 131500   |
| lives                   | 131500   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 857829   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.74     |
| episodes                | 131600   |
| lives                   | 131600   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 858570   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.72     |
| episodes                | 131700   |
| lives                   | 131700   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 8.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 859323   |
| value_loss              | 1.91     |
--------------------------------------
Saving model due to running mean reward increase: 8.8555 -> 8.8569
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.69     |
| episodes                | 131800   |
| lives                   | 131800   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 8.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 860036   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.68     |
| episodes                | 131900   |
| lives                   | 131900   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 9.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 860783   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.65     |
| episodes                | 132000   |
| lives                   | 132000   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 861509   |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.75     |
| episodes                | 132100   |
| lives                   | 132100   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 862271   |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.76     |
| episodes                | 132200   |
| lives                   | 132200   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 863016   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.75     |
| episodes                | 132300   |
| lives                   | 132300   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 8.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 863754   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.69     |
| episodes                | 132400   |
| lives                   | 132400   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 864483   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.76     |
| episodes                | 132500   |
| lives                   | 132500   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 8.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 865219   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.75     |
| episodes                | 132600   |
| lives                   | 132600   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 865972   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.77     |
| episodes                | 132700   |
| lives                   | 132700   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 866716   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.75     |
| episodes                | 132800   |
| lives                   | 132800   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 867467   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.74     |
| episodes                | 132900   |
| lives                   | 132900   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 868188   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.69     |
| episodes                | 133000   |
| lives                   | 133000   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 868923   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.66     |
| episodes                | 133100   |
| lives                   | 133100   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 8.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 869626   |
| value_loss              | 1.76     |
--------------------------------------
Saving model due to running mean reward increase: 8.7642 -> 9.0516
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 1.7      |
| episodes                | 133200   |
| lives                   | 133200   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 870380   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 1.63     |
| episodes                | 133300   |
| lives                   | 133300   |
| mean 100 episode length | 8.18     |
| mean 100 episode reward | 8.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 871098   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.68     |
| episodes                | 133400   |
| lives                   | 133400   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 871842   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.67     |
| episodes                | 133500   |
| lives                   | 133500   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 872580   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.66     |
| episodes                | 133600   |
| lives                   | 133600   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 873314   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.68     |
| episodes                | 133700   |
| lives                   | 133700   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 874048   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.73     |
| episodes                | 133800   |
| lives                   | 133800   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 874817   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.68     |
| episodes                | 133900   |
| lives                   | 133900   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 875558   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.59     |
| episodes                | 134000   |
| lives                   | 134000   |
| mean 100 episode length | 8.05     |
| mean 100 episode reward | 8.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 876263   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.68     |
| episodes                | 134100   |
| lives                   | 134100   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 9.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 877012   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.66     |
| episodes                | 134200   |
| lives                   | 134200   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 877729   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.7      |
| episodes                | 134300   |
| lives                   | 134300   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 878465   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.64     |
| episodes                | 134400   |
| lives                   | 134400   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 8.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 879180   |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.68     |
| episodes                | 134500   |
| lives                   | 134500   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 879916   |
| value_loss              | 1.58     |
--------------------------------------
Saving model due to running mean reward increase: 8.7431 -> 9.2333
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.74     |
| episodes                | 134600   |
| lives                   | 134600   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 880729   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.68     |
| episodes                | 134700   |
| lives                   | 134700   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 8.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 881480   |
| value_loss              | 2.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0079  |
| entropy                 | 1.68     |
| episodes                | 134800   |
| lives                   | 134800   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 882214   |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.74     |
| episodes                | 134900   |
| lives                   | 134900   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 883002   |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.76     |
| episodes                | 135000   |
| lives                   | 135000   |
| mean 100 episode length | 8.74     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 883776   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.72     |
| episodes                | 135100   |
| lives                   | 135100   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 884551   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.64     |
| episodes                | 135200   |
| lives                   | 135200   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 885313   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.67     |
| episodes                | 135300   |
| lives                   | 135300   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 886051   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.67     |
| episodes                | 135400   |
| lives                   | 135400   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 886793   |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.69     |
| episodes                | 135500   |
| lives                   | 135500   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 887558   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.6      |
| episodes                | 135600   |
| lives                   | 135600   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 888274   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.62     |
| episodes                | 135700   |
| lives                   | 135700   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 889003   |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.61     |
| episodes                | 135800   |
| lives                   | 135800   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 8.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 889704   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 1.61     |
| episodes                | 135900   |
| lives                   | 135900   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.01    |
| steps                   | 890438   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.66     |
| episodes                | 136000   |
| lives                   | 136000   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 891183   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.64     |
| episodes                | 136100   |
| lives                   | 136100   |
| mean 100 episode length | 8.13     |
| mean 100 episode reward | 8.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 891896   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.68     |
| episodes                | 136200   |
| lives                   | 136200   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 892641   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.67     |
| episodes                | 136300   |
| lives                   | 136300   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 893407   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.75     |
| episodes                | 136400   |
| lives                   | 136400   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 894186   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.75     |
| episodes                | 136500   |
| lives                   | 136500   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 894987   |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.71     |
| episodes                | 136600   |
| lives                   | 136600   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 895748   |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.66     |
| episodes                | 136700   |
| lives                   | 136700   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 8.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 896476   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.67     |
| episodes                | 136800   |
| lives                   | 136800   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 897236   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.67     |
| episodes                | 136900   |
| lives                   | 136900   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 897986   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.67     |
| episodes                | 137000   |
| lives                   | 137000   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 898754   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.66     |
| episodes                | 137100   |
| lives                   | 137100   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 899520   |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.56     |
| episodes                | 137200   |
| lives                   | 137200   |
| mean 100 episode length | 8.16     |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 900236   |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.65     |
| episodes                | 137300   |
| lives                   | 137300   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 9.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 901007   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.69     |
| episodes                | 137400   |
| lives                   | 137400   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 901779   |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.66     |
| episodes                | 137500   |
| lives                   | 137500   |
| mean 100 episode length | 8.38     |
| mean 100 episode reward | 8.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 902517   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.73     |
| episodes                | 137600   |
| lives                   | 137600   |
| mean 100 episode length | 8.98     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 903315   |
| value_loss              | 2.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0054  |
| entropy                 | 1.68     |
| episodes                | 137700   |
| lives                   | 137700   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 904070   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.69     |
| episodes                | 137800   |
| lives                   | 137800   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 904847   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.64     |
| episodes                | 137900   |
| lives                   | 137900   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 905592   |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.7      |
| episodes                | 138000   |
| lives                   | 138000   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 8.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 906356   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0059  |
| entropy                 | 1.74     |
| episodes                | 138100   |
| lives                   | 138100   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 907145   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.79     |
| episodes                | 138200   |
| lives                   | 138200   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 9.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 907965   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.72     |
| episodes                | 138300   |
| lives                   | 138300   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 908734   |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.72     |
| episodes                | 138400   |
| lives                   | 138400   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 909518   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.69     |
| episodes                | 138500   |
| lives                   | 138500   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 8.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 910260   |
| value_loss              | 2.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.67     |
| episodes                | 138600   |
| lives                   | 138600   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 911021   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.62     |
| episodes                | 138700   |
| lives                   | 138700   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 8.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 911751   |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.67     |
| episodes                | 138800   |
| lives                   | 138800   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 912501   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.63     |
| episodes                | 138900   |
| lives                   | 138900   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 8.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 913221   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.68     |
| episodes                | 139000   |
| lives                   | 139000   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 913985   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.69     |
| episodes                | 139100   |
| lives                   | 139100   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 914740   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.69     |
| episodes                | 139200   |
| lives                   | 139200   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.008   |
| steps                   | 915492   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.62     |
| episodes                | 139300   |
| lives                   | 139300   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 8.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 916212   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.68     |
| episodes                | 139400   |
| lives                   | 139400   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 916957   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.68     |
| episodes                | 139500   |
| lives                   | 139500   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 917739   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.66     |
| episodes                | 139600   |
| lives                   | 139600   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 918500   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.69     |
| episodes                | 139700   |
| lives                   | 139700   |
| mean 100 episode length | 8.63     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 919263   |
| value_loss              | 1.44     |
--------------------------------------
Saving model due to running mean reward increase: 9.2994 -> 9.5447
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.68     |
| episodes                | 139800   |
| lives                   | 139800   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 920036   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.68     |
| episodes                | 139900   |
| lives                   | 139900   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 920783   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.7      |
| episodes                | 140000   |
| lives                   | 140000   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 921562   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.69     |
| episodes                | 140100   |
| lives                   | 140100   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 922296   |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.72     |
| episodes                | 140200   |
| lives                   | 140200   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 923047   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.68     |
| episodes                | 140300   |
| lives                   | 140300   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 923806   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.69     |
| episodes                | 140400   |
| lives                   | 140400   |
| mean 100 episode length | 8.34     |
| mean 100 episode reward | 8.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 924540   |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.68     |
| episodes                | 140500   |
| lives                   | 140500   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 925299   |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.63     |
| episodes                | 140600   |
| lives                   | 140600   |
| mean 100 episode length | 7.91     |
| mean 100 episode reward | 8.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 925990   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.59     |
| episodes                | 140700   |
| lives                   | 140700   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 8.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 926691   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.74     |
| episodes                | 140800   |
| lives                   | 140800   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 927412   |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.75     |
| episodes                | 140900   |
| lives                   | 140900   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 928171   |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.71     |
| episodes                | 141000   |
| lives                   | 141000   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 8.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 928914   |
| value_loss              | 2.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.79     |
| episodes                | 141100   |
| lives                   | 141100   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 929693   |
| value_loss              | 1.77     |
--------------------------------------
Saving model due to running mean reward increase: 9.042 -> 9.1998
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.84     |
| episodes                | 141200   |
| lives                   | 141200   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 930502   |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.77     |
| episodes                | 141300   |
| lives                   | 141300   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 931259   |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.79     |
| episodes                | 141400   |
| lives                   | 141400   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 9.06     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 932030   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 1.72     |
| episodes                | 141500   |
| lives                   | 141500   |
| mean 100 episode length | 8.2      |
| mean 100 episode reward | 8.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 932750   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.8      |
| episodes                | 141600   |
| lives                   | 141600   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 933506   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.81     |
| episodes                | 141700   |
| lives                   | 141700   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 934274   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.8      |
| episodes                | 141800   |
| lives                   | 141800   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 8.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 935016   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.78     |
| episodes                | 141900   |
| lives                   | 141900   |
| mean 100 episode length | 8.25     |
| mean 100 episode reward | 8.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 935741   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.82     |
| episodes                | 142000   |
| lives                   | 142000   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 8.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 936484   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.83     |
| episodes                | 142100   |
| lives                   | 142100   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 937250   |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.79     |
| episodes                | 142200   |
| lives                   | 142200   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 8.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 938007   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.84     |
| episodes                | 142300   |
| lives                   | 142300   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 8.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 938772   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.8      |
| episodes                | 142400   |
| lives                   | 142400   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 939518   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.78     |
| episodes                | 142500   |
| lives                   | 142500   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 8.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 940266   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.7      |
| episodes                | 142600   |
| lives                   | 142600   |
| mean 100 episode length | 8.01     |
| mean 100 episode reward | 8.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 940967   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.75     |
| episodes                | 142700   |
| lives                   | 142700   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 941718   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.69     |
| episodes                | 142800   |
| lives                   | 142800   |
| mean 100 episode length | 8.15     |
| mean 100 episode reward | 8.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 942433   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.73     |
| episodes                | 142900   |
| lives                   | 142900   |
| mean 100 episode length | 8.19     |
| mean 100 episode reward | 8.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 943152   |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.78     |
| episodes                | 143000   |
| lives                   | 143000   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 8.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 943889   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.85     |
| episodes                | 143100   |
| lives                   | 143100   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 944682   |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.72     |
| episodes                | 143200   |
| lives                   | 143200   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 8.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 945399   |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.72     |
| episodes                | 143300   |
| lives                   | 143300   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 946143   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.72     |
| episodes                | 143400   |
| lives                   | 143400   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 946898   |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0053  |
| entropy                 | 1.7      |
| episodes                | 143500   |
| lives                   | 143500   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 947640   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.71     |
| episodes                | 143600   |
| lives                   | 143600   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 9.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 948377   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.75     |
| episodes                | 143700   |
| lives                   | 143700   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 949128   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.75     |
| episodes                | 143800   |
| lives                   | 143800   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 949881   |
| value_loss              | 1.47     |
--------------------------------------
Saving model due to running mean reward increase: 9.0243 -> 9.4408
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.7      |
| episodes                | 143900   |
| lives                   | 143900   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 9.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 950621   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.75     |
| episodes                | 144000   |
| lives                   | 144000   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 951388   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0058  |
| entropy                 | 1.67     |
| episodes                | 144100   |
| lives                   | 144100   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 952129   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.65     |
| episodes                | 144200   |
| lives                   | 144200   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 952882   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.65     |
| episodes                | 144300   |
| lives                   | 144300   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 953624   |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.63     |
| episodes                | 144400   |
| lives                   | 144400   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 954364   |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.66     |
| episodes                | 144500   |
| lives                   | 144500   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 955130   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.63     |
| episodes                | 144600   |
| lives                   | 144600   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 955886   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.62     |
| episodes                | 144700   |
| lives                   | 144700   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 956644   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.6      |
| episodes                | 144800   |
| lives                   | 144800   |
| mean 100 episode length | 8.33     |
| mean 100 episode reward | 8.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 957377   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.62     |
| episodes                | 144900   |
| lives                   | 144900   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 958133   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0061  |
| entropy                 | 1.67     |
| episodes                | 145000   |
| lives                   | 145000   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 958903   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.65     |
| episodes                | 145100   |
| lives                   | 145100   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 959676   |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.66     |
| episodes                | 145200   |
| lives                   | 145200   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 960451   |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.68     |
| episodes                | 145300   |
| lives                   | 145300   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 961211   |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.65     |
| episodes                | 145400   |
| lives                   | 145400   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 9.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 961997   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.65     |
| episodes                | 145500   |
| lives                   | 145500   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 962794   |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.64     |
| episodes                | 145600   |
| lives                   | 145600   |
| mean 100 episode length | 8.8      |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 963574   |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.65     |
| episodes                | 145700   |
| lives                   | 145700   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 964328   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.64     |
| episodes                | 145800   |
| lives                   | 145800   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 965093   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.69     |
| episodes                | 145900   |
| lives                   | 145900   |
| mean 100 episode length | 8.74     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 965867   |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.68     |
| episodes                | 146000   |
| lives                   | 146000   |
| mean 100 episode length | 8.74     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 966641   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.61     |
| episodes                | 146100   |
| lives                   | 146100   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 967405   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.58     |
| episodes                | 146200   |
| lives                   | 146200   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 968141   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.62     |
| episodes                | 146300   |
| lives                   | 146300   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 968906   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.66     |
| episodes                | 146400   |
| lives                   | 146400   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 969677   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.65     |
| episodes                | 146500   |
| lives                   | 146500   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 970445   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.59     |
| episodes                | 146600   |
| lives                   | 146600   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 971193   |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.69     |
| episodes                | 146700   |
| lives                   | 146700   |
| mean 100 episode length | 8.42     |
| mean 100 episode reward | 9.07     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 971935   |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.68     |
| episodes                | 146800   |
| lives                   | 146800   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 972681   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.64     |
| episodes                | 146900   |
| lives                   | 146900   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 973407   |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.7      |
| episodes                | 147000   |
| lives                   | 147000   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 974163   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.69     |
| episodes                | 147100   |
| lives                   | 147100   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 974887   |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.68     |
| episodes                | 147200   |
| lives                   | 147200   |
| mean 100 episode length | 8.06     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 975593   |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.73     |
| episodes                | 147300   |
| lives                   | 147300   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 976341   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.76     |
| episodes                | 147400   |
| lives                   | 147400   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 977109   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.71     |
| episodes                | 147500   |
| lives                   | 147500   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 977861   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.76     |
| episodes                | 147600   |
| lives                   | 147600   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 9.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 978647   |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.76     |
| episodes                | 147700   |
| lives                   | 147700   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 979428   |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.73     |
| episodes                | 147800   |
| lives                   | 147800   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 980177   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.73     |
| episodes                | 147900   |
| lives                   | 147900   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 9.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 980920   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.71     |
| episodes                | 148000   |
| lives                   | 148000   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 981666   |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.75     |
| episodes                | 148100   |
| lives                   | 148100   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0072  |
| steps                   | 982422   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.73     |
| episodes                | 148200   |
| lives                   | 148200   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0082  |
| steps                   | 983173   |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.72     |
| episodes                | 148300   |
| lives                   | 148300   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 983904   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.76     |
| episodes                | 148400   |
| lives                   | 148400   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 984701   |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.73     |
| episodes                | 148500   |
| lives                   | 148500   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 985458   |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.66     |
| episodes                | 148600   |
| lives                   | 148600   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 986182   |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.66     |
| episodes                | 148700   |
| lives                   | 148700   |
| mean 100 episode length | 8.03     |
| mean 100 episode reward | 8.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 986885   |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.7      |
| episodes                | 148800   |
| lives                   | 148800   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 987658   |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.67     |
| episodes                | 148900   |
| lives                   | 148900   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 988436   |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.64     |
| episodes                | 149000   |
| lives                   | 149000   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 989191   |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.72     |
| episodes                | 149100   |
| lives                   | 149100   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 989981   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.68     |
| episodes                | 149200   |
| lives                   | 149200   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 990741   |
| value_loss              | 2.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.72     |
| episodes                | 149300   |
| lives                   | 149300   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 991522   |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.69     |
| episodes                | 149400   |
| lives                   | 149400   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 8.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 992270   |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.72     |
| episodes                | 149500   |
| lives                   | 149500   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 993055   |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.68     |
| episodes                | 149600   |
| lives                   | 149600   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 993813   |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.66     |
| episodes                | 149700   |
| lives                   | 149700   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 994559   |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.61     |
| episodes                | 149800   |
| lives                   | 149800   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 995291   |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.63     |
| episodes                | 149900   |
| lives                   | 149900   |
| mean 100 episode length | 8.36     |
| mean 100 episode reward | 8.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 996027   |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.68     |
| episodes                | 150000   |
| lives                   | 150000   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 996803   |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.57     |
| episodes                | 150100   |
| lives                   | 150100   |
| mean 100 episode length | 8.26     |
| mean 100 episode reward | 9.1      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 997529   |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.66     |
| episodes                | 150200   |
| lives                   | 150200   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 998302   |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.65     |
| episodes                | 150300   |
| lives                   | 150300   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 999075   |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.66     |
| episodes                | 150400   |
| lives                   | 150400   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 999828   |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.66     |
| episodes                | 150500   |
| lives                   | 150500   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 8.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1000577  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.65     |
| episodes                | 150600   |
| lives                   | 150600   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1001362  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.66     |
| episodes                | 150700   |
| lives                   | 150700   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1002141  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.62     |
| episodes                | 150800   |
| lives                   | 150800   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1002892  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.68     |
| episodes                | 150900   |
| lives                   | 150900   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1003665  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.63     |
| episodes                | 151000   |
| lives                   | 151000   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.11     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0081  |
| steps                   | 1004416  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.69     |
| episodes                | 151100   |
| lives                   | 151100   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1005186  |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.63     |
| episodes                | 151200   |
| lives                   | 151200   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 9.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1005940  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.66     |
| episodes                | 151300   |
| lives                   | 151300   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1006705  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.61     |
| episodes                | 151400   |
| lives                   | 151400   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 8.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1007432  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.66     |
| episodes                | 151500   |
| lives                   | 151500   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.12     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1008187  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.68     |
| episodes                | 151600   |
| lives                   | 151600   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1008939  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.68     |
| episodes                | 151700   |
| lives                   | 151700   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1009707  |
| value_loss              | 1.71     |
--------------------------------------
Saving model due to running mean reward increase: 8.9398 -> 9.3862
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.67     |
| episodes                | 151800   |
| lives                   | 151800   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1010453  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0007   |
| entropy                 | 1.65     |
| episodes                | 151900   |
| lives                   | 151900   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 9        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1011188  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.69     |
| episodes                | 152000   |
| lives                   | 152000   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1011925  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.71     |
| episodes                | 152100   |
| lives                   | 152100   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1012687  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.8      |
| episodes                | 152200   |
| lives                   | 152200   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1013486  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.78     |
| episodes                | 152300   |
| lives                   | 152300   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 8.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1014264  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.83     |
| episodes                | 152400   |
| lives                   | 152400   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1015099  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.77     |
| episodes                | 152500   |
| lives                   | 152500   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1015867  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.72     |
| episodes                | 152600   |
| lives                   | 152600   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 8.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1016610  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.74     |
| episodes                | 152700   |
| lives                   | 152700   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1017382  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.72     |
| episodes                | 152800   |
| lives                   | 152800   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1018193  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.72     |
| episodes                | 152900   |
| lives                   | 152900   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 9.01     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1018942  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.75     |
| episodes                | 153000   |
| lives                   | 153000   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 8.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1019696  |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.81     |
| episodes                | 153100   |
| lives                   | 153100   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1020523  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.7      |
| episodes                | 153200   |
| lives                   | 153200   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9        |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1021284  |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.72     |
| episodes                | 153300   |
| lives                   | 153300   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1022035  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.73     |
| episodes                | 153400   |
| lives                   | 153400   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1022813  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.74     |
| episodes                | 153500   |
| lives                   | 153500   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1023625  |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.74     |
| episodes                | 153600   |
| lives                   | 153600   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1024416  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.69     |
| episodes                | 153700   |
| lives                   | 153700   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 9.05     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0075  |
| steps                   | 1025159  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.71     |
| episodes                | 153800   |
| lives                   | 153800   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1025898  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.68     |
| episodes                | 153900   |
| lives                   | 153900   |
| mean 100 episode length | 8.35     |
| mean 100 episode reward | 8.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1026633  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.72     |
| episodes                | 154000   |
| lives                   | 154000   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1027403  |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.65     |
| episodes                | 154100   |
| lives                   | 154100   |
| mean 100 episode length | 8.29     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1028132  |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.67     |
| episodes                | 154200   |
| lives                   | 154200   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1028899  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.69     |
| episodes                | 154300   |
| lives                   | 154300   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1029647  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.65     |
| episodes                | 154400   |
| lives                   | 154400   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1030415  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.68     |
| episodes                | 154500   |
| lives                   | 154500   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1031193  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.72     |
| episodes                | 154600   |
| lives                   | 154600   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1031990  |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.61     |
| episodes                | 154700   |
| lives                   | 154700   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1032731  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.61     |
| episodes                | 154800   |
| lives                   | 154800   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1033484  |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0056  |
| entropy                 | 1.64     |
| episodes                | 154900   |
| lives                   | 154900   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1034235  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.57     |
| episodes                | 155000   |
| lives                   | 155000   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1034974  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.64     |
| episodes                | 155100   |
| lives                   | 155100   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1035714  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.66     |
| episodes                | 155200   |
| lives                   | 155200   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1036489  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0051  |
| entropy                 | 1.69     |
| episodes                | 155300   |
| lives                   | 155300   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1037257  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.67     |
| episodes                | 155400   |
| lives                   | 155400   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1038018  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.68     |
| episodes                | 155500   |
| lives                   | 155500   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1038776  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.62     |
| episodes                | 155600   |
| lives                   | 155600   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1039497  |
| value_loss              | 1.79     |
--------------------------------------
Saving model due to running mean reward increase: 8.58 -> 8.927
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.6      |
| episodes                | 155700   |
| lives                   | 155700   |
| mean 100 episode length | 7.96     |
| mean 100 episode reward | 8.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1040193  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.66     |
| episodes                | 155800   |
| lives                   | 155800   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1040951  |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.7      |
| episodes                | 155900   |
| lives                   | 155900   |
| mean 100 episode length | 8.3      |
| mean 100 episode reward | 8.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1041681  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.79     |
| episodes                | 156000   |
| lives                   | 156000   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1042467  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.75     |
| episodes                | 156100   |
| lives                   | 156100   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1043251  |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.73     |
| episodes                | 156200   |
| lives                   | 156200   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 9.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1043995  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.76     |
| episodes                | 156300   |
| lives                   | 156300   |
| mean 100 episode length | 8.74     |
| mean 100 episode reward | 9.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1044769  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.75     |
| episodes                | 156400   |
| lives                   | 156400   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1045550  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.65     |
| episodes                | 156500   |
| lives                   | 156500   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1046310  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.69     |
| episodes                | 156600   |
| lives                   | 156600   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1047109  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.69     |
| episodes                | 156700   |
| lives                   | 156700   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 9.02     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1047858  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.68     |
| episodes                | 156800   |
| lives                   | 156800   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1048647  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.72     |
| episodes                | 156900   |
| lives                   | 156900   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1049413  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.66     |
| episodes                | 157000   |
| lives                   | 157000   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1050161  |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.71     |
| episodes                | 157100   |
| lives                   | 157100   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1050947  |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.74     |
| episodes                | 157200   |
| lives                   | 157200   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.48     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1051759  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.71     |
| episodes                | 157300   |
| lives                   | 157300   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1052552  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.63     |
| episodes                | 157400   |
| lives                   | 157400   |
| mean 100 episode length | 8.49     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1053301  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.67     |
| episodes                | 157500   |
| lives                   | 157500   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1054047  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.63     |
| episodes                | 157600   |
| lives                   | 157600   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1054803  |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.64     |
| episodes                | 157700   |
| lives                   | 157700   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1055572  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.68     |
| episodes                | 157800   |
| lives                   | 157800   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1056334  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.7      |
| episodes                | 157900   |
| lives                   | 157900   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1057109  |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.66     |
| episodes                | 158000   |
| lives                   | 158000   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1057859  |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.6      |
| episodes                | 158100   |
| lives                   | 158100   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1058591  |
| value_loss              | 2.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.67     |
| episodes                | 158200   |
| lives                   | 158200   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1059355  |
| value_loss              | 1.65     |
--------------------------------------
Saving model due to mean reward increase: 9.5581 -> 9.6168
Saving model due to running mean reward increase: 9.4116 -> 9.6168
--------------------------------------
| approx_kl               | -0.0062  |
| entropy                 | 1.7      |
| episodes                | 158300   |
| lives                   | 158300   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1060150  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.71     |
| episodes                | 158400   |
| lives                   | 158400   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0089  |
| steps                   | 1060959  |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.69     |
| episodes                | 158500   |
| lives                   | 158500   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1061743  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.62     |
| episodes                | 158600   |
| lives                   | 158600   |
| mean 100 episode length | 8.28     |
| mean 100 episode reward | 9.13     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1062471  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.66     |
| episodes                | 158700   |
| lives                   | 158700   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.16     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1063224  |
| value_loss              | 2.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.66     |
| episodes                | 158800   |
| lives                   | 158800   |
| mean 100 episode length | 8.57     |
| mean 100 episode reward | 9.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1063981  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.6      |
| episodes                | 158900   |
| lives                   | 158900   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1064749  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.59     |
| episodes                | 159000   |
| lives                   | 159000   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1065503  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.67     |
| episodes                | 159100   |
| lives                   | 159100   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1066275  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.67     |
| episodes                | 159200   |
| lives                   | 159200   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1067056  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.66     |
| episodes                | 159300   |
| lives                   | 159300   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1067839  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.74     |
| episodes                | 159400   |
| lives                   | 159400   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1068643  |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.7      |
| episodes                | 159500   |
| lives                   | 159500   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1069418  |
| value_loss              | 1.88     |
--------------------------------------
Saving model due to running mean reward increase: 9.1082 -> 9.228
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.63     |
| episodes                | 159600   |
| lives                   | 159600   |
| mean 100 episode length | 8.48     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1070166  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.65     |
| episodes                | 159700   |
| lives                   | 159700   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1070932  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.67     |
| episodes                | 159800   |
| lives                   | 159800   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1071694  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.7      |
| episodes                | 159900   |
| lives                   | 159900   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1072455  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.71     |
| episodes                | 160000   |
| lives                   | 160000   |
| mean 100 episode length | 8.39     |
| mean 100 episode reward | 8.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1073194  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.73     |
| episodes                | 160100   |
| lives                   | 160100   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1073982  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.75     |
| episodes                | 160200   |
| lives                   | 160200   |
| mean 100 episode length | 8.8      |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1074762  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.66     |
| episodes                | 160300   |
| lives                   | 160300   |
| mean 100 episode length | 8.43     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1075505  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.65     |
| episodes                | 160400   |
| lives                   | 160400   |
| mean 100 episode length | 8.21     |
| mean 100 episode reward | 8.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1076226  |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.61     |
| episodes                | 160500   |
| lives                   | 160500   |
| mean 100 episode length | 8.31     |
| mean 100 episode reward | 8.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1076957  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.66     |
| episodes                | 160600   |
| lives                   | 160600   |
| mean 100 episode length | 8.46     |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1077703  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.68     |
| episodes                | 160700   |
| lives                   | 160700   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1078473  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0056  |
| entropy                 | 1.63     |
| episodes                | 160800   |
| lives                   | 160800   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1079239  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.65     |
| episodes                | 160900   |
| lives                   | 160900   |
| mean 100 episode length | 8.54     |
| mean 100 episode reward | 9.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1079993  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.69     |
| episodes                | 161000   |
| lives                   | 161000   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1080749  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.69     |
| episodes                | 161100   |
| lives                   | 161100   |
| mean 100 episode length | 8.17     |
| mean 100 episode reward | 8.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1081466  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.68     |
| episodes                | 161200   |
| lives                   | 161200   |
| mean 100 episode length | 8.4      |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1082206  |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.71     |
| episodes                | 161300   |
| lives                   | 161300   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1082981  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.63     |
| episodes                | 161400   |
| lives                   | 161400   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1083749  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.66     |
| episodes                | 161500   |
| lives                   | 161500   |
| mean 100 episode length | 8.98     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1084547  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.62     |
| episodes                | 161600   |
| lives                   | 161600   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1085307  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.58     |
| episodes                | 161700   |
| lives                   | 161700   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1086076  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.68     |
| episodes                | 161800   |
| lives                   | 161800   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1086885  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.66     |
| episodes                | 161900   |
| lives                   | 161900   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1087707  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.62     |
| episodes                | 162000   |
| lives                   | 162000   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1088489  |
| value_loss              | 2.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.65     |
| episodes                | 162100   |
| lives                   | 162100   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1089289  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.6      |
| episodes                | 162200   |
| lives                   | 162200   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1090061  |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.58     |
| episodes                | 162300   |
| lives                   | 162300   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1090806  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.67     |
| episodes                | 162400   |
| lives                   | 162400   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1091558  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.64     |
| episodes                | 162500   |
| lives                   | 162500   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1092335  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.66     |
| episodes                | 162600   |
| lives                   | 162600   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1093105  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.72     |
| episodes                | 162700   |
| lives                   | 162700   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1093909  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.75     |
| episodes                | 162800   |
| lives                   | 162800   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1094702  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.69     |
| episodes                | 162900   |
| lives                   | 162900   |
| mean 100 episode length | 8.8      |
| mean 100 episode reward | 9.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1095482  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.65     |
| episodes                | 163000   |
| lives                   | 163000   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.39     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1096246  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.67     |
| episodes                | 163100   |
| lives                   | 163100   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.35     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1097015  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.68     |
| episodes                | 163200   |
| lives                   | 163200   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1097807  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.66     |
| episodes                | 163300   |
| lives                   | 163300   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1098580  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.71     |
| episodes                | 163400   |
| lives                   | 163400   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1099379  |
| value_loss              | 1.49     |
--------------------------------------
Saving model due to mean reward increase: 9.6168 -> 9.6304
Saving model due to running mean reward increase: 9.418 -> 9.6304
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.71     |
| episodes                | 163500   |
| lives                   | 163500   |
| mean 100 episode length | 8.96     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1100175  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.66     |
| episodes                | 163600   |
| lives                   | 163600   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.33     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1100951  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.68     |
| episodes                | 163700   |
| lives                   | 163700   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1101736  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.62     |
| episodes                | 163800   |
| lives                   | 163800   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1102500  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.62     |
| episodes                | 163900   |
| lives                   | 163900   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1103279  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.62     |
| episodes                | 164000   |
| lives                   | 164000   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.03     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1104031  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.61     |
| episodes                | 164100   |
| lives                   | 164100   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1104799  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.63     |
| episodes                | 164200   |
| lives                   | 164200   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1105549  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.61     |
| episodes                | 164300   |
| lives                   | 164300   |
| mean 100 episode length | 8.56     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1106305  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.64     |
| episodes                | 164400   |
| lives                   | 164400   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1107112  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.61     |
| episodes                | 164500   |
| lives                   | 164500   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 8.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1107865  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.56     |
| episodes                | 164600   |
| lives                   | 164600   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.37     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1108620  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.55     |
| episodes                | 164700   |
| lives                   | 164700   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1109413  |
| value_loss              | 1.71     |
--------------------------------------
Saving model due to running mean reward increase: 9.3495 -> 9.494
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.59     |
| episodes                | 164800   |
| lives                   | 164800   |
| mean 100 episode length | 8.86     |
| mean 100 episode reward | 9.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1110199  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.57     |
| episodes                | 164900   |
| lives                   | 164900   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1110967  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.57     |
| episodes                | 165000   |
| lives                   | 165000   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1111720  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.6      |
| episodes                | 165100   |
| lives                   | 165100   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1112498  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.58     |
| episodes                | 165200   |
| lives                   | 165200   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1113267  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.61     |
| episodes                | 165300   |
| lives                   | 165300   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1114035  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.59     |
| episodes                | 165400   |
| lives                   | 165400   |
| mean 100 episode length | 8.44     |
| mean 100 episode reward | 9.04     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1114779  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.66     |
| episodes                | 165500   |
| lives                   | 165500   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1115540  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.7      |
| episodes                | 165600   |
| lives                   | 165600   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1116312  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.74     |
| episodes                | 165700   |
| lives                   | 165700   |
| mean 100 episode length | 8.96     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1117108  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.73     |
| episodes                | 165800   |
| lives                   | 165800   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1117901  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.73     |
| episodes                | 165900   |
| lives                   | 165900   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1118680  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.7      |
| episodes                | 166000   |
| lives                   | 166000   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 8.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1119425  |
| value_loss              | 2.05     |
--------------------------------------
Saving model due to running mean reward increase: 9.03 -> 9.0359
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.68     |
| episodes                | 166100   |
| lives                   | 166100   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 8.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1120162  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.67     |
| episodes                | 166200   |
| lives                   | 166200   |
| mean 100 episode length | 8.6      |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0086  |
| steps                   | 1120922  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.6      |
| episodes                | 166300   |
| lives                   | 166300   |
| mean 100 episode length | 8.32     |
| mean 100 episode reward | 9.09     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1121654  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.64     |
| episodes                | 166400   |
| lives                   | 166400   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1122421  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.61     |
| episodes                | 166500   |
| lives                   | 166500   |
| mean 100 episode length | 8.45     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1123166  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.65     |
| episodes                | 166600   |
| lives                   | 166600   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1123949  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.62     |
| episodes                | 166700   |
| lives                   | 166700   |
| mean 100 episode length | 8.27     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1124676  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.63     |
| episodes                | 166800   |
| lives                   | 166800   |
| mean 100 episode length | 8.37     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1125413  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.64     |
| episodes                | 166900   |
| lives                   | 166900   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.34     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1126168  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.64     |
| episodes                | 167000   |
| lives                   | 167000   |
| mean 100 episode length | 8.5      |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1126918  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.72     |
| episodes                | 167100   |
| lives                   | 167100   |
| mean 100 episode length | 9.06     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1127724  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.65     |
| episodes                | 167200   |
| lives                   | 167200   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1128508  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.62     |
| episodes                | 167300   |
| lives                   | 167300   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 9.2      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1129279  |
| value_loss              | 1.67     |
--------------------------------------
Saving model due to running mean reward increase: 9.1328 -> 9.5929
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.63     |
| episodes                | 167400   |
| lives                   | 167400   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1130047  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.69     |
| episodes                | 167500   |
| lives                   | 167500   |
| mean 100 episode length | 9.03     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1130850  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.67     |
| episodes                | 167600   |
| lives                   | 167600   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1131625  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.72     |
| episodes                | 167700   |
| lives                   | 167700   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1132434  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.7      |
| episodes                | 167800   |
| lives                   | 167800   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1133238  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.69     |
| episodes                | 167900   |
| lives                   | 167900   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1134029  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.65     |
| episodes                | 168000   |
| lives                   | 168000   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1134797  |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.65     |
| episodes                | 168100   |
| lives                   | 168100   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1135564  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.62     |
| episodes                | 168200   |
| lives                   | 168200   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1136316  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.66     |
| episodes                | 168300   |
| lives                   | 168300   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1137109  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.6      |
| episodes                | 168400   |
| lives                   | 168400   |
| mean 100 episode length | 8.64     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1137873  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.57     |
| episodes                | 168500   |
| lives                   | 168500   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1138631  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.64     |
| episodes                | 168600   |
| lives                   | 168600   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1139418  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.6      |
| episodes                | 168700   |
| lives                   | 168700   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1140184  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.65     |
| episodes                | 168800   |
| lives                   | 168800   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1140960  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.66     |
| episodes                | 168900   |
| lives                   | 168900   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1141742  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.68     |
| episodes                | 169000   |
| lives                   | 169000   |
| mean 100 episode length | 9.03     |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1142545  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.68     |
| episodes                | 169100   |
| lives                   | 169100   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1143342  |
| value_loss              | 2.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.59     |
| episodes                | 169200   |
| lives                   | 169200   |
| mean 100 episode length | 8.55     |
| mean 100 episode reward | 9.17     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1144097  |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.68     |
| episodes                | 169300   |
| lives                   | 169300   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1144885  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.6      |
| episodes                | 169400   |
| lives                   | 169400   |
| mean 100 episode length | 8.24     |
| mean 100 episode reward | 8.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1145609  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.72     |
| episodes                | 169500   |
| lives                   | 169500   |
| mean 100 episode length | 9.18     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1146427  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.71     |
| episodes                | 169600   |
| lives                   | 169600   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1147229  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.62     |
| episodes                | 169700   |
| lives                   | 169700   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.15     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1147990  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.6      |
| episodes                | 169800   |
| lives                   | 169800   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.19     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1148756  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.57     |
| episodes                | 169900   |
| lives                   | 169900   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1149528  |
| value_loss              | 1.38     |
--------------------------------------
Saving model due to mean reward increase: 9.6304 -> 9.7796
Saving model due to running mean reward increase: 9.4262 -> 9.7796
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.64     |
| episodes                | 170000   |
| lives                   | 170000   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1150332  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.64     |
| episodes                | 170100   |
| lives                   | 170100   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.5      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1151133  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.59     |
| episodes                | 170200   |
| lives                   | 170200   |
| mean 100 episode length | 8.67     |
| mean 100 episode reward | 9.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1151900  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.66     |
| episodes                | 170300   |
| lives                   | 170300   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1152715  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.63     |
| episodes                | 170400   |
| lives                   | 170400   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1153490  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.62     |
| episodes                | 170500   |
| lives                   | 170500   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1154262  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.63     |
| episodes                | 170600   |
| lives                   | 170600   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.22     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1155037  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.6      |
| episodes                | 170700   |
| lives                   | 170700   |
| mean 100 episode length | 8.66     |
| mean 100 episode reward | 9.36     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1155803  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.62     |
| episodes                | 170800   |
| lives                   | 170800   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1156584  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.55     |
| episodes                | 170900   |
| lives                   | 170900   |
| mean 100 episode length | 8.51     |
| mean 100 episode reward | 9.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1157335  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.58     |
| episodes                | 171000   |
| lives                   | 171000   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1158136  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.6      |
| episodes                | 171100   |
| lives                   | 171100   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1158921  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.56     |
| episodes                | 171200   |
| lives                   | 171200   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1159691  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.55     |
| episodes                | 171300   |
| lives                   | 171300   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1160469  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.52     |
| episodes                | 171400   |
| lives                   | 171400   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.23     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1161237  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.59     |
| episodes                | 171500   |
| lives                   | 171500   |
| mean 100 episode length | 9.16     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1162053  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.55     |
| episodes                | 171600   |
| lives                   | 171600   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1162840  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.54     |
| episodes                | 171700   |
| lives                   | 171700   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1163624  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.62     |
| episodes                | 171800   |
| lives                   | 171800   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1164405  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.62     |
| episodes                | 171900   |
| lives                   | 171900   |
| mean 100 episode length | 8.8      |
| mean 100 episode reward | 9.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1165185  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.57     |
| episodes                | 172000   |
| lives                   | 172000   |
| mean 100 episode length | 8.59     |
| mean 100 episode reward | 9.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1165944  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.66     |
| episodes                | 172100   |
| lives                   | 172100   |
| mean 100 episode length | 9.08     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1166752  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.6      |
| episodes                | 172200   |
| lives                   | 172200   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1167542  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.64     |
| episodes                | 172300   |
| lives                   | 172300   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1168347  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.6      |
| episodes                | 172400   |
| lives                   | 172400   |
| mean 100 episode length | 9.06     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1169153  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.6      |
| episodes                | 172500   |
| lives                   | 172500   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1169940  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.64     |
| episodes                | 172600   |
| lives                   | 172600   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1170734  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.62     |
| episodes                | 172700   |
| lives                   | 172700   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1171521  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.57     |
| episodes                | 172800   |
| lives                   | 172800   |
| mean 100 episode length | 8.69     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1172290  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.66     |
| episodes                | 172900   |
| lives                   | 172900   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1173094  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.59     |
| episodes                | 173000   |
| lives                   | 173000   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1173862  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.57     |
| episodes                | 173100   |
| lives                   | 173100   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1174624  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.59     |
| episodes                | 173200   |
| lives                   | 173200   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 9.24     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1175371  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.61     |
| episodes                | 173300   |
| lives                   | 173300   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1176161  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.64     |
| episodes                | 173400   |
| lives                   | 173400   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1176952  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.61     |
| episodes                | 173500   |
| lives                   | 173500   |
| mean 100 episode length | 8.41     |
| mean 100 episode reward | 9.08     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1177693  |
| value_loss              | 1.98     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.68     |
| episodes                | 173600   |
| lives                   | 173600   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1178504  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.69     |
| episodes                | 173700   |
| lives                   | 173700   |
| mean 100 episode length | 9.06     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1179310  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.71     |
| episodes                | 173800   |
| lives                   | 173800   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1180110  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.69     |
| episodes                | 173900   |
| lives                   | 173900   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0079  |
| steps                   | 1180886  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.73     |
| episodes                | 174000   |
| lives                   | 174000   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1181718  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.62     |
| episodes                | 174100   |
| lives                   | 174100   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1182494  |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 1.6      |
| episodes                | 174200   |
| lives                   | 174200   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.28     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1183294  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.68     |
| episodes                | 174300   |
| lives                   | 174300   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1184111  |
| value_loss              | 1.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.65     |
| episodes                | 174400   |
| lives                   | 174400   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1184901  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.61     |
| episodes                | 174500   |
| lives                   | 174500   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1185695  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.6      |
| episodes                | 174600   |
| lives                   | 174600   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1186474  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.59     |
| episodes                | 174700   |
| lives                   | 174700   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.38     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1187255  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.61     |
| episodes                | 174800   |
| lives                   | 174800   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1188050  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.61     |
| episodes                | 174900   |
| lives                   | 174900   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1188854  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.66     |
| episodes                | 175000   |
| lives                   | 175000   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1189695  |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.65     |
| episodes                | 175100   |
| lives                   | 175100   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.29     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1190482  |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.59     |
| episodes                | 175200   |
| lives                   | 175200   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1191283  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.65     |
| episodes                | 175300   |
| lives                   | 175300   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1192102  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.66     |
| episodes                | 175400   |
| lives                   | 175400   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 9.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1192927  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.61     |
| episodes                | 175500   |
| lives                   | 175500   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1193706  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.62     |
| episodes                | 175600   |
| lives                   | 175600   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1194497  |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.62     |
| episodes                | 175700   |
| lives                   | 175700   |
| mean 100 episode length | 8.96     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1195293  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.55     |
| episodes                | 175800   |
| lives                   | 175800   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1196088  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.56     |
| episodes                | 175900   |
| lives                   | 175900   |
| mean 100 episode length | 8.8      |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1196868  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.6      |
| episodes                | 176000   |
| lives                   | 176000   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1197649  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.59     |
| episodes                | 176100   |
| lives                   | 176100   |
| mean 100 episode length | 8.96     |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1198445  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.57     |
| episodes                | 176200   |
| lives                   | 176200   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1199235  |
| value_loss              | 1.54     |
--------------------------------------
Saving model due to mean reward increase: 9.7796 -> 9.7991
Saving model due to running mean reward increase: 9.5147 -> 9.7991
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.6      |
| episodes                | 176300   |
| lives                   | 176300   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 9.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1200062  |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.68     |
| episodes                | 176400   |
| lives                   | 176400   |
| mean 100 episode length | 9.3      |
| mean 100 episode reward | 9.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1200892  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.61     |
| episodes                | 176500   |
| lives                   | 176500   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1201701  |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.63     |
| episodes                | 176600   |
| lives                   | 176600   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 9.61     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1202514  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.56     |
| episodes                | 176700   |
| lives                   | 176700   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1203305  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0048  |
| entropy                 | 1.62     |
| episodes                | 176800   |
| lives                   | 176800   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1204136  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.59     |
| episodes                | 176900   |
| lives                   | 176900   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 9.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1204963  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.6      |
| episodes                | 177000   |
| lives                   | 177000   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1205765  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.62     |
| episodes                | 177100   |
| lives                   | 177100   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1206580  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.63     |
| episodes                | 177200   |
| lives                   | 177200   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1207395  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 1.62     |
| episodes                | 177300   |
| lives                   | 177300   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1208196  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.58     |
| episodes                | 177400   |
| lives                   | 177400   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1208988  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 1.51     |
| episodes                | 177500   |
| lives                   | 177500   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1209753  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.63     |
| episodes                | 177600   |
| lives                   | 177600   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1210591  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.66     |
| episodes                | 177700   |
| lives                   | 177700   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1211423  |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.59     |
| episodes                | 177800   |
| lives                   | 177800   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1212227  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.59     |
| episodes                | 177900   |
| lives                   | 177900   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1213009  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.6      |
| episodes                | 178000   |
| lives                   | 178000   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1213840  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.66     |
| episodes                | 178100   |
| lives                   | 178100   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1214692  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.61     |
| episodes                | 178200   |
| lives                   | 178200   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1215492  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.56     |
| episodes                | 178300   |
| lives                   | 178300   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.4      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1216271  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.61     |
| episodes                | 178400   |
| lives                   | 178400   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 9.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1217096  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.56     |
| episodes                | 178500   |
| lives                   | 178500   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1217891  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.58     |
| episodes                | 178600   |
| lives                   | 178600   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1218669  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.63     |
| episodes                | 178700   |
| lives                   | 178700   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1219493  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.52     |
| episodes                | 178800   |
| lives                   | 178800   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1220278  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.54     |
| episodes                | 178900   |
| lives                   | 178900   |
| mean 100 episode length | 8.98     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1221076  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.55     |
| episodes                | 179000   |
| lives                   | 179000   |
| mean 100 episode length | 9.08     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1221884  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.55     |
| episodes                | 179100   |
| lives                   | 179100   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.52     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1222688  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.56     |
| episodes                | 179200   |
| lives                   | 179200   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 9.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1223513  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.55     |
| episodes                | 179300   |
| lives                   | 179300   |
| mean 100 episode length | 9.18     |
| mean 100 episode reward | 9.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1224331  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0057  |
| entropy                 | 1.48     |
| episodes                | 179400   |
| lives                   | 179400   |
| mean 100 episode length | 8.58     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1225089  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.58     |
| episodes                | 179500   |
| lives                   | 179500   |
| mean 100 episode length | 9.1      |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1225899  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.52     |
| episodes                | 179600   |
| lives                   | 179600   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1226700  |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.53     |
| episodes                | 179700   |
| lives                   | 179700   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1227497  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.59     |
| episodes                | 179800   |
| lives                   | 179800   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 9.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1228330  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.56     |
| episodes                | 179900   |
| lives                   | 179900   |
| mean 100 episode length | 9.06     |
| mean 100 episode reward | 9.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1229136  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.57     |
| episodes                | 180000   |
| lives                   | 180000   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1229967  |
| value_loss              | 1.46     |
--------------------------------------
Saving model due to mean reward increase: 9.7991 -> 10.2663
Saving model due to running mean reward increase: 9.9663 -> 10.2663
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.59     |
| episodes                | 180100   |
| lives                   | 180100   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 9.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1230795  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.58     |
| episodes                | 180200   |
| lives                   | 180200   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1231614  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.56     |
| episodes                | 180300   |
| lives                   | 180300   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1232419  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.58     |
| episodes                | 180400   |
| lives                   | 180400   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1233240  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.56     |
| episodes                | 180500   |
| lives                   | 180500   |
| mean 100 episode length | 9.46     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1234086  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.55     |
| episodes                | 180600   |
| lives                   | 180600   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1234893  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.53     |
| episodes                | 180700   |
| lives                   | 180700   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 9.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1235724  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.52     |
| episodes                | 180800   |
| lives                   | 180800   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1236538  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.51     |
| episodes                | 180900   |
| lives                   | 180900   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1237350  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.5      |
| episodes                | 181000   |
| lives                   | 181000   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1238164  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.55     |
| episodes                | 181100   |
| lives                   | 181100   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1238975  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.54     |
| episodes                | 181200   |
| lives                   | 181200   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1239789  |
| value_loss              | 2.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.52     |
| episodes                | 181300   |
| lives                   | 181300   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1240588  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.5      |
| episodes                | 181400   |
| lives                   | 181400   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0065  |
| steps                   | 1241395  |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.55     |
| episodes                | 181500   |
| lives                   | 181500   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1242206  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.54     |
| episodes                | 181600   |
| lives                   | 181600   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1243018  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.52     |
| episodes                | 181700   |
| lives                   | 181700   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1243805  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.48     |
| episodes                | 181800   |
| lives                   | 181800   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1244578  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.51     |
| episodes                | 181900   |
| lives                   | 181900   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1245382  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.54     |
| episodes                | 182000   |
| lives                   | 182000   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1246201  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.5      |
| episodes                | 182100   |
| lives                   | 182100   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1247006  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.55     |
| episodes                | 182200   |
| lives                   | 182200   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1247795  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.5      |
| episodes                | 182300   |
| lives                   | 182300   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.42     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1248585  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.54     |
| episodes                | 182400   |
| lives                   | 182400   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 9.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1249398  |
| value_loss              | 1.7      |
--------------------------------------
Saving model due to running mean reward increase: 9.6118 -> 9.9637
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.56     |
| episodes                | 182500   |
| lives                   | 182500   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1250235  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.58     |
| episodes                | 182600   |
| lives                   | 182600   |
| mean 100 episode length | 9.3      |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1251065  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.55     |
| episodes                | 182700   |
| lives                   | 182700   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1251856  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.56     |
| episodes                | 182800   |
| lives                   | 182800   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1252658  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.56     |
| episodes                | 182900   |
| lives                   | 182900   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.27     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1253448  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.55     |
| episodes                | 183000   |
| lives                   | 183000   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1254236  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.5      |
| episodes                | 183100   |
| lives                   | 183100   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.43     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1254989  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.46     |
| episodes                | 183200   |
| lives                   | 183200   |
| mean 100 episode length | 8.53     |
| mean 100 episode reward | 9.21     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1255742  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.46     |
| episodes                | 183300   |
| lives                   | 183300   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1256515  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.49     |
| episodes                | 183400   |
| lives                   | 183400   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1257280  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.48     |
| episodes                | 183500   |
| lives                   | 183500   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1258053  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.56     |
| episodes                | 183600   |
| lives                   | 183600   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1258848  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.58     |
| episodes                | 183700   |
| lives                   | 183700   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1259663  |
| value_loss              | 1.76     |
--------------------------------------
Saving model due to running mean reward increase: 9.5448 -> 10.0031
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.57     |
| episodes                | 183800   |
| lives                   | 183800   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1260462  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.52     |
| episodes                | 183900   |
| lives                   | 183900   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1261243  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.55     |
| episodes                | 184000   |
| lives                   | 184000   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1262027  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.56     |
| episodes                | 184100   |
| lives                   | 184100   |
| mean 100 episode length | 9.1      |
| mean 100 episode reward | 9.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1262837  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.51     |
| episodes                | 184200   |
| lives                   | 184200   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.49     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1263619  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.5      |
| episodes                | 184300   |
| lives                   | 184300   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1264402  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.51     |
| episodes                | 184400   |
| lives                   | 184400   |
| mean 100 episode length | 8.87     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1265189  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.58     |
| episodes                | 184500   |
| lives                   | 184500   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1266017  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.53     |
| episodes                | 184600   |
| lives                   | 184600   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1266831  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0045  |
| entropy                 | 1.54     |
| episodes                | 184700   |
| lives                   | 184700   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1267645  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.52     |
| episodes                | 184800   |
| lives                   | 184800   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1268449  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.51     |
| episodes                | 184900   |
| lives                   | 184900   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1269222  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.45     |
| episodes                | 185000   |
| lives                   | 185000   |
| mean 100 episode length | 8.61     |
| mean 100 episode reward | 9.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1269983  |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.53     |
| episodes                | 185100   |
| lives                   | 185100   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 9.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1270796  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.5      |
| episodes                | 185200   |
| lives                   | 185200   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1271598  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.48     |
| episodes                | 185300   |
| lives                   | 185300   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1272392  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.58     |
| episodes                | 185400   |
| lives                   | 185400   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1273218  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.55     |
| episodes                | 185500   |
| lives                   | 185500   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 9.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1274025  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.45     |
| episodes                | 185600   |
| lives                   | 185600   |
| mean 100 episode length | 8.62     |
| mean 100 episode reward | 9.3      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1274787  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.59     |
| episodes                | 185700   |
| lives                   | 185700   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1275619  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.55     |
| episodes                | 185800   |
| lives                   | 185800   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1276418  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.52     |
| episodes                | 185900   |
| lives                   | 185900   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1277206  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.48     |
| episodes                | 186000   |
| lives                   | 186000   |
| mean 100 episode length | 8.47     |
| mean 100 episode reward | 9.26     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1277953  |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.53     |
| episodes                | 186100   |
| lives                   | 186100   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1278742  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.59     |
| episodes                | 186200   |
| lives                   | 186200   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1279535  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.56     |
| episodes                | 186300   |
| lives                   | 186300   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.31     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0071  |
| steps                   | 1280316  |
| value_loss              | 1.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.54     |
| episodes                | 186400   |
| lives                   | 186400   |
| mean 100 episode length | 8.68     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1281084  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.56     |
| episodes                | 186500   |
| lives                   | 186500   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1281865  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.58     |
| episodes                | 186600   |
| lives                   | 186600   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1282697  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.59     |
| episodes                | 186700   |
| lives                   | 186700   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1283489  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.59     |
| episodes                | 186800   |
| lives                   | 186800   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1284296  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.54     |
| episodes                | 186900   |
| lives                   | 186900   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1285078  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.57     |
| episodes                | 187000   |
| lives                   | 187000   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1285868  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.59     |
| episodes                | 187100   |
| lives                   | 187100   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1286660  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.55     |
| episodes                | 187200   |
| lives                   | 187200   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1287438  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.57     |
| episodes                | 187300   |
| lives                   | 187300   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1288230  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.55     |
| episodes                | 187400   |
| lives                   | 187400   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1289019  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.54     |
| episodes                | 187500   |
| lives                   | 187500   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1289796  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.57     |
| episodes                | 187600   |
| lives                   | 187600   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1290596  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.54     |
| episodes                | 187700   |
| lives                   | 187700   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1291419  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.48     |
| episodes                | 187800   |
| lives                   | 187800   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1292210  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.54     |
| episodes                | 187900   |
| lives                   | 187900   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1293030  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.5      |
| episodes                | 188000   |
| lives                   | 188000   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1293819  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.48     |
| episodes                | 188100   |
| lives                   | 188100   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1294604  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.51     |
| episodes                | 188200   |
| lives                   | 188200   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1295401  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.5      |
| episodes                | 188300   |
| lives                   | 188300   |
| mean 100 episode length | 8.72     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1296173  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.59     |
| episodes                | 188400   |
| lives                   | 188400   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1297008  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.53     |
| episodes                | 188500   |
| lives                   | 188500   |
| mean 100 episode length | 8.82     |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1297790  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.48     |
| episodes                | 188600   |
| lives                   | 188600   |
| mean 100 episode length | 8.78     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1298568  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.55     |
| episodes                | 188700   |
| lives                   | 188700   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1299385  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.56     |
| episodes                | 188800   |
| lives                   | 188800   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1300173  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.5      |
| episodes                | 188900   |
| lives                   | 188900   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1300962  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.55     |
| episodes                | 189000   |
| lives                   | 189000   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 9.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1301767  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.62     |
| episodes                | 189100   |
| lives                   | 189100   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1302596  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.54     |
| episodes                | 189200   |
| lives                   | 189200   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1303410  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.6      |
| episodes                | 189300   |
| lives                   | 189300   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 9.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1304241  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.56     |
| episodes                | 189400   |
| lives                   | 189400   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1305066  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.48     |
| episodes                | 189500   |
| lives                   | 189500   |
| mean 100 episode length | 8.81     |
| mean 100 episode reward | 9.41     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1305847  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.53     |
| episodes                | 189600   |
| lives                   | 189600   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1306658  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.48     |
| episodes                | 189700   |
| lives                   | 189700   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1307443  |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0055  |
| entropy                 | 1.49     |
| episodes                | 189800   |
| lives                   | 189800   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1308218  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.41     |
| episodes                | 189900   |
| lives                   | 189900   |
| mean 100 episode length | 8.73     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1308991  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.51     |
| episodes                | 190000   |
| lives                   | 190000   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 9.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1309808  |
| value_loss              | 1.69     |
--------------------------------------
Saving model due to running mean reward increase: 9.5968 -> 9.7824
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.53     |
| episodes                | 190100   |
| lives                   | 190100   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1310610  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.61     |
| episodes                | 190200   |
| lives                   | 190200   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1311457  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.52     |
| episodes                | 190300   |
| lives                   | 190300   |
| mean 100 episode length | 8.76     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1312233  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.55     |
| episodes                | 190400   |
| lives                   | 190400   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1313026  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.6      |
| episodes                | 190500   |
| lives                   | 190500   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1313850  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.58     |
| episodes                | 190600   |
| lives                   | 190600   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0068  |
| steps                   | 1314690  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.53     |
| episodes                | 190700   |
| lives                   | 190700   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1315519  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.56     |
| episodes                | 190800   |
| lives                   | 190800   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1316330  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.56     |
| episodes                | 190900   |
| lives                   | 190900   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1317120  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.58     |
| episodes                | 191000   |
| lives                   | 191000   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0073  |
| steps                   | 1317914  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.63     |
| episodes                | 191100   |
| lives                   | 191100   |
| mean 100 episode length | 9.18     |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1318732  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.56     |
| episodes                | 191200   |
| lives                   | 191200   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1319511  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.57     |
| episodes                | 191300   |
| lives                   | 191300   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1320305  |
| value_loss              | 1.74     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.63     |
| episodes                | 191400   |
| lives                   | 191400   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1321145  |
| value_loss              | 1.79     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.55     |
| episodes                | 191500   |
| lives                   | 191500   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1321933  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.55     |
| episodes                | 191600   |
| lives                   | 191600   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1322746  |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.54     |
| episodes                | 191700   |
| lives                   | 191700   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 9.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1323551  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.59     |
| episodes                | 191800   |
| lives                   | 191800   |
| mean 100 episode length | 9.1      |
| mean 100 episode reward | 9.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1324361  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.58     |
| episodes                | 191900   |
| lives                   | 191900   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 9.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1325180  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.53     |
| episodes                | 192000   |
| lives                   | 192000   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1325963  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.63     |
| episodes                | 192100   |
| lives                   | 192100   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1326810  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.56     |
| episodes                | 192200   |
| lives                   | 192200   |
| mean 100 episode length | 8.95     |
| mean 100 episode reward | 9.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1327605  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.55     |
| episodes                | 192300   |
| lives                   | 192300   |
| mean 100 episode length | 9.01     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1328406  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.48     |
| episodes                | 192400   |
| lives                   | 192400   |
| mean 100 episode length | 8.52     |
| mean 100 episode reward | 9.32     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1329158  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.54     |
| episodes                | 192500   |
| lives                   | 192500   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1329977  |
| value_loss              | 1.52     |
--------------------------------------
Saving model due to running mean reward increase: 9.3364 -> 9.849
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.53     |
| episodes                | 192600   |
| lives                   | 192600   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1330786  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.54     |
| episodes                | 192700   |
| lives                   | 192700   |
| mean 100 episode length | 9.08     |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1331594  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.59     |
| episodes                | 192800   |
| lives                   | 192800   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1332450  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.57     |
| episodes                | 192900   |
| lives                   | 192900   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 9.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1333278  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.52     |
| episodes                | 193000   |
| lives                   | 193000   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1334090  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.58     |
| episodes                | 193100   |
| lives                   | 193100   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 9.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1334912  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.52     |
| episodes                | 193200   |
| lives                   | 193200   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1335716  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.56     |
| episodes                | 193300   |
| lives                   | 193300   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1336542  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.52     |
| episodes                | 193400   |
| lives                   | 193400   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.7      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1337331  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.57     |
| episodes                | 193500   |
| lives                   | 193500   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1338125  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.54     |
| episodes                | 193600   |
| lives                   | 193600   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.57     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1338925  |
| value_loss              | 1.83     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.55     |
| episodes                | 193700   |
| lives                   | 193700   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1339739  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.6      |
| episodes                | 193800   |
| lives                   | 193800   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1340563  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.57     |
| episodes                | 193900   |
| lives                   | 193900   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 9.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1341394  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.59     |
| episodes                | 194000   |
| lives                   | 194000   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1342223  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.55     |
| episodes                | 194100   |
| lives                   | 194100   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1343038  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.52     |
| episodes                | 194200   |
| lives                   | 194200   |
| mean 100 episode length | 8.79     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0077  |
| steps                   | 1343817  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.51     |
| episodes                | 194300   |
| lives                   | 194300   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1344600  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.5      |
| episodes                | 194400   |
| lives                   | 194400   |
| mean 100 episode length | 8.7      |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1345370  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.53     |
| episodes                | 194500   |
| lives                   | 194500   |
| mean 100 episode length | 8.71     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1346141  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.56     |
| episodes                | 194600   |
| lives                   | 194600   |
| mean 100 episode length | 8.75     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1346916  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.61     |
| episodes                | 194700   |
| lives                   | 194700   |
| mean 100 episode length | 8.98     |
| mean 100 episode reward | 9.45     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1347714  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.58     |
| episodes                | 194800   |
| lives                   | 194800   |
| mean 100 episode length | 8.96     |
| mean 100 episode reward | 9.55     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1348510  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.61     |
| episodes                | 194900   |
| lives                   | 194900   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1349321  |
| value_loss              | 1.71     |
--------------------------------------
Saving model due to running mean reward increase: 9.6771 -> 9.9194
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.61     |
| episodes                | 195000   |
| lives                   | 195000   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 9.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1350140  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.59     |
| episodes                | 195100   |
| lives                   | 195100   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 9.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1350989  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.59     |
| episodes                | 195200   |
| lives                   | 195200   |
| mean 100 episode length | 9.03     |
| mean 100 episode reward | 9.59     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1351792  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.61     |
| episodes                | 195300   |
| lives                   | 195300   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 9.66     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1352615  |
| value_loss              | 1.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.61     |
| episodes                | 195400   |
| lives                   | 195400   |
| mean 100 episode length | 9.03     |
| mean 100 episode reward | 9.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1353418  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.64     |
| episodes                | 195500   |
| lives                   | 195500   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1354238  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.63     |
| episodes                | 195600   |
| lives                   | 195600   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1355050  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.66     |
| episodes                | 195700   |
| lives                   | 195700   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1355898  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.61     |
| episodes                | 195800   |
| lives                   | 195800   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1356711  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.66     |
| episodes                | 195900   |
| lives                   | 195900   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1357540  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.65     |
| episodes                | 196000   |
| lives                   | 196000   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 9.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1358389  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.57     |
| episodes                | 196100   |
| lives                   | 196100   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.46     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1359174  |
| value_loss              | 1.55     |
--------------------------------------
Saving model due to running mean reward increase: 9.3985 -> 10.2247
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.66     |
| episodes                | 196200   |
| lives                   | 196200   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1360033  |
| value_loss              | 2        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.57     |
| episodes                | 196300   |
| lives                   | 196300   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.54     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1360818  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.63     |
| episodes                | 196400   |
| lives                   | 196400   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1361660  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.6      |
| episodes                | 196500   |
| lives                   | 196500   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0074  |
| steps                   | 1362486  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.64     |
| episodes                | 196600   |
| lives                   | 196600   |
| mean 100 episode length | 9.46     |
| mean 100 episode reward | 9.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1363332  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.6      |
| episodes                | 196700   |
| lives                   | 196700   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 9.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1364159  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.6      |
| episodes                | 196800   |
| lives                   | 196800   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1364970  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.62     |
| episodes                | 196900   |
| lives                   | 196900   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 9.75     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1365803  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0056  |
| entropy                 | 1.53     |
| episodes                | 197000   |
| lives                   | 197000   |
| mean 100 episode length | 8.77     |
| mean 100 episode reward | 9.18     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0078  |
| steps                   | 1366580  |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.59     |
| episodes                | 197100   |
| lives                   | 197100   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1367403  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.6      |
| episodes                | 197200   |
| lives                   | 197200   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1368225  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.6      |
| episodes                | 197300   |
| lives                   | 197300   |
| mean 100 episode length | 8.83     |
| mean 100 episode reward | 9.44     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1369008  |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.55     |
| episodes                | 197400   |
| lives                   | 197400   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.53     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1369797  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.56     |
| episodes                | 197500   |
| lives                   | 197500   |
| mean 100 episode length | 8.74     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1370571  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.56     |
| episodes                | 197600   |
| lives                   | 197600   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1371402  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.56     |
| episodes                | 197700   |
| lives                   | 197700   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1372201  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.57     |
| episodes                | 197800   |
| lives                   | 197800   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.98     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1373027  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.58     |
| episodes                | 197900   |
| lives                   | 197900   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.6      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1373820  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.6      |
| episodes                | 198000   |
| lives                   | 198000   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1374640  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.55     |
| episodes                | 198100   |
| lives                   | 198100   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.63     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1375437  |
| value_loss              | 2.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.62     |
| episodes                | 198200   |
| lives                   | 198200   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1376263  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.62     |
| episodes                | 198300   |
| lives                   | 198300   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1377084  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.57     |
| episodes                | 198400   |
| lives                   | 198400   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1377909  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.6      |
| episodes                | 198500   |
| lives                   | 198500   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1378752  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.6      |
| episodes                | 198600   |
| lives                   | 198600   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 9.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1379574  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.61     |
| episodes                | 198700   |
| lives                   | 198700   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 9.68     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1380394  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.58     |
| episodes                | 198800   |
| lives                   | 198800   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1381217  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.58     |
| episodes                | 198900   |
| lives                   | 198900   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1382014  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.6      |
| episodes                | 199000   |
| lives                   | 199000   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1382849  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.52     |
| episodes                | 199100   |
| lives                   | 199100   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.65     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1383637  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.51     |
| episodes                | 199200   |
| lives                   | 199200   |
| mean 100 episode length | 8.91     |
| mean 100 episode reward | 9.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1384428  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.52     |
| episodes                | 199300   |
| lives                   | 199300   |
| mean 100 episode length | 8.9      |
| mean 100 episode reward | 9.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1385218  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.51     |
| episodes                | 199400   |
| lives                   | 199400   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1386045  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.55     |
| episodes                | 199500   |
| lives                   | 199500   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 9.83     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1386864  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.55     |
| episodes                | 199600   |
| lives                   | 199600   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1387709  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.56     |
| episodes                | 199700   |
| lives                   | 199700   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 9.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1388530  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | 0.0002   |
| entropy                 | 1.53     |
| episodes                | 199800   |
| lives                   | 199800   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1389373  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.57     |
| episodes                | 199900   |
| lives                   | 199900   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 9.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1390213  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.6      |
| episodes                | 200000   |
| lives                   | 200000   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1391048  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.55     |
| episodes                | 200100   |
| lives                   | 200100   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1391874  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.62     |
| episodes                | 200200   |
| lives                   | 200200   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1392733  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.59     |
| episodes                | 200300   |
| lives                   | 200300   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 9.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0076  |
| steps                   | 1393580  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.57     |
| episodes                | 200400   |
| lives                   | 200400   |
| mean 100 episode length | 9.16     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1394396  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.62     |
| episodes                | 200500   |
| lives                   | 200500   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1395247  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0047  |
| entropy                 | 1.59     |
| episodes                | 200600   |
| lives                   | 200600   |
| mean 100 episode length | 9.07     |
| mean 100 episode reward | 9.72     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1396054  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.65     |
| episodes                | 200700   |
| lives                   | 200700   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 9.97     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1396906  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.53     |
| episodes                | 200800   |
| lives                   | 200800   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.25     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1397694  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.57     |
| episodes                | 200900   |
| lives                   | 200900   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.58     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0063  |
| steps                   | 1398494  |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.6      |
| episodes                | 201000   |
| lives                   | 201000   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1399317  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.57     |
| episodes                | 201100   |
| lives                   | 201100   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 9.69     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1400140  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.58     |
| episodes                | 201200   |
| lives                   | 201200   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 9.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1400962  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.57     |
| episodes                | 201300   |
| lives                   | 201300   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1401800  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.61     |
| episodes                | 201400   |
| lives                   | 201400   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1402637  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.58     |
| episodes                | 201500   |
| lives                   | 201500   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 9.84     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1403469  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.61     |
| episodes                | 201600   |
| lives                   | 201600   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1404344  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.61     |
| episodes                | 201700   |
| lives                   | 201700   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1405216  |
| value_loss              | 1.86     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.54     |
| episodes                | 201800   |
| lives                   | 201800   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1406028  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.58     |
| episodes                | 201900   |
| lives                   | 201900   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1406865  |
| value_loss              | 1.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.52     |
| episodes                | 202000   |
| lives                   | 202000   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1407662  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.48     |
| episodes                | 202100   |
| lives                   | 202100   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.67     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1408461  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.49     |
| episodes                | 202200   |
| lives                   | 202200   |
| mean 100 episode length | 9.1      |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1409271  |
| value_loss              | 1.79     |
--------------------------------------
Saving model due to running mean reward increase: 9.8924 -> 9.8961
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.5      |
| episodes                | 202300   |
| lives                   | 202300   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1410095  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.54     |
| episodes                | 202400   |
| lives                   | 202400   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1410955  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.51     |
| episodes                | 202500   |
| lives                   | 202500   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1411791  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.51     |
| episodes                | 202600   |
| lives                   | 202600   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1412625  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.45     |
| episodes                | 202700   |
| lives                   | 202700   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 9.71     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1413448  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.47     |
| episodes                | 202800   |
| lives                   | 202800   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1414262  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.48     |
| episodes                | 202900   |
| lives                   | 202900   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1415101  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.52     |
| episodes                | 203000   |
| lives                   | 203000   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1415939  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.55     |
| episodes                | 203100   |
| lives                   | 203100   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1416803  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.51     |
| episodes                | 203200   |
| lives                   | 203200   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1417629  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.45     |
| episodes                | 203300   |
| lives                   | 203300   |
| mean 100 episode length | 8.93     |
| mean 100 episode reward | 9.51     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1418422  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.53     |
| episodes                | 203400   |
| lives                   | 203400   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 9.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1419279  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.51     |
| episodes                | 203500   |
| lives                   | 203500   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 9.56     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1420096  |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.53     |
| episodes                | 203600   |
| lives                   | 203600   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1420930  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.51     |
| episodes                | 203700   |
| lives                   | 203700   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 9.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1421766  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.52     |
| episodes                | 203800   |
| lives                   | 203800   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1422592  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.51     |
| episodes                | 203900   |
| lives                   | 203900   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1423421  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.44     |
| episodes                | 204000   |
| lives                   | 204000   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1424210  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.45     |
| episodes                | 204100   |
| lives                   | 204100   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.73     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1425012  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.54     |
| episodes                | 204200   |
| lives                   | 204200   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1425862  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.52     |
| episodes                | 204300   |
| lives                   | 204300   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 9.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1426687  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.49     |
| episodes                | 204400   |
| lives                   | 204400   |
| mean 100 episode length | 9.03     |
| mean 100 episode reward | 9.79     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1427490  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.5      |
| episodes                | 204500   |
| lives                   | 204500   |
| mean 100 episode length | 9.04     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1428294  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.46     |
| episodes                | 204600   |
| lives                   | 204600   |
| mean 100 episode length | 9.03     |
| mean 100 episode reward | 9.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1429097  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.46     |
| episodes                | 204700   |
| lives                   | 204700   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.86     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1429911  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.52     |
| episodes                | 204800   |
| lives                   | 204800   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1430728  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.48     |
| episodes                | 204900   |
| lives                   | 204900   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1431537  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.48     |
| episodes                | 205000   |
| lives                   | 205000   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1432350  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.52     |
| episodes                | 205100   |
| lives                   | 205100   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1433188  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.5      |
| episodes                | 205200   |
| lives                   | 205200   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1434028  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.5      |
| episodes                | 205300   |
| lives                   | 205300   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1434842  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.57     |
| episodes                | 205400   |
| lives                   | 205400   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 9.95     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1435676  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.55     |
| episodes                | 205500   |
| lives                   | 205500   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1436518  |
| value_loss              | 2.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.51     |
| episodes                | 205600   |
| lives                   | 205600   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 9.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1437360  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.42     |
| episodes                | 205700   |
| lives                   | 205700   |
| mean 100 episode length | 8.85     |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1438145  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.46     |
| episodes                | 205800   |
| lives                   | 205800   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1438945  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.45     |
| episodes                | 205900   |
| lives                   | 205900   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1439773  |
| value_loss              | 1.63     |
--------------------------------------
Saving model due to running mean reward increase: 9.7702 -> 9.9735
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.45     |
| episodes                | 206000   |
| lives                   | 206000   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 9.81     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1440587  |
| value_loss              | 1.88     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.45     |
| episodes                | 206100   |
| lives                   | 206100   |
| mean 100 episode length | 9.18     |
| mean 100 episode reward | 9.99     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1441405  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.53     |
| episodes                | 206200   |
| lives                   | 206200   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1442260  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.46     |
| episodes                | 206300   |
| lives                   | 206300   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1443099  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.5      |
| episodes                | 206400   |
| lives                   | 206400   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1443938  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.49     |
| episodes                | 206500   |
| lives                   | 206500   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1444764  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.47     |
| episodes                | 206600   |
| lives                   | 206600   |
| mean 100 episode length | 8.98     |
| mean 100 episode reward | 9.8      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1445562  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.49     |
| episodes                | 206700   |
| lives                   | 206700   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 9.9      |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1446385  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.53     |
| episodes                | 206800   |
| lives                   | 206800   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1447226  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.43     |
| episodes                | 206900   |
| lives                   | 206900   |
| mean 100 episode length | 9.12     |
| mean 100 episode reward | 9.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1448038  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.48     |
| episodes                | 207000   |
| lives                   | 207000   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1448862  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.49     |
| episodes                | 207100   |
| lives                   | 207100   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1449713  |
| value_loss              | 1.28     |
--------------------------------------
Saving model due to running mean reward increase: 10.1838 -> 10.2332
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.41     |
| episodes                | 207200   |
| lives                   | 207200   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.91     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1450513  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.45     |
| episodes                | 207300   |
| lives                   | 207300   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1451341  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.42     |
| episodes                | 207400   |
| lives                   | 207400   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.96     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1452140  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.47     |
| episodes                | 207500   |
| lives                   | 207500   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1452963  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.35     |
| episodes                | 207600   |
| lives                   | 207600   |
| mean 100 episode length | 8.65     |
| mean 100 episode reward | 9.64     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1453728  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.42     |
| episodes                | 207700   |
| lives                   | 207700   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1454545  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.43     |
| episodes                | 207800   |
| lives                   | 207800   |
| mean 100 episode length | 8.89     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1455334  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.44     |
| episodes                | 207900   |
| lives                   | 207900   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1456169  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.47     |
| episodes                | 208000   |
| lives                   | 208000   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 9.88     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1456990  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.46     |
| episodes                | 208100   |
| lives                   | 208100   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1457825  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.45     |
| episodes                | 208200   |
| lives                   | 208200   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1458653  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.48     |
| episodes                | 208300   |
| lives                   | 208300   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1459487  |
| value_loss              | 1.56     |
--------------------------------------
Saving model due to running mean reward increase: 9.9017 -> 10.1256
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.46     |
| episodes                | 208400   |
| lives                   | 208400   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1460327  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.48     |
| episodes                | 208500   |
| lives                   | 208500   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1461168  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.38     |
| episodes                | 208600   |
| lives                   | 208600   |
| mean 100 episode length | 8.94     |
| mean 100 episode reward | 9.85     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1461962  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.4      |
| episodes                | 208700   |
| lives                   | 208700   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1462773  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.42     |
| episodes                | 208800   |
| lives                   | 208800   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1463593  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.42     |
| episodes                | 208900   |
| lives                   | 208900   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1464426  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.46     |
| episodes                | 209000   |
| lives                   | 209000   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1465259  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.43     |
| episodes                | 209100   |
| lives                   | 209100   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1466085  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.41     |
| episodes                | 209200   |
| lives                   | 209200   |
| mean 100 episode length | 9        |
| mean 100 episode reward | 9.82     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1466885  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.48     |
| episodes                | 209300   |
| lives                   | 209300   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1467736  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.42     |
| episodes                | 209400   |
| lives                   | 209400   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1468563  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.45     |
| episodes                | 209500   |
| lives                   | 209500   |
| mean 100 episode length | 9.1      |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1469373  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.46     |
| episodes                | 209600   |
| lives                   | 209600   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.89     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1470188  |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.57     |
| episodes                | 209700   |
| lives                   | 209700   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1471057  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.46     |
| episodes                | 209800   |
| lives                   | 209800   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 9.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1471872  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.4      |
| episodes                | 209900   |
| lives                   | 209900   |
| mean 100 episode length | 8.84     |
| mean 100 episode reward | 9.76     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1472656  |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.48     |
| episodes                | 210000   |
| lives                   | 210000   |
| mean 100 episode length | 9.16     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1473472  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.47     |
| episodes                | 210100   |
| lives                   | 210100   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 9.77     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1474281  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.44     |
| episodes                | 210200   |
| lives                   | 210200   |
| mean 100 episode length | 9.1      |
| mean 100 episode reward | 9.87     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1475091  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.51     |
| episodes                | 210300   |
| lives                   | 210300   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1475913  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.48     |
| episodes                | 210400   |
| lives                   | 210400   |
| mean 100 episode length | 8.92     |
| mean 100 episode reward | 9.47     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1476705  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.49     |
| episodes                | 210500   |
| lives                   | 210500   |
| mean 100 episode length | 8.8      |
| mean 100 episode reward | 9.62     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1477485  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.47     |
| episodes                | 210600   |
| lives                   | 210600   |
| mean 100 episode length | 8.97     |
| mean 100 episode reward | 9.78     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1478282  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.46     |
| episodes                | 210700   |
| lives                   | 210700   |
| mean 100 episode length | 9.08     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1479090  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.47     |
| episodes                | 210800   |
| lives                   | 210800   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1479919  |
| value_loss              | 1.43     |
--------------------------------------
Saving model due to running mean reward increase: 9.9998 -> 10.0717
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.42     |
| episodes                | 210900   |
| lives                   | 210900   |
| mean 100 episode length | 9.17     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1480736  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.45     |
| episodes                | 211000   |
| lives                   | 211000   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1481560  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.41     |
| episodes                | 211100   |
| lives                   | 211100   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1482380  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.42     |
| episodes                | 211200   |
| lives                   | 211200   |
| mean 100 episode length | 8.98     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1483178  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.46     |
| episodes                | 211300   |
| lives                   | 211300   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1484015  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.51     |
| episodes                | 211400   |
| lives                   | 211400   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1484862  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.41     |
| episodes                | 211500   |
| lives                   | 211500   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1485664  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.47     |
| episodes                | 211600   |
| lives                   | 211600   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1486491  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.5      |
| episodes                | 211700   |
| lives                   | 211700   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1487340  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.49     |
| episodes                | 211800   |
| lives                   | 211800   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1488171  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.49     |
| episodes                | 211900   |
| lives                   | 211900   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1489019  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.42     |
| episodes                | 212000   |
| lives                   | 212000   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1489838  |
| value_loss              | 1.37     |
--------------------------------------
Saving model due to running mean reward increase: 10.1573 -> 10.2126
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.5      |
| episodes                | 212100   |
| lives                   | 212100   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1490694  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.47     |
| episodes                | 212200   |
| lives                   | 212200   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1491541  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.43     |
| episodes                | 212300   |
| lives                   | 212300   |
| mean 100 episode length | 8.88     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1492329  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.45     |
| episodes                | 212400   |
| lives                   | 212400   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1493164  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.51     |
| episodes                | 212500   |
| lives                   | 212500   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1494011  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.47     |
| episodes                | 212600   |
| lives                   | 212600   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1494849  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.47     |
| episodes                | 212700   |
| lives                   | 212700   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1495674  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.41     |
| episodes                | 212800   |
| lives                   | 212800   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1496493  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.4      |
| episodes                | 212900   |
| lives                   | 212900   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.007   |
| steps                   | 1497308  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.4      |
| episodes                | 213000   |
| lives                   | 213000   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1498139  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.42     |
| episodes                | 213100   |
| lives                   | 213100   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1498960  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.41     |
| episodes                | 213200   |
| lives                   | 213200   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1499793  |
| value_loss              | 1.59     |
--------------------------------------
Saving model due to running mean reward increase: 10.0898 -> 10.1966
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.41     |
| episodes                | 213300   |
| lives                   | 213300   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1500615  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.49     |
| episodes                | 213400   |
| lives                   | 213400   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1501474  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.43     |
| episodes                | 213500   |
| lives                   | 213500   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1502298  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.41     |
| episodes                | 213600   |
| lives                   | 213600   |
| mean 100 episode length | 9.06     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1503104  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.42     |
| episodes                | 213700   |
| lives                   | 213700   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1503946  |
| value_loss              | 1.89     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.45     |
| episodes                | 213800   |
| lives                   | 213800   |
| mean 100 episode length | 9.22     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1504768  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.45     |
| episodes                | 213900   |
| lives                   | 213900   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1505610  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.42     |
| episodes                | 214000   |
| lives                   | 214000   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1506435  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.42     |
| episodes                | 214100   |
| lives                   | 214100   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1507276  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.43     |
| episodes                | 214200   |
| lives                   | 214200   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1508121  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.43     |
| episodes                | 214300   |
| lives                   | 214300   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 9.94     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0064  |
| steps                   | 1508946  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.004   |
| entropy                 | 1.38     |
| episodes                | 214400   |
| lives                   | 214400   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1509770  |
| value_loss              | 1.15     |
--------------------------------------
Saving model due to running mean reward increase: 9.9801 -> 10.0444
--------------------------------------
| approx_kl               | -0.0049  |
| entropy                 | 1.35     |
| episodes                | 214500   |
| lives                   | 214500   |
| mean 100 episode length | 9.15     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1510585  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.4      |
| episodes                | 214600   |
| lives                   | 214600   |
| mean 100 episode length | 9.3      |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1511415  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.37     |
| episodes                | 214700   |
| lives                   | 214700   |
| mean 100 episode length | 9.11     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1512226  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.43     |
| episodes                | 214800   |
| lives                   | 214800   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1513084  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.41     |
| episodes                | 214900   |
| lives                   | 214900   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1513912  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.42     |
| episodes                | 215000   |
| lives                   | 215000   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1514750  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.47     |
| episodes                | 215100   |
| lives                   | 215100   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1515622  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.5      |
| episodes                | 215200   |
| lives                   | 215200   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1516514  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.4      |
| episodes                | 215300   |
| lives                   | 215300   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1517350  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.38     |
| episodes                | 215400   |
| lives                   | 215400   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1518177  |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.36     |
| episodes                | 215500   |
| lives                   | 215500   |
| mean 100 episode length | 8.99     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1518976  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.42     |
| episodes                | 215600   |
| lives                   | 215600   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1519826  |
| value_loss              | 1.27     |
--------------------------------------
Saving model due to mean reward increase: 10.2663 -> 10.6344
Saving model due to running mean reward increase: 10.0533 -> 10.6344
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.47     |
| episodes                | 215700   |
| lives                   | 215700   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1520714  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.48     |
| episodes                | 215800   |
| lives                   | 215800   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1521582  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.43     |
| episodes                | 215900   |
| lives                   | 215900   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1522465  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.37     |
| episodes                | 216000   |
| lives                   | 216000   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1523309  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.4      |
| episodes                | 216100   |
| lives                   | 216100   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1524143  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.37     |
| episodes                | 216200   |
| lives                   | 216200   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1524972  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.47     |
| episodes                | 216300   |
| lives                   | 216300   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1525840  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.39     |
| episodes                | 216400   |
| lives                   | 216400   |
| mean 100 episode length | 9.18     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1526658  |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.44     |
| episodes                | 216500   |
| lives                   | 216500   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1527522  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.43     |
| episodes                | 216600   |
| lives                   | 216600   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1528355  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.38     |
| episodes                | 216700   |
| lives                   | 216700   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1529199  |
| value_loss              | 1.69     |
--------------------------------------
Saving model due to running mean reward increase: 10.2533 -> 10.4452
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.46     |
| episodes                | 216800   |
| lives                   | 216800   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1530068  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.39     |
| episodes                | 216900   |
| lives                   | 216900   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1530910  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.49     |
| episodes                | 217000   |
| lives                   | 217000   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1531805  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.39     |
| episodes                | 217100   |
| lives                   | 217100   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1532642  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.37     |
| episodes                | 217200   |
| lives                   | 217200   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0057  |
| steps                   | 1533466  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.41     |
| episodes                | 217300   |
| lives                   | 217300   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1534287  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.39     |
| episodes                | 217400   |
| lives                   | 217400   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1535131  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.37     |
| episodes                | 217500   |
| lives                   | 217500   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1535950  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.37     |
| episodes                | 217600   |
| lives                   | 217600   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1536778  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.38     |
| episodes                | 217700   |
| lives                   | 217700   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1537599  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.39     |
| episodes                | 217800   |
| lives                   | 217800   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1538438  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.39     |
| episodes                | 217900   |
| lives                   | 217900   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1539295  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.32     |
| episodes                | 218000   |
| lives                   | 218000   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1540097  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.34     |
| episodes                | 218100   |
| lives                   | 218100   |
| mean 100 episode length | 9.25     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1540922  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.33     |
| episodes                | 218200   |
| lives                   | 218200   |
| mean 100 episode length | 9.14     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1541736  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.32     |
| episodes                | 218300   |
| lives                   | 218300   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1542557  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.36     |
| episodes                | 218400   |
| lives                   | 218400   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1543399  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.37     |
| episodes                | 218500   |
| lives                   | 218500   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1544234  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.4      |
| episodes                | 218600   |
| lives                   | 218600   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1545091  |
| value_loss              | 1.78     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.44     |
| episodes                | 218700   |
| lives                   | 218700   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1545969  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.45     |
| episodes                | 218800   |
| lives                   | 218800   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1546821  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.37     |
| episodes                | 218900   |
| lives                   | 218900   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 9.74     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1547623  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.45     |
| episodes                | 219000   |
| lives                   | 219000   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1548481  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.41     |
| episodes                | 219100   |
| lives                   | 219100   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1549324  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.38     |
| episodes                | 219200   |
| lives                   | 219200   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1550169  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.38     |
| episodes                | 219300   |
| lives                   | 219300   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1551008  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.37     |
| episodes                | 219400   |
| lives                   | 219400   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1551827  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.44     |
| episodes                | 219500   |
| lives                   | 219500   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0067  |
| steps                   | 1552708  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.35     |
| episodes                | 219600   |
| lives                   | 219600   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1553553  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.37     |
| episodes                | 219700   |
| lives                   | 219700   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1554390  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.41     |
| episodes                | 219800   |
| lives                   | 219800   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1555265  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.43     |
| episodes                | 219900   |
| lives                   | 219900   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1556102  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.43     |
| episodes                | 220000   |
| lives                   | 220000   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1556952  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.35     |
| episodes                | 220100   |
| lives                   | 220100   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1557775  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.43     |
| episodes                | 220200   |
| lives                   | 220200   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1558645  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.44     |
| episodes                | 220300   |
| lives                   | 220300   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1559512  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.41     |
| episodes                | 220400   |
| lives                   | 220400   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1560356  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.36     |
| episodes                | 220500   |
| lives                   | 220500   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1561196  |
| value_loss              | 1.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.44     |
| episodes                | 220600   |
| lives                   | 220600   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1562078  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.4      |
| episodes                | 220700   |
| lives                   | 220700   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1562917  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.39     |
| episodes                | 220800   |
| lives                   | 220800   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1563779  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.35     |
| episodes                | 220900   |
| lives                   | 220900   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1564620  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.37     |
| episodes                | 221000   |
| lives                   | 221000   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1565458  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.38     |
| episodes                | 221100   |
| lives                   | 221100   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1566294  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.42     |
| episodes                | 221200   |
| lives                   | 221200   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1567170  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.39     |
| episodes                | 221300   |
| lives                   | 221300   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1568011  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.42     |
| episodes                | 221400   |
| lives                   | 221400   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1568868  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.43     |
| episodes                | 221500   |
| lives                   | 221500   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1569735  |
| value_loss              | 1.43     |
--------------------------------------
Saving model due to running mean reward increase: 10.5236 -> 10.6129
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.45     |
| episodes                | 221600   |
| lives                   | 221600   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1570604  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.39     |
| episodes                | 221700   |
| lives                   | 221700   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1571454  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.35     |
| episodes                | 221800   |
| lives                   | 221800   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1572283  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.38     |
| episodes                | 221900   |
| lives                   | 221900   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1573130  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.33     |
| episodes                | 222000   |
| lives                   | 222000   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 9.93     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1573957  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.35     |
| episodes                | 222100   |
| lives                   | 222100   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1574780  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.39     |
| episodes                | 222200   |
| lives                   | 222200   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1575621  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.32     |
| episodes                | 222300   |
| lives                   | 222300   |
| mean 100 episode length | 9.09     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1576430  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.38     |
| episodes                | 222400   |
| lives                   | 222400   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1577272  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.4      |
| episodes                | 222500   |
| lives                   | 222500   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1578134  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.4      |
| episodes                | 222600   |
| lives                   | 222600   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1578995  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.37     |
| episodes                | 222700   |
| lives                   | 222700   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1579839  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.39     |
| episodes                | 222800   |
| lives                   | 222800   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1580691  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.34     |
| episodes                | 222900   |
| lives                   | 222900   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1581532  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.41     |
| episodes                | 223000   |
| lives                   | 223000   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1582388  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.38     |
| episodes                | 223100   |
| lives                   | 223100   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1583219  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.41     |
| episodes                | 223200   |
| lives                   | 223200   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1584066  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.38     |
| episodes                | 223300   |
| lives                   | 223300   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1584918  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.35     |
| episodes                | 223400   |
| lives                   | 223400   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1585754  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.35     |
| episodes                | 223500   |
| lives                   | 223500   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1586594  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.42     |
| episodes                | 223600   |
| lives                   | 223600   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1587478  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.35     |
| episodes                | 223700   |
| lives                   | 223700   |
| mean 100 episode length | 9.2      |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1588298  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.36     |
| episodes                | 223800   |
| lives                   | 223800   |
| mean 100 episode length | 9.37     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1589135  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.39     |
| episodes                | 223900   |
| lives                   | 223900   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1589968  |
| value_loss              | 1.28     |
--------------------------------------
Saving model due to running mean reward increase: 10.0602 -> 10.2467
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.42     |
| episodes                | 224000   |
| lives                   | 224000   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1590843  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.34     |
| episodes                | 224100   |
| lives                   | 224100   |
| mean 100 episode length | 9.19     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1591662  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 1.4      |
| episodes                | 224200   |
| lives                   | 224200   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1592500  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.45     |
| episodes                | 224300   |
| lives                   | 224300   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1593377  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.44     |
| episodes                | 224400   |
| lives                   | 224400   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1594245  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.38     |
| episodes                | 224500   |
| lives                   | 224500   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1595071  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.43     |
| episodes                | 224600   |
| lives                   | 224600   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1595915  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.39     |
| episodes                | 224700   |
| lives                   | 224700   |
| mean 100 episode length | 9.16     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1596731  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.41     |
| episodes                | 224800   |
| lives                   | 224800   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1597566  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.4      |
| episodes                | 224900   |
| lives                   | 224900   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1598405  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.46     |
| episodes                | 225000   |
| lives                   | 225000   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1599278  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.37     |
| episodes                | 225100   |
| lives                   | 225100   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1600120  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.42     |
| episodes                | 225200   |
| lives                   | 225200   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1600970  |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.39     |
| episodes                | 225300   |
| lives                   | 225300   |
| mean 100 episode length | 9.3      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1601800  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.43     |
| episodes                | 225400   |
| lives                   | 225400   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1602650  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.42     |
| episodes                | 225500   |
| lives                   | 225500   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1603512  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.4      |
| episodes                | 225600   |
| lives                   | 225600   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1604359  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.38     |
| episodes                | 225700   |
| lives                   | 225700   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 9.92     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1605182  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.39     |
| episodes                | 225800   |
| lives                   | 225800   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1606006  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.37     |
| episodes                | 225900   |
| lives                   | 225900   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.006   |
| steps                   | 1606845  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.37     |
| episodes                | 226000   |
| lives                   | 226000   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1607690  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.41     |
| episodes                | 226100   |
| lives                   | 226100   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1608542  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.41     |
| episodes                | 226200   |
| lives                   | 226200   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1609403  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.38     |
| episodes                | 226300   |
| lives                   | 226300   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1610235  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.36     |
| episodes                | 226400   |
| lives                   | 226400   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1611069  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.38     |
| episodes                | 226500   |
| lives                   | 226500   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1611917  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.31     |
| episodes                | 226600   |
| lives                   | 226600   |
| mean 100 episode length | 9.3      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1612747  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.38     |
| episodes                | 226700   |
| lives                   | 226700   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1613598  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.33     |
| episodes                | 226800   |
| lives                   | 226800   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1614432  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.34     |
| episodes                | 226900   |
| lives                   | 226900   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1615298  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.36     |
| episodes                | 227000   |
| lives                   | 227000   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1616163  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.36     |
| episodes                | 227100   |
| lives                   | 227100   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1617012  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.38     |
| episodes                | 227200   |
| lives                   | 227200   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1617884  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.37     |
| episodes                | 227300   |
| lives                   | 227300   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0069  |
| steps                   | 1618735  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.36     |
| episodes                | 227400   |
| lives                   | 227400   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1619606  |
| value_loss              | 1.46     |
--------------------------------------
Saving model due to mean reward increase: 10.6344 -> 10.8058
Saving model due to running mean reward increase: 10.4066 -> 10.8058
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.34     |
| episodes                | 227500   |
| lives                   | 227500   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1620444  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.36     |
| episodes                | 227600   |
| lives                   | 227600   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1621295  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.31     |
| episodes                | 227700   |
| lives                   | 227700   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1622131  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.35     |
| episodes                | 227800   |
| lives                   | 227800   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1622996  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.32     |
| episodes                | 227900   |
| lives                   | 227900   |
| mean 100 episode length | 9.36     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1623832  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.32     |
| episodes                | 228000   |
| lives                   | 228000   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1624667  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.34     |
| episodes                | 228100   |
| lives                   | 228100   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1625525  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.34     |
| episodes                | 228200   |
| lives                   | 228200   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1626352  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0043  |
| entropy                 | 1.34     |
| episodes                | 228300   |
| lives                   | 228300   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0058  |
| steps                   | 1627217  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.33     |
| episodes                | 228400   |
| lives                   | 228400   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1628055  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.4      |
| episodes                | 228500   |
| lives                   | 228500   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1628926  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.34     |
| episodes                | 228600   |
| lives                   | 228600   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1629766  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.33     |
| episodes                | 228700   |
| lives                   | 228700   |
| mean 100 episode length | 9.23     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1630589  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.33     |
| episodes                | 228800   |
| lives                   | 228800   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1631432  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.36     |
| episodes                | 228900   |
| lives                   | 228900   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1632293  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.36     |
| episodes                | 229000   |
| lives                   | 229000   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1633140  |
| value_loss              | 1.82     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.41     |
| episodes                | 229100   |
| lives                   | 229100   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1634028  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.41     |
| episodes                | 229200   |
| lives                   | 229200   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1634899  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.35     |
| episodes                | 229300   |
| lives                   | 229300   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1635767  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.31     |
| episodes                | 229400   |
| lives                   | 229400   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1636622  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.31     |
| episodes                | 229500   |
| lives                   | 229500   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1637465  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.3      |
| episodes                | 229600   |
| lives                   | 229600   |
| mean 100 episode length | 9.02     |
| mean 100 episode reward | 10       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1638267  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.4      |
| episodes                | 229700   |
| lives                   | 229700   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1639156  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.37     |
| episodes                | 229800   |
| lives                   | 229800   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1640011  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.32     |
| episodes                | 229900   |
| lives                   | 229900   |
| mean 100 episode length | 9.31     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1640842  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.38     |
| episodes                | 230000   |
| lives                   | 230000   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1641719  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.37     |
| episodes                | 230100   |
| lives                   | 230100   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1642574  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.32     |
| episodes                | 230200   |
| lives                   | 230200   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1643412  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.34     |
| episodes                | 230300   |
| lives                   | 230300   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1644238  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.35     |
| episodes                | 230400   |
| lives                   | 230400   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1645089  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.4      |
| episodes                | 230500   |
| lives                   | 230500   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1645962  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.43     |
| episodes                | 230600   |
| lives                   | 230600   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1646846  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.41     |
| episodes                | 230700   |
| lives                   | 230700   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1647708  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.4      |
| episodes                | 230800   |
| lives                   | 230800   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1648581  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.35     |
| episodes                | 230900   |
| lives                   | 230900   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1649437  |
| value_loss              | 1.63     |
--------------------------------------
Saving model due to running mean reward increase: 10.5803 -> 10.734
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.49     |
| episodes                | 231000   |
| lives                   | 231000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1650348  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.33     |
| episodes                | 231100   |
| lives                   | 231100   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1651195  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.35     |
| episodes                | 231200   |
| lives                   | 231200   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1652036  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.32     |
| episodes                | 231300   |
| lives                   | 231300   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1652870  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.37     |
| episodes                | 231400   |
| lives                   | 231400   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1653729  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.42     |
| episodes                | 231500   |
| lives                   | 231500   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1654627  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.35     |
| episodes                | 231600   |
| lives                   | 231600   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1655486  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.38     |
| episodes                | 231700   |
| lives                   | 231700   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1656349  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.38     |
| episodes                | 231800   |
| lives                   | 231800   |
| mean 100 episode length | 9.46     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0047  |
| steps                   | 1657195  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.37     |
| episodes                | 231900   |
| lives                   | 231900   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1658037  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.4      |
| episodes                | 232000   |
| lives                   | 232000   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1658899  |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.39     |
| episodes                | 232100   |
| lives                   | 232100   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1659737  |
| value_loss              | 1.49     |
--------------------------------------
Saving model due to running mean reward increase: 10.3914 -> 10.5403
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.4      |
| episodes                | 232200   |
| lives                   | 232200   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1660616  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.41     |
| episodes                | 232300   |
| lives                   | 232300   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0055  |
| steps                   | 1661491  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.43     |
| episodes                | 232400   |
| lives                   | 232400   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1662365  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.35     |
| episodes                | 232500   |
| lives                   | 232500   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1663216  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.35     |
| episodes                | 232600   |
| lives                   | 232600   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1664048  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.3      |
| episodes                | 232700   |
| lives                   | 232700   |
| mean 100 episode length | 9.13     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1664861  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.39     |
| episodes                | 232800   |
| lives                   | 232800   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1665722  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.4      |
| episodes                | 232900   |
| lives                   | 232900   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1666592  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.37     |
| episodes                | 233000   |
| lives                   | 233000   |
| mean 100 episode length | 9.46     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1667438  |
| value_loss              | 1.72     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.4      |
| episodes                | 233100   |
| lives                   | 233100   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1668293  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.41     |
| episodes                | 233200   |
| lives                   | 233200   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1669138  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.35     |
| episodes                | 233300   |
| lives                   | 233300   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1669970  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.41     |
| episodes                | 233400   |
| lives                   | 233400   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1670829  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.41     |
| episodes                | 233500   |
| lives                   | 233500   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1671700  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.37     |
| episodes                | 233600   |
| lives                   | 233600   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1672545  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.38     |
| episodes                | 233700   |
| lives                   | 233700   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1673394  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.33     |
| episodes                | 233800   |
| lives                   | 233800   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1674218  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.37     |
| episodes                | 233900   |
| lives                   | 233900   |
| mean 100 episode length | 9.44     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1675062  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.4      |
| episodes                | 234000   |
| lives                   | 234000   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1675938  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.38     |
| episodes                | 234100   |
| lives                   | 234100   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1676794  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.38     |
| episodes                | 234200   |
| lives                   | 234200   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1677643  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.4      |
| episodes                | 234300   |
| lives                   | 234300   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1678531  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.42     |
| episodes                | 234400   |
| lives                   | 234400   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1679414  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.4      |
| episodes                | 234500   |
| lives                   | 234500   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1680281  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.38     |
| episodes                | 234600   |
| lives                   | 234600   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1681140  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.36     |
| episodes                | 234700   |
| lives                   | 234700   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0054  |
| steps                   | 1681978  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0044  |
| entropy                 | 1.39     |
| episodes                | 234800   |
| lives                   | 234800   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1682819  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0039  |
| entropy                 | 1.35     |
| episodes                | 234900   |
| lives                   | 234900   |
| mean 100 episode length | 9.39     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0056  |
| steps                   | 1683658  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.32     |
| episodes                | 235000   |
| lives                   | 235000   |
| mean 100 episode length | 9.21     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1684479  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0042  |
| entropy                 | 1.39     |
| episodes                | 235100   |
| lives                   | 235100   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1685347  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.3      |
| episodes                | 235200   |
| lives                   | 235200   |
| mean 100 episode length | 9.29     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1686176  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.38     |
| episodes                | 235300   |
| lives                   | 235300   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1687051  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.37     |
| episodes                | 235400   |
| lives                   | 235400   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1687918  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.41     |
| episodes                | 235500   |
| lives                   | 235500   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1688797  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.37     |
| episodes                | 235600   |
| lives                   | 235600   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1689652  |
| value_loss              | 1.22     |
--------------------------------------
Saving model due to running mean reward increase: 10.2668 -> 10.4607
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.38     |
| episodes                | 235700   |
| lives                   | 235700   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1690514  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.38     |
| episodes                | 235800   |
| lives                   | 235800   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1691356  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.34     |
| episodes                | 235900   |
| lives                   | 235900   |
| mean 100 episode length | 9.46     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1692202  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.38     |
| episodes                | 236000   |
| lives                   | 236000   |
| mean 100 episode length | 9.46     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1693048  |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.33     |
| episodes                | 236100   |
| lives                   | 236100   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1693897  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.34     |
| episodes                | 236200   |
| lives                   | 236200   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1694756  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.34     |
| episodes                | 236300   |
| lives                   | 236300   |
| mean 100 episode length | 9.32     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0059  |
| steps                   | 1695588  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.3      |
| episodes                | 236400   |
| lives                   | 236400   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1696436  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.34     |
| episodes                | 236500   |
| lives                   | 236500   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1697293  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.32     |
| episodes                | 236600   |
| lives                   | 236600   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1698135  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.32     |
| episodes                | 236700   |
| lives                   | 236700   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1698992  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.29     |
| episodes                | 236800   |
| lives                   | 236800   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1699835  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.35     |
| episodes                | 236900   |
| lives                   | 236900   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1700702  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.35     |
| episodes                | 237000   |
| lives                   | 237000   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1701561  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.29     |
| episodes                | 237100   |
| lives                   | 237100   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1702388  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.34     |
| episodes                | 237200   |
| lives                   | 237200   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1703235  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.37     |
| episodes                | 237300   |
| lives                   | 237300   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1704110  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.34     |
| episodes                | 237400   |
| lives                   | 237400   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1704971  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.34     |
| episodes                | 237500   |
| lives                   | 237500   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1705829  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.32     |
| episodes                | 237600   |
| lives                   | 237600   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1706689  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.33     |
| episodes                | 237700   |
| lives                   | 237700   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1707547  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.38     |
| episodes                | 237800   |
| lives                   | 237800   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0062  |
| steps                   | 1708430  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.35     |
| episodes                | 237900   |
| lives                   | 237900   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1709277  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.35     |
| episodes                | 238000   |
| lives                   | 238000   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1710140  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.37     |
| episodes                | 238100   |
| lives                   | 238100   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1711019  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.39     |
| episodes                | 238200   |
| lives                   | 238200   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1711912  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.32     |
| episodes                | 238300   |
| lives                   | 238300   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1712774  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.34     |
| episodes                | 238400   |
| lives                   | 238400   |
| mean 100 episode length | 9.87     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1713661  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.32     |
| episodes                | 238500   |
| lives                   | 238500   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.005   |
| steps                   | 1714537  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.31     |
| episodes                | 238600   |
| lives                   | 238600   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1715402  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.35     |
| episodes                | 238700   |
| lives                   | 238700   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1716294  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.28     |
| episodes                | 238800   |
| lives                   | 238800   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1717158  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.23     |
| episodes                | 238900   |
| lives                   | 238900   |
| mean 100 episode length | 9.45     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1718003  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.28     |
| episodes                | 239000   |
| lives                   | 239000   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1718876  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.34     |
| episodes                | 239100   |
| lives                   | 239100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1719785  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.29     |
| episodes                | 239200   |
| lives                   | 239200   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1720651  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.26     |
| episodes                | 239300   |
| lives                   | 239300   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1721486  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.34     |
| episodes                | 239400   |
| lives                   | 239400   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1722356  |
| value_loss              | 1.92     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.31     |
| episodes                | 239500   |
| lives                   | 239500   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1723199  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.3      |
| episodes                | 239600   |
| lives                   | 239600   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1724064  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.33     |
| episodes                | 239700   |
| lives                   | 239700   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1724945  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.29     |
| episodes                | 239800   |
| lives                   | 239800   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1725812  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.31     |
| episodes                | 239900   |
| lives                   | 239900   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1726685  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.28     |
| episodes                | 240000   |
| lives                   | 240000   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1727556  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.26     |
| episodes                | 240100   |
| lives                   | 240100   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1728403  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.28     |
| episodes                | 240200   |
| lives                   | 240200   |
| mean 100 episode length | 9.54     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1729257  |
| value_loss              | 1.27     |
--------------------------------------
Saving model due to running mean reward increase: 10.4018 -> 10.7072
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.29     |
| episodes                | 240300   |
| lives                   | 240300   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1730122  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.28     |
| episodes                | 240400   |
| lives                   | 240400   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1730985  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.26     |
| episodes                | 240500   |
| lives                   | 240500   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1731845  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.28     |
| episodes                | 240600   |
| lives                   | 240600   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1732719  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.24     |
| episodes                | 240700   |
| lives                   | 240700   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1733569  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.33     |
| episodes                | 240800   |
| lives                   | 240800   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1734446  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.32     |
| episodes                | 240900   |
| lives                   | 240900   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1735320  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.35     |
| episodes                | 241000   |
| lives                   | 241000   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1736190  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.33     |
| episodes                | 241100   |
| lives                   | 241100   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1737063  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.3      |
| episodes                | 241200   |
| lives                   | 241200   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1737929  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.28     |
| episodes                | 241300   |
| lives                   | 241300   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1738786  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0036  |
| entropy                 | 1.25     |
| episodes                | 241400   |
| lives                   | 241400   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1739649  |
| value_loss              | 1.48     |
--------------------------------------
Saving model due to mean reward increase: 10.8058 -> 10.9584
Saving model due to running mean reward increase: 10.519 -> 10.9584
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.27     |
| episodes                | 241500   |
| lives                   | 241500   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1740527  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.33     |
| episodes                | 241600   |
| lives                   | 241600   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1741415  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.34     |
| episodes                | 241700   |
| lives                   | 241700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1742322  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.32     |
| episodes                | 241800   |
| lives                   | 241800   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1743196  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.27     |
| episodes                | 241900   |
| lives                   | 241900   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1744063  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.29     |
| episodes                | 242000   |
| lives                   | 242000   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1744942  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.3      |
| episodes                | 242100   |
| lives                   | 242100   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1745800  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.27     |
| episodes                | 242200   |
| lives                   | 242200   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1746677  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.23     |
| episodes                | 242300   |
| lives                   | 242300   |
| mean 100 episode length | 9.54     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1747531  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.28     |
| episodes                | 242400   |
| lives                   | 242400   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1748399  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.26     |
| episodes                | 242500   |
| lives                   | 242500   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1749284  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.26     |
| episodes                | 242600   |
| lives                   | 242600   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1750152  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.27     |
| episodes                | 242700   |
| lives                   | 242700   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1751021  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.29     |
| episodes                | 242800   |
| lives                   | 242800   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1751917  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.27     |
| episodes                | 242900   |
| lives                   | 242900   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1752776  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.27     |
| episodes                | 243000   |
| lives                   | 243000   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1753659  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.3      |
| episodes                | 243100   |
| lives                   | 243100   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1754541  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.3      |
| episodes                | 243200   |
| lives                   | 243200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1755445  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.3      |
| episodes                | 243300   |
| lives                   | 243300   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1756334  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.29     |
| episodes                | 243400   |
| lives                   | 243400   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1757226  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0038  |
| entropy                 | 1.28     |
| episodes                | 243500   |
| lives                   | 243500   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0061  |
| steps                   | 1758116  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0046  |
| entropy                 | 1.24     |
| episodes                | 243600   |
| lives                   | 243600   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1758966  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.28     |
| episodes                | 243700   |
| lives                   | 243700   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1759848  |
| value_loss              | 1.37     |
--------------------------------------
Saving model due to mean reward increase: 10.9584 -> 10.9911
Saving model due to running mean reward increase: 10.4814 -> 10.9911
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.25     |
| episodes                | 243800   |
| lives                   | 243800   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1760726  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.27     |
| episodes                | 243900   |
| lives                   | 243900   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1761617  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.26     |
| episodes                | 244000   |
| lives                   | 244000   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1762506  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.28     |
| episodes                | 244100   |
| lives                   | 244100   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1763390  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.24     |
| episodes                | 244200   |
| lives                   | 244200   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1764237  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.3      |
| episodes                | 244300   |
| lives                   | 244300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1765137  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.25     |
| episodes                | 244400   |
| lives                   | 244400   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1765999  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.33     |
| episodes                | 244500   |
| lives                   | 244500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1766902  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.28     |
| episodes                | 244600   |
| lives                   | 244600   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1767763  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.26     |
| episodes                | 244700   |
| lives                   | 244700   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1768652  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.27     |
| episodes                | 244800   |
| lives                   | 244800   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1769531  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.26     |
| episodes                | 244900   |
| lives                   | 244900   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1770378  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.25     |
| episodes                | 245000   |
| lives                   | 245000   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1771245  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.25     |
| episodes                | 245100   |
| lives                   | 245100   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1772103  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.36     |
| episodes                | 245200   |
| lives                   | 245200   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1773001  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.31     |
| episodes                | 245300   |
| lives                   | 245300   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1773890  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.34     |
| episodes                | 245400   |
| lives                   | 245400   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1774771  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.31     |
| episodes                | 245500   |
| lives                   | 245500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1775674  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.31     |
| episodes                | 245600   |
| lives                   | 245600   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1776559  |
| value_loss              | 1.67     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.24     |
| episodes                | 245700   |
| lives                   | 245700   |
| mean 100 episode length | 9.53     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1777412  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.25     |
| episodes                | 245800   |
| lives                   | 245800   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1778281  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.31     |
| episodes                | 245900   |
| lives                   | 245900   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1779149  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.26     |
| episodes                | 246000   |
| lives                   | 246000   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1780033  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.26     |
| episodes                | 246100   |
| lives                   | 246100   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1780897  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.3      |
| episodes                | 246200   |
| lives                   | 246200   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1781781  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.31     |
| episodes                | 246300   |
| lives                   | 246300   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1782680  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0037  |
| entropy                 | 1.26     |
| episodes                | 246400   |
| lives                   | 246400   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1783551  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.37     |
| episodes                | 246500   |
| lives                   | 246500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0066  |
| steps                   | 1784498  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.27     |
| episodes                | 246600   |
| lives                   | 246600   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1785359  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.34     |
| episodes                | 246700   |
| lives                   | 246700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0048  |
| steps                   | 1786266  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.34     |
| episodes                | 246800   |
| lives                   | 246800   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1787147  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.27     |
| episodes                | 246900   |
| lives                   | 246900   |
| mean 100 episode length | 9.53     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1788000  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.25     |
| episodes                | 247000   |
| lives                   | 247000   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1788841  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.3      |
| episodes                | 247100   |
| lives                   | 247100   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1789719  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.27     |
| episodes                | 247200   |
| lives                   | 247200   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1790581  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.29     |
| episodes                | 247300   |
| lives                   | 247300   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1791477  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.3      |
| episodes                | 247400   |
| lives                   | 247400   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1792356  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.28     |
| episodes                | 247500   |
| lives                   | 247500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1793257  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.24     |
| episodes                | 247600   |
| lives                   | 247600   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1794100  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.3      |
| episodes                | 247700   |
| lives                   | 247700   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0052  |
| steps                   | 1794983  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.25     |
| episodes                | 247800   |
| lives                   | 247800   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1795860  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.28     |
| episodes                | 247900   |
| lives                   | 247900   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1796764  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.27     |
| episodes                | 248000   |
| lives                   | 248000   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1797633  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.23     |
| episodes                | 248100   |
| lives                   | 248100   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1798499  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.21     |
| episodes                | 248200   |
| lives                   | 248200   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1799350  |
| value_loss              | 1.19     |
--------------------------------------
Saving model due to running mean reward increase: 10.6535 -> 10.7899
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.26     |
| episodes                | 248300   |
| lives                   | 248300   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1800235  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.24     |
| episodes                | 248400   |
| lives                   | 248400   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1801111  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.27     |
| episodes                | 248500   |
| lives                   | 248500   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1801973  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.28     |
| episodes                | 248600   |
| lives                   | 248600   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1802867  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.26     |
| episodes                | 248700   |
| lives                   | 248700   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1803742  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.24     |
| episodes                | 248800   |
| lives                   | 248800   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1804624  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.27     |
| episodes                | 248900   |
| lives                   | 248900   |
| mean 100 episode length | 9.87     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1805511  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.18     |
| episodes                | 249000   |
| lives                   | 249000   |
| mean 100 episode length | 9.28     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1806339  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.21     |
| episodes                | 249100   |
| lives                   | 249100   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1807191  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.22     |
| episodes                | 249200   |
| lives                   | 249200   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1808075  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.27     |
| episodes                | 249300   |
| lives                   | 249300   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1808963  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.17     |
| episodes                | 249400   |
| lives                   | 249400   |
| mean 100 episode length | 9.43     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1809806  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.21     |
| episodes                | 249500   |
| lives                   | 249500   |
| mean 100 episode length | 9.41     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1810647  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.22     |
| episodes                | 249600   |
| lives                   | 249600   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1811523  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.21     |
| episodes                | 249700   |
| lives                   | 249700   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1812390  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.24     |
| episodes                | 249800   |
| lives                   | 249800   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1813281  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.25     |
| episodes                | 249900   |
| lives                   | 249900   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1814141  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.3      |
| episodes                | 250000   |
| lives                   | 250000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1815049  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.24     |
| episodes                | 250100   |
| lives                   | 250100   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1815914  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.29     |
| episodes                | 250200   |
| lives                   | 250200   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1816804  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.24     |
| episodes                | 250300   |
| lives                   | 250300   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1817680  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.28     |
| episodes                | 250400   |
| lives                   | 250400   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1818561  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.23     |
| episodes                | 250500   |
| lives                   | 250500   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1819426  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.26     |
| episodes                | 250600   |
| lives                   | 250600   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1820298  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.29     |
| episodes                | 250700   |
| lives                   | 250700   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1821192  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.23     |
| episodes                | 250800   |
| lives                   | 250800   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1822062  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.25     |
| episodes                | 250900   |
| lives                   | 250900   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1822910  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.27     |
| episodes                | 251000   |
| lives                   | 251000   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1823773  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.28     |
| episodes                | 251100   |
| lives                   | 251100   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1824657  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.29     |
| episodes                | 251200   |
| lives                   | 251200   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1825520  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0035  |
| entropy                 | 1.23     |
| episodes                | 251300   |
| lives                   | 251300   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0053  |
| steps                   | 1826387  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.32     |
| episodes                | 251400   |
| lives                   | 251400   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1827281  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.18     |
| episodes                | 251500   |
| lives                   | 251500   |
| mean 100 episode length | 9.26     |
| mean 100 episode reward | 10.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1828107  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.23     |
| episodes                | 251600   |
| lives                   | 251600   |
| mean 100 episode length | 9.54     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1828961  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.23     |
| episodes                | 251700   |
| lives                   | 251700   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1829825  |
| value_loss              | 1.11     |
--------------------------------------
Saving model due to running mean reward increase: 10.8622 -> 10.917
--------------------------------------
| approx_kl               | -0.0041  |
| entropy                 | 1.3      |
| episodes                | 251800   |
| lives                   | 251800   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1830694  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.21     |
| episodes                | 251900   |
| lives                   | 251900   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1831551  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.18     |
| episodes                | 252000   |
| lives                   | 252000   |
| mean 100 episode length | 9.4      |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1832391  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.26     |
| episodes                | 252100   |
| lives                   | 252100   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1833280  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.29     |
| episodes                | 252200   |
| lives                   | 252200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1834201  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.28     |
| episodes                | 252300   |
| lives                   | 252300   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0043  |
| steps                   | 1835098  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.26     |
| episodes                | 252400   |
| lives                   | 252400   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1835990  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.22     |
| episodes                | 252500   |
| lives                   | 252500   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1836876  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.25     |
| episodes                | 252600   |
| lives                   | 252600   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1837735  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.27     |
| episodes                | 252700   |
| lives                   | 252700   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1838594  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.27     |
| episodes                | 252800   |
| lives                   | 252800   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1839469  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.23     |
| episodes                | 252900   |
| lives                   | 252900   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1840317  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.25     |
| episodes                | 253000   |
| lives                   | 253000   |
| mean 100 episode length | 9.35     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1841152  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.29     |
| episodes                | 253100   |
| lives                   | 253100   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1842026  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.26     |
| episodes                | 253200   |
| lives                   | 253200   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1842885  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.18     |
| episodes                | 253300   |
| lives                   | 253300   |
| mean 100 episode length | 9.38     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1843723  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.24     |
| episodes                | 253400   |
| lives                   | 253400   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1844591  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.003   |
| entropy                 | 1.22     |
| episodes                | 253500   |
| lives                   | 253500   |
| mean 100 episode length | 9.24     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1845415  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.27     |
| episodes                | 253600   |
| lives                   | 253600   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1846285  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.22     |
| episodes                | 253700   |
| lives                   | 253700   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1847140  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.25     |
| episodes                | 253800   |
| lives                   | 253800   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1848032  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.23     |
| episodes                | 253900   |
| lives                   | 253900   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1848899  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.23     |
| episodes                | 254000   |
| lives                   | 254000   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1849765  |
| value_loss              | 1.17     |
--------------------------------------
Saving model due to mean reward increase: 10.9911 -> 11.0637
Saving model due to running mean reward increase: 10.6415 -> 11.0637
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.22     |
| episodes                | 254100   |
| lives                   | 254100   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1850653  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.18     |
| episodes                | 254200   |
| lives                   | 254200   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 1851495  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.21     |
| episodes                | 254300   |
| lives                   | 254300   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1852364  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.26     |
| episodes                | 254400   |
| lives                   | 254400   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1853257  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.16     |
| episodes                | 254500   |
| lives                   | 254500   |
| mean 100 episode length | 9.18     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1854075  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.25     |
| episodes                | 254600   |
| lives                   | 254600   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1854955  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.25     |
| episodes                | 254700   |
| lives                   | 254700   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1855824  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.22     |
| episodes                | 254800   |
| lives                   | 254800   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1856681  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.25     |
| episodes                | 254900   |
| lives                   | 254900   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1857562  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.21     |
| episodes                | 255000   |
| lives                   | 255000   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1858411  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0031  |
| entropy                 | 1.2      |
| episodes                | 255100   |
| lives                   | 255100   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1859262  |
| value_loss              | 1.31     |
--------------------------------------
Saving model due to running mean reward increase: 10.5151 -> 10.62
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.17     |
| episodes                | 255200   |
| lives                   | 255200   |
| mean 100 episode length | 9.34     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1860096  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.23     |
| episodes                | 255300   |
| lives                   | 255300   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1860964  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.19     |
| episodes                | 255400   |
| lives                   | 255400   |
| mean 100 episode length | 9.52     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1861816  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.26     |
| episodes                | 255500   |
| lives                   | 255500   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1862710  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.2      |
| episodes                | 255600   |
| lives                   | 255600   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1863572  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.29     |
| episodes                | 255700   |
| lives                   | 255700   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1864462  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.28     |
| episodes                | 255800   |
| lives                   | 255800   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1865364  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.26     |
| episodes                | 255900   |
| lives                   | 255900   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1866249  |
| value_loss              | 2.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.28     |
| episodes                | 256000   |
| lives                   | 256000   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1867134  |
| value_loss              | 1.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.26     |
| episodes                | 256100   |
| lives                   | 256100   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1868020  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.27     |
| episodes                | 256200   |
| lives                   | 256200   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1868905  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.28     |
| episodes                | 256300   |
| lives                   | 256300   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1869783  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.22     |
| episodes                | 256400   |
| lives                   | 256400   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1870646  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.23     |
| episodes                | 256500   |
| lives                   | 256500   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1871524  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.23     |
| episodes                | 256600   |
| lives                   | 256600   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1872408  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.22     |
| episodes                | 256700   |
| lives                   | 256700   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1873267  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.24     |
| episodes                | 256800   |
| lives                   | 256800   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1874123  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.16     |
| episodes                | 256900   |
| lives                   | 256900   |
| mean 100 episode length | 9.05     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1874928  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.26     |
| episodes                | 257000   |
| lives                   | 257000   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1875792  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.22     |
| episodes                | 257100   |
| lives                   | 257100   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 1876668  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.23     |
| episodes                | 257200   |
| lives                   | 257200   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0049  |
| steps                   | 1877548  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.17     |
| episodes                | 257300   |
| lives                   | 257300   |
| mean 100 episode length | 9.33     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1878381  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.22     |
| episodes                | 257400   |
| lives                   | 257400   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1879258  |
| value_loss              | 1.06     |
--------------------------------------
Saving model due to mean reward increase: 11.0637 -> 11.4127
Saving model due to running mean reward increase: 10.8375 -> 11.4127
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.28     |
| episodes                | 257500   |
| lives                   | 257500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1880160  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.26     |
| episodes                | 257600   |
| lives                   | 257600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1881065  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.18     |
| episodes                | 257700   |
| lives                   | 257700   |
| mean 100 episode length | 9.49     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1881914  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.21     |
| episodes                | 257800   |
| lives                   | 257800   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1882780  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.26     |
| episodes                | 257900   |
| lives                   | 257900   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.004   |
| steps                   | 1883661  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0033  |
| entropy                 | 1.25     |
| episodes                | 258000   |
| lives                   | 258000   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0051  |
| steps                   | 1884553  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.23     |
| episodes                | 258100   |
| lives                   | 258100   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1885427  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.2      |
| episodes                | 258200   |
| lives                   | 258200   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1886285  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.23     |
| episodes                | 258300   |
| lives                   | 258300   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1887159  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.23     |
| episodes                | 258400   |
| lives                   | 258400   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1888009  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0034  |
| entropy                 | 1.26     |
| episodes                | 258500   |
| lives                   | 258500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1888909  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.25     |
| episodes                | 258600   |
| lives                   | 258600   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1889787  |
| value_loss              | 1.81     |
--------------------------------------
Saving model due to running mean reward increase: 10.8047 -> 10.9425
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.23     |
| episodes                | 258700   |
| lives                   | 258700   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1890647  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.26     |
| episodes                | 258800   |
| lives                   | 258800   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1891526  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.23     |
| episodes                | 258900   |
| lives                   | 258900   |
| mean 100 episode length | 9.51     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1892377  |
| value_loss              | 1.77     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.26     |
| episodes                | 259000   |
| lives                   | 259000   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1893261  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.21     |
| episodes                | 259100   |
| lives                   | 259100   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1894136  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.27     |
| episodes                | 259200   |
| lives                   | 259200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1895048  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.2      |
| episodes                | 259300   |
| lives                   | 259300   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1895913  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.22     |
| episodes                | 259400   |
| lives                   | 259400   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1896791  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.21     |
| episodes                | 259500   |
| lives                   | 259500   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1897649  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.22     |
| episodes                | 259600   |
| lives                   | 259600   |
| mean 100 episode length | 9.61     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1898510  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.26     |
| episodes                | 259700   |
| lives                   | 259700   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1899392  |
| value_loss              | 1.25     |
--------------------------------------
Saving model due to running mean reward increase: 10.8728 -> 10.8876
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.19     |
| episodes                | 259800   |
| lives                   | 259800   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1900262  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.24     |
| episodes                | 259900   |
| lives                   | 259900   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1901131  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.26     |
| episodes                | 260000   |
| lives                   | 260000   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1902010  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.2      |
| episodes                | 260100   |
| lives                   | 260100   |
| mean 100 episode length | 9.5      |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1902860  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.21     |
| episodes                | 260200   |
| lives                   | 260200   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1903727  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.23     |
| episodes                | 260300   |
| lives                   | 260300   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1904575  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.25     |
| episodes                | 260400   |
| lives                   | 260400   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1905443  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.27     |
| episodes                | 260500   |
| lives                   | 260500   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1906314  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.26     |
| episodes                | 260600   |
| lives                   | 260600   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1907177  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.26     |
| episodes                | 260700   |
| lives                   | 260700   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1908037  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.28     |
| episodes                | 260800   |
| lives                   | 260800   |
| mean 100 episode length | 9.56     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1908893  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.28     |
| episodes                | 260900   |
| lives                   | 260900   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1909784  |
| value_loss              | 1.2      |
--------------------------------------
Saving model due to running mean reward increase: 10.673 -> 10.9171
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.29     |
| episodes                | 261000   |
| lives                   | 261000   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1910658  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.25     |
| episodes                | 261100   |
| lives                   | 261100   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1911520  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.25     |
| episodes                | 261200   |
| lives                   | 261200   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1912411  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.29     |
| episodes                | 261300   |
| lives                   | 261300   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1913292  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.25     |
| episodes                | 261400   |
| lives                   | 261400   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1914152  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.22     |
| episodes                | 261500   |
| lives                   | 261500   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1915026  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.26     |
| episodes                | 261600   |
| lives                   | 261600   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 1915900  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.23     |
| episodes                | 261700   |
| lives                   | 261700   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1916763  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.19     |
| episodes                | 261800   |
| lives                   | 261800   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1917635  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.31     |
| episodes                | 261900   |
| lives                   | 261900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 1918567  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.3      |
| episodes                | 262000   |
| lives                   | 262000   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1919447  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.22     |
| episodes                | 262100   |
| lives                   | 262100   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1920310  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.22     |
| episodes                | 262200   |
| lives                   | 262200   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1921173  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.19     |
| episodes                | 262300   |
| lives                   | 262300   |
| mean 100 episode length | 9.53     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1922026  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.21     |
| episodes                | 262400   |
| lives                   | 262400   |
| mean 100 episode length | 9.42     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 1922868  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.24     |
| episodes                | 262500   |
| lives                   | 262500   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1923751  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.2      |
| episodes                | 262600   |
| lives                   | 262600   |
| mean 100 episode length | 9.59     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1924610  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.23     |
| episodes                | 262700   |
| lives                   | 262700   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1925474  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.25     |
| episodes                | 262800   |
| lives                   | 262800   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1926351  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.23     |
| episodes                | 262900   |
| lives                   | 262900   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 1927232  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.23     |
| episodes                | 263000   |
| lives                   | 263000   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1928110  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.2      |
| episodes                | 263100   |
| lives                   | 263100   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1928988  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.14     |
| episodes                | 263200   |
| lives                   | 263200   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1929836  |
| value_loss              | 1.1      |
--------------------------------------
Saving model due to running mean reward increase: 10.7597 -> 10.9326
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.23     |
| episodes                | 263300   |
| lives                   | 263300   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1930734  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.22     |
| episodes                | 263400   |
| lives                   | 263400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1931634  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.14     |
| episodes                | 263500   |
| lives                   | 263500   |
| mean 100 episode length | 9.53     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 1932487  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.19     |
| episodes                | 263600   |
| lives                   | 263600   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1933354  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.23     |
| episodes                | 263700   |
| lives                   | 263700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 1934259  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.19     |
| episodes                | 263800   |
| lives                   | 263800   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1935128  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.22     |
| episodes                | 263900   |
| lives                   | 263900   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1936022  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.11     |
| episodes                | 264000   |
| lives                   | 264000   |
| mean 100 episode length | 9.27     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1936849  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.15     |
| episodes                | 264100   |
| lives                   | 264100   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 1937706  |
| value_loss              | 1.9      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.21     |
| episodes                | 264200   |
| lives                   | 264200   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1938574  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.15     |
| episodes                | 264300   |
| lives                   | 264300   |
| mean 100 episode length | 9.48     |
| mean 100 episode reward | 10.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1939422  |
| value_loss              | 1.21     |
--------------------------------------
Saving model due to running mean reward increase: 10.5364 -> 10.5529
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.19     |
| episodes                | 264400   |
| lives                   | 264400   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1940289  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.22     |
| episodes                | 264500   |
| lives                   | 264500   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1941165  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.22     |
| episodes                | 264600   |
| lives                   | 264600   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1942045  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.21     |
| episodes                | 264700   |
| lives                   | 264700   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1942923  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.23     |
| episodes                | 264800   |
| lives                   | 264800   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1943826  |
| value_loss              | 1        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.21     |
| episodes                | 264900   |
| lives                   | 264900   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1944701  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.2      |
| episodes                | 265000   |
| lives                   | 265000   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1945580  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.24     |
| episodes                | 265100   |
| lives                   | 265100   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1946462  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.23     |
| episodes                | 265200   |
| lives                   | 265200   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1947354  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.22     |
| episodes                | 265300   |
| lives                   | 265300   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1948237  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.25     |
| episodes                | 265400   |
| lives                   | 265400   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1949102  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.2      |
| episodes                | 265500   |
| lives                   | 265500   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1949966  |
| value_loss              | 1.21     |
--------------------------------------
Saving model due to running mean reward increase: 10.6351 -> 10.7229
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.2      |
| episodes                | 265600   |
| lives                   | 265600   |
| mean 100 episode length | 9.53     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1950819  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.25     |
| episodes                | 265700   |
| lives                   | 265700   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 1951717  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.26     |
| episodes                | 265800   |
| lives                   | 265800   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1952588  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.23     |
| episodes                | 265900   |
| lives                   | 265900   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1953464  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.26     |
| episodes                | 266000   |
| lives                   | 266000   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1954364  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.23     |
| episodes                | 266100   |
| lives                   | 266100   |
| mean 100 episode length | 9.66     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1955230  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.2      |
| episodes                | 266200   |
| lives                   | 266200   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1956092  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.28     |
| episodes                | 266300   |
| lives                   | 266300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 1957000  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.27     |
| episodes                | 266400   |
| lives                   | 266400   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1957891  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.25     |
| episodes                | 266500   |
| lives                   | 266500   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1958784  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.24     |
| episodes                | 266600   |
| lives                   | 266600   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1959677  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.25     |
| episodes                | 266700   |
| lives                   | 266700   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 1960580  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.24     |
| episodes                | 266800   |
| lives                   | 266800   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 1961479  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.24     |
| episodes                | 266900   |
| lives                   | 266900   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0046  |
| steps                   | 1962378  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.24     |
| episodes                | 267000   |
| lives                   | 267000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1963289  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.28     |
| episodes                | 267100   |
| lives                   | 267100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1964210  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.2      |
| episodes                | 267200   |
| lives                   | 267200   |
| mean 100 episode length | 9.87     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 1965097  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.28     |
| episodes                | 267300   |
| lives                   | 267300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1966027  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.21     |
| episodes                | 267400   |
| lives                   | 267400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 1966929  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.2      |
| episodes                | 267500   |
| lives                   | 267500   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1967811  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.28     |
| episodes                | 267600   |
| lives                   | 267600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1968722  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.26     |
| episodes                | 267700   |
| lives                   | 267700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1969630  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.18     |
| episodes                | 267800   |
| lives                   | 267800   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 1970506  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.24     |
| episodes                | 267900   |
| lives                   | 267900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1971412  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.17     |
| episodes                | 268000   |
| lives                   | 268000   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1972280  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.25     |
| episodes                | 268100   |
| lives                   | 268100   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1973176  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.25     |
| episodes                | 268200   |
| lives                   | 268200   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1974075  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.2      |
| episodes                | 268300   |
| lives                   | 268300   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 1974938  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.23     |
| episodes                | 268400   |
| lives                   | 268400   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1975803  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.24     |
| episodes                | 268500   |
| lives                   | 268500   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1976693  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.21     |
| episodes                | 268600   |
| lives                   | 268600   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1977570  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.16     |
| episodes                | 268700   |
| lives                   | 268700   |
| mean 100 episode length | 9.3      |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 1978400  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.28     |
| episodes                | 268800   |
| lives                   | 268800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1979307  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.2      |
| episodes                | 268900   |
| lives                   | 268900   |
| mean 100 episode length | 9.54     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 1980161  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.2      |
| episodes                | 269000   |
| lives                   | 269000   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1981030  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.19     |
| episodes                | 269100   |
| lives                   | 269100   |
| mean 100 episode length | 9.58     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 1981888  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.18     |
| episodes                | 269200   |
| lives                   | 269200   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1982751  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.22     |
| episodes                | 269300   |
| lives                   | 269300   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 1983633  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.21     |
| episodes                | 269400   |
| lives                   | 269400   |
| mean 100 episode length | 9.87     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1984520  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.18     |
| episodes                | 269500   |
| lives                   | 269500   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1985397  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.21     |
| episodes                | 269600   |
| lives                   | 269600   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1986288  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.24     |
| episodes                | 269700   |
| lives                   | 269700   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 1987184  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.25     |
| episodes                | 269800   |
| lives                   | 269800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 1988108  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.21     |
| episodes                | 269900   |
| lives                   | 269900   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1989006  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.26     |
| episodes                | 270000   |
| lives                   | 270000   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 1989897  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0028  |
| entropy                 | 1.22     |
| episodes                | 270100   |
| lives                   | 270100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0045  |
| steps                   | 1990802  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.23     |
| episodes                | 270200   |
| lives                   | 270200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1991704  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.24     |
| episodes                | 270300   |
| lives                   | 270300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1992614  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.25     |
| episodes                | 270400   |
| lives                   | 270400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1993545  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.19     |
| episodes                | 270500   |
| lives                   | 270500   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1994423  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.15     |
| episodes                | 270600   |
| lives                   | 270600   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 1995285  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.21     |
| episodes                | 270700   |
| lives                   | 270700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 1996198  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.21     |
| episodes                | 270800   |
| lives                   | 270800   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 1997098  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.18     |
| episodes                | 270900   |
| lives                   | 270900   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 1997991  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.17     |
| episodes                | 271000   |
| lives                   | 271000   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 1998881  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.16     |
| episodes                | 271100   |
| lives                   | 271100   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 1999744  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.21     |
| episodes                | 271200   |
| lives                   | 271200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 2000655  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.22     |
| episodes                | 271300   |
| lives                   | 271300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 2001569  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.17     |
| episodes                | 271400   |
| lives                   | 271400   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2002439  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.16     |
| episodes                | 271500   |
| lives                   | 271500   |
| mean 100 episode length | 9.68     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2003307  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.18     |
| episodes                | 271600   |
| lives                   | 271600   |
| mean 100 episode length | 9.57     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2004164  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.2      |
| episodes                | 271700   |
| lives                   | 271700   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 2005047  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.18     |
| episodes                | 271800   |
| lives                   | 271800   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2005922  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.27     |
| episodes                | 271900   |
| lives                   | 271900   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 2006846  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.24     |
| episodes                | 272000   |
| lives                   | 272000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2007759  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.21     |
| episodes                | 272100   |
| lives                   | 272100   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2008660  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.25     |
| episodes                | 272200   |
| lives                   | 272200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0044  |
| steps                   | 2009571  |
| value_loss              | 1.47     |
--------------------------------------
Saving model due to running mean reward increase: 10.9855 -> 11.0124
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.22     |
| episodes                | 272300   |
| lives                   | 272300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2010485  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.26     |
| episodes                | 272400   |
| lives                   | 272400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 2011393  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.19     |
| episodes                | 272500   |
| lives                   | 272500   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 2012282  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.23     |
| episodes                | 272600   |
| lives                   | 272600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2013196  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.25     |
| episodes                | 272700   |
| lives                   | 272700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2014111  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.25     |
| episodes                | 272800   |
| lives                   | 272800   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2015014  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.22     |
| episodes                | 272900   |
| lives                   | 272900   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 2015917  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.26     |
| episodes                | 273000   |
| lives                   | 273000   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2016818  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.23     |
| episodes                | 273100   |
| lives                   | 273100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 2017729  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.22     |
| episodes                | 273200   |
| lives                   | 273200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2018642  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.26     |
| episodes                | 273300   |
| lives                   | 273300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 2019570  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.18     |
| episodes                | 273400   |
| lives                   | 273400   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2020456  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.16     |
| episodes                | 273500   |
| lives                   | 273500   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2021326  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.27     |
| episodes                | 273600   |
| lives                   | 273600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2022267  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.18     |
| episodes                | 273700   |
| lives                   | 273700   |
| mean 100 episode length | 9.77     |
| mean 100 episode reward | 10.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2023144  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.22     |
| episodes                | 273800   |
| lives                   | 273800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2024061  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.2      |
| episodes                | 273900   |
| lives                   | 273900   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2024955  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.2      |
| episodes                | 274000   |
| lives                   | 274000   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2025840  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.22     |
| episodes                | 274100   |
| lives                   | 274100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2026755  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.21     |
| episodes                | 274200   |
| lives                   | 274200   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2027645  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.22     |
| episodes                | 274300   |
| lives                   | 274300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2028546  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0029  |
| entropy                 | 1.21     |
| episodes                | 274400   |
| lives                   | 274400   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0042  |
| steps                   | 2029438  |
| value_loss              | 1.58     |
--------------------------------------
Saving model due to running mean reward increase: 10.8701 -> 10.9673
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.13     |
| episodes                | 274500   |
| lives                   | 274500   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2030312  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.26     |
| episodes                | 274600   |
| lives                   | 274600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2031242  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.18     |
| episodes                | 274700   |
| lives                   | 274700   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 2032138  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.13     |
| episodes                | 274800   |
| lives                   | 274800   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2033012  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.22     |
| episodes                | 274900   |
| lives                   | 274900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2033920  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.21     |
| episodes                | 275000   |
| lives                   | 275000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2034830  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.19     |
| episodes                | 275100   |
| lives                   | 275100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2035735  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.19     |
| episodes                | 275200   |
| lives                   | 275200   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2036633  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.18     |
| episodes                | 275300   |
| lives                   | 275300   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2037532  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.19     |
| episodes                | 275400   |
| lives                   | 275400   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2038428  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.2      |
| episodes                | 275500   |
| lives                   | 275500   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2039319  |
| value_loss              | 1.63     |
--------------------------------------
Saving model due to running mean reward increase: 11.0035 -> 11.1579
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.2      |
| episodes                | 275600   |
| lives                   | 275600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 2040236  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.2      |
| episodes                | 275700   |
| lives                   | 275700   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2041138  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.13     |
| episodes                | 275800   |
| lives                   | 275800   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2042023  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.16     |
| episodes                | 275900   |
| lives                   | 275900   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2042903  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.19     |
| episodes                | 276000   |
| lives                   | 276000   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2043800  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.24     |
| episodes                | 276100   |
| lives                   | 276100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2044716  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.15     |
| episodes                | 276200   |
| lives                   | 276200   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2045604  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.24     |
| episodes                | 276300   |
| lives                   | 276300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2046537  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0032  |
| entropy                 | 1.23     |
| episodes                | 276400   |
| lives                   | 276400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 2047465  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.18     |
| episodes                | 276500   |
| lives                   | 276500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2048371  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.14     |
| episodes                | 276600   |
| lives                   | 276600   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0038  |
| steps                   | 2049256  |
| value_loss              | 1.08     |
--------------------------------------
Saving model due to running mean reward increase: 11.1045 -> 11.201
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.19     |
| episodes                | 276700   |
| lives                   | 276700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2050164  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.19     |
| episodes                | 276800   |
| lives                   | 276800   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2051067  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.18     |
| episodes                | 276900   |
| lives                   | 276900   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2051965  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.17     |
| episodes                | 277000   |
| lives                   | 277000   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2052835  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.18     |
| episodes                | 277100   |
| lives                   | 277100   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2053724  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.23     |
| episodes                | 277200   |
| lives                   | 277200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2054625  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.16     |
| episodes                | 277300   |
| lives                   | 277300   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2055489  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.22     |
| episodes                | 277400   |
| lives                   | 277400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2056399  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.16     |
| episodes                | 277500   |
| lives                   | 277500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2057299  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.18     |
| episodes                | 277600   |
| lives                   | 277600   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2058171  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.15     |
| episodes                | 277700   |
| lives                   | 277700   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2059051  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.22     |
| episodes                | 277800   |
| lives                   | 277800   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2059949  |
| value_loss              | 1.39     |
--------------------------------------
Saving model due to running mean reward increase: 11.1209 -> 11.2195
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.19     |
| episodes                | 277900   |
| lives                   | 277900   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2060846  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.26     |
| episodes                | 278000   |
| lives                   | 278000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 2061771  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.21     |
| episodes                | 278100   |
| lives                   | 278100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2062682  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.26     |
| episodes                | 278200   |
| lives                   | 278200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2063612  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.21     |
| episodes                | 278300   |
| lives                   | 278300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2064524  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.17     |
| episodes                | 278400   |
| lives                   | 278400   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2065409  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.15     |
| episodes                | 278500   |
| lives                   | 278500   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2066295  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.19     |
| episodes                | 278600   |
| lives                   | 278600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2067200  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.13     |
| episodes                | 278700   |
| lives                   | 278700   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 2068088  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.15     |
| episodes                | 278800   |
| lives                   | 278800   |
| mean 100 episode length | 9.7      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 2068958  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.17     |
| episodes                | 278900   |
| lives                   | 278900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2069865  |
| value_loss              | 0.976    |
--------------------------------------
Saving model due to running mean reward increase: 10.8791 -> 11.0402
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.15     |
| episodes                | 279000   |
| lives                   | 279000   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2070747  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.21     |
| episodes                | 279100   |
| lives                   | 279100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2071690  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.16     |
| episodes                | 279200   |
| lives                   | 279200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2072597  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.13     |
| episodes                | 279300   |
| lives                   | 279300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2073497  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.11     |
| episodes                | 279400   |
| lives                   | 279400   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2074382  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.21     |
| episodes                | 279500   |
| lives                   | 279500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 2075321  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.23     |
| episodes                | 279600   |
| lives                   | 279600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0039  |
| steps                   | 2076245  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.18     |
| episodes                | 279700   |
| lives                   | 279700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2077154  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.19     |
| episodes                | 279800   |
| lives                   | 279800   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2078054  |
| value_loss              | 0.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.19     |
| episodes                | 279900   |
| lives                   | 279900   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2078936  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.16     |
| episodes                | 280000   |
| lives                   | 280000   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2079834  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.19     |
| episodes                | 280100   |
| lives                   | 280100   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2080729  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.19     |
| episodes                | 280200   |
| lives                   | 280200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2081633  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.24     |
| episodes                | 280300   |
| lives                   | 280300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2082569  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.16     |
| episodes                | 280400   |
| lives                   | 280400   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2083452  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.2      |
| episodes                | 280500   |
| lives                   | 280500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2084377  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0025  |
| entropy                 | 1.14     |
| episodes                | 280600   |
| lives                   | 280600   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2085273  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.14     |
| episodes                | 280700   |
| lives                   | 280700   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 2086151  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.09     |
| episodes                | 280800   |
| lives                   | 280800   |
| mean 100 episode length | 9.81     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2087032  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.12     |
| episodes                | 280900   |
| lives                   | 280900   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2087923  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.15     |
| episodes                | 281000   |
| lives                   | 281000   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2088817  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.13     |
| episodes                | 281100   |
| lives                   | 281100   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2089714  |
| value_loss              | 1.36     |
--------------------------------------
Saving model due to running mean reward increase: 11.1211 -> 11.1963
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.15     |
| episodes                | 281200   |
| lives                   | 281200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2090614  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.11     |
| episodes                | 281300   |
| lives                   | 281300   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2091497  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.16     |
| episodes                | 281400   |
| lives                   | 281400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2092402  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.16     |
| episodes                | 281500   |
| lives                   | 281500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2093312  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.16     |
| episodes                | 281600   |
| lives                   | 281600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2094212  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.09     |
| episodes                | 281700   |
| lives                   | 281700   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2095081  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.09     |
| episodes                | 281800   |
| lives                   | 281800   |
| mean 100 episode length | 9.63     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2095944  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.15     |
| episodes                | 281900   |
| lives                   | 281900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2096849  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.12     |
| episodes                | 282000   |
| lives                   | 282000   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2097738  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.13     |
| episodes                | 282100   |
| lives                   | 282100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0035  |
| steps                   | 2098659  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.13     |
| episodes                | 282200   |
| lives                   | 282200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2099567  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.13     |
| episodes                | 282300   |
| lives                   | 282300   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 2100445  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.11     |
| episodes                | 282400   |
| lives                   | 282400   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2101325  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.21     |
| episodes                | 282500   |
| lives                   | 282500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2102265  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.12     |
| episodes                | 282600   |
| lives                   | 282600   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2103141  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.14     |
| episodes                | 282700   |
| lives                   | 282700   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2104044  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.23     |
| episodes                | 282800   |
| lives                   | 282800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2104988  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.11     |
| episodes                | 282900   |
| lives                   | 282900   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2105878  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.18     |
| episodes                | 283000   |
| lives                   | 283000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2106790  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.14     |
| episodes                | 283100   |
| lives                   | 283100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2107702  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.12     |
| episodes                | 283200   |
| lives                   | 283200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2108609  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.13     |
| episodes                | 283300   |
| lives                   | 283300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2109519  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.14     |
| episodes                | 283400   |
| lives                   | 283400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2110423  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.14     |
| episodes                | 283500   |
| lives                   | 283500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2111337  |
| value_loss              | 0.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.12     |
| episodes                | 283600   |
| lives                   | 283600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2112237  |
| value_loss              | 1        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.12     |
| episodes                | 283700   |
| lives                   | 283700   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2113141  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.13     |
| episodes                | 283800   |
| lives                   | 283800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2114047  |
| value_loss              | 0.914    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.09     |
| episodes                | 283900   |
| lives                   | 283900   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2114938  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.15     |
| episodes                | 284000   |
| lives                   | 284000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2115855  |
| value_loss              | 0.974    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.17     |
| episodes                | 284100   |
| lives                   | 284100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2116787  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.16     |
| episodes                | 284200   |
| lives                   | 284200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2117699  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.12     |
| episodes                | 284300   |
| lives                   | 284300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2118601  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.13     |
| episodes                | 284400   |
| lives                   | 284400   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2119499  |
| value_loss              | 1.14     |
--------------------------------------
Saving model due to mean reward increase: 11.4127 -> 11.4462
Saving model due to running mean reward increase: 11.2651 -> 11.4462
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.19     |
| episodes                | 284500   |
| lives                   | 284500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2120437  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.16     |
| episodes                | 284600   |
| lives                   | 284600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0034  |
| steps                   | 2121346  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.14     |
| episodes                | 284700   |
| lives                   | 284700   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2122245  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.08     |
| episodes                | 284800   |
| lives                   | 284800   |
| mean 100 episode length | 9.64     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2123109  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.15     |
| episodes                | 284900   |
| lives                   | 284900   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2124004  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.16     |
| episodes                | 285000   |
| lives                   | 285000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2124919  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.14     |
| episodes                | 285100   |
| lives                   | 285100   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2125797  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.14     |
| episodes                | 285200   |
| lives                   | 285200   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2126666  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.19     |
| episodes                | 285300   |
| lives                   | 285300   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2127564  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.15     |
| episodes                | 285400   |
| lives                   | 285400   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2128455  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.18     |
| episodes                | 285500   |
| lives                   | 285500   |
| mean 100 episode length | 9.84     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2129339  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.21     |
| episodes                | 285600   |
| lives                   | 285600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2130241  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.17     |
| episodes                | 285700   |
| lives                   | 285700   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2131120  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.17     |
| episodes                | 285800   |
| lives                   | 285800   |
| mean 100 episode length | 9.96     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2132016  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0026  |
| entropy                 | 1.18     |
| episodes                | 285900   |
| lives                   | 285900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 2132927  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.11     |
| episodes                | 286000   |
| lives                   | 286000   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2133792  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.17     |
| episodes                | 286100   |
| lives                   | 286100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2134702  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.14     |
| episodes                | 286200   |
| lives                   | 286200   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2135581  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.14     |
| episodes                | 286300   |
| lives                   | 286300   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2136471  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.15     |
| episodes                | 286400   |
| lives                   | 286400   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2137345  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.15     |
| episodes                | 286500   |
| lives                   | 286500   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2138231  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.15     |
| episodes                | 286600   |
| lives                   | 286600   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2139114  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.11     |
| episodes                | 286700   |
| lives                   | 286700   |
| mean 100 episode length | 9.62     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2139976  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.2      |
| episodes                | 286800   |
| lives                   | 286800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2140884  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.1      |
| episodes                | 286900   |
| lives                   | 286900   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2141772  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.13     |
| episodes                | 287000   |
| lives                   | 287000   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2142650  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.15     |
| episodes                | 287100   |
| lives                   | 287100   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2143549  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.17     |
| episodes                | 287200   |
| lives                   | 287200   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2144432  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.17     |
| episodes                | 287300   |
| lives                   | 287300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2145343  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.16     |
| episodes                | 287400   |
| lives                   | 287400   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2146236  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.14     |
| episodes                | 287500   |
| lives                   | 287500   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2147115  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.15     |
| episodes                | 287600   |
| lives                   | 287600   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2147998  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.19     |
| episodes                | 287700   |
| lives                   | 287700   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2148898  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.15     |
| episodes                | 287800   |
| lives                   | 287800   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2149777  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.2      |
| episodes                | 287900   |
| lives                   | 287900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2150690  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.09     |
| episodes                | 288000   |
| lives                   | 288000   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2151545  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.2      |
| episodes                | 288100   |
| lives                   | 288100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2152463  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.12     |
| episodes                | 288200   |
| lives                   | 288200   |
| mean 100 episode length | 9.75     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0037  |
| steps                   | 2153338  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.19     |
| episodes                | 288300   |
| lives                   | 288300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2154254  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.14     |
| episodes                | 288400   |
| lives                   | 288400   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2155146  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.15     |
| episodes                | 288500   |
| lives                   | 288500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2156054  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.22     |
| episodes                | 288600   |
| lives                   | 288600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2156974  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.17     |
| episodes                | 288700   |
| lives                   | 288700   |
| mean 100 episode length | 9.87     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2157861  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.12     |
| episodes                | 288800   |
| lives                   | 288800   |
| mean 100 episode length | 9.8      |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2158741  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.17     |
| episodes                | 288900   |
| lives                   | 288900   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2159636  |
| value_loss              | 1.4      |
--------------------------------------
Saving model due to running mean reward increase: 10.8124 -> 11.0683
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.16     |
| episodes                | 289000   |
| lives                   | 289000   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2160538  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.15     |
| episodes                | 289100   |
| lives                   | 289100   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2161429  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.17     |
| episodes                | 289200   |
| lives                   | 289200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2162345  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.12     |
| episodes                | 289300   |
| lives                   | 289300   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2163234  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.12     |
| episodes                | 289400   |
| lives                   | 289400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2164136  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.14     |
| episodes                | 289500   |
| lives                   | 289500   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2165030  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.15     |
| episodes                | 289600   |
| lives                   | 289600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2165932  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.11     |
| episodes                | 289700   |
| lives                   | 289700   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2166815  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.12     |
| episodes                | 289800   |
| lives                   | 289800   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2167709  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.08     |
| episodes                | 289900   |
| lives                   | 289900   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2168595  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.11     |
| episodes                | 290000   |
| lives                   | 290000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2169510  |
| value_loss              | 1.39     |
--------------------------------------
Saving model due to running mean reward increase: 11.2071 -> 11.4062
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.15     |
| episodes                | 290100   |
| lives                   | 290100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2170425  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.08     |
| episodes                | 290200   |
| lives                   | 290200   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2171298  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.12     |
| episodes                | 290300   |
| lives                   | 290300   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2172192  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.14     |
| episodes                | 290400   |
| lives                   | 290400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2173093  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.14     |
| episodes                | 290500   |
| lives                   | 290500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2173995  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.16     |
| episodes                | 290600   |
| lives                   | 290600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2174895  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.16     |
| episodes                | 290700   |
| lives                   | 290700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2175823  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.2      |
| episodes                | 290800   |
| lives                   | 290800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2176733  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.11     |
| episodes                | 290900   |
| lives                   | 290900   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2177627  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.18     |
| episodes                | 291000   |
| lives                   | 291000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2178534  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.12     |
| episodes                | 291100   |
| lives                   | 291100   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2179432  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.15     |
| episodes                | 291200   |
| lives                   | 291200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2180336  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.16     |
| episodes                | 291300   |
| lives                   | 291300   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2181221  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.13     |
| episodes                | 291400   |
| lives                   | 291400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2182122  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.14     |
| episodes                | 291500   |
| lives                   | 291500   |
| mean 100 episode length | 9.83     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2183005  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.19     |
| episodes                | 291600   |
| lives                   | 291600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2183930  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.15     |
| episodes                | 291700   |
| lives                   | 291700   |
| mean 100 episode length | 9.89     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2184819  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.14     |
| episodes                | 291800   |
| lives                   | 291800   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2185690  |
| value_loss              | 0.883    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.12     |
| episodes                | 291900   |
| lives                   | 291900   |
| mean 100 episode length | 9.55     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2186545  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.12     |
| episodes                | 292000   |
| lives                   | 292000   |
| mean 100 episode length | 9.72     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2187417  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.13     |
| episodes                | 292100   |
| lives                   | 292100   |
| mean 100 episode length | 9.6      |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2188277  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.16     |
| episodes                | 292200   |
| lives                   | 292200   |
| mean 100 episode length | 9.65     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2189142  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.07     |
| episodes                | 292300   |
| lives                   | 292300   |
| mean 100 episode length | 9.47     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2189989  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.13     |
| episodes                | 292400   |
| lives                   | 292400   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2190867  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.07     |
| episodes                | 292500   |
| lives                   | 292500   |
| mean 100 episode length | 9.67     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2191734  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.12     |
| episodes                | 292600   |
| lives                   | 292600   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2192622  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.1      |
| episodes                | 292700   |
| lives                   | 292700   |
| mean 100 episode length | 9.86     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2193508  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.08     |
| episodes                | 292800   |
| lives                   | 292800   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2194384  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.12     |
| episodes                | 292900   |
| lives                   | 292900   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2195281  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.15     |
| episodes                | 293000   |
| lives                   | 293000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2196204  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.12     |
| episodes                | 293100   |
| lives                   | 293100   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2197082  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.12     |
| episodes                | 293200   |
| lives                   | 293200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2197986  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.15     |
| episodes                | 293300   |
| lives                   | 293300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2198909  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.1      |
| episodes                | 293400   |
| lives                   | 293400   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0033  |
| steps                   | 2199803  |
| value_loss              | 1.26     |
--------------------------------------
Saving model due to running mean reward increase: 11.1189 -> 11.3286
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.14     |
| episodes                | 293500   |
| lives                   | 293500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2200715  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.17     |
| episodes                | 293600   |
| lives                   | 293600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2201659  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.18     |
| episodes                | 293700   |
| lives                   | 293700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2202606  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.17     |
| episodes                | 293800   |
| lives                   | 293800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2203543  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.14     |
| episodes                | 293900   |
| lives                   | 293900   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2204463  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.16     |
| episodes                | 294000   |
| lives                   | 294000   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2205362  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.1      |
| episodes                | 294100   |
| lives                   | 294100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2206289  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.13     |
| episodes                | 294200   |
| lives                   | 294200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2207198  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.09     |
| episodes                | 294300   |
| lives                   | 294300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2208106  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.14     |
| episodes                | 294400   |
| lives                   | 294400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2209030  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.1      |
| episodes                | 294500   |
| lives                   | 294500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2209941  |
| value_loss              | 0.955    |
--------------------------------------
Saving model due to running mean reward increase: 11.3245 -> 11.4013
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.15     |
| episodes                | 294600   |
| lives                   | 294600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2210851  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.1      |
| episodes                | 294700   |
| lives                   | 294700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2211756  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.08     |
| episodes                | 294800   |
| lives                   | 294800   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2212635  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.1      |
| episodes                | 294900   |
| lives                   | 294900   |
| mean 100 episode length | 9.88     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2213523  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.12     |
| episodes                | 295000   |
| lives                   | 295000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2214435  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.13     |
| episodes                | 295100   |
| lives                   | 295100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2215340  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.13     |
| episodes                | 295200   |
| lives                   | 295200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2216243  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.14     |
| episodes                | 295300   |
| lives                   | 295300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2217166  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.11     |
| episodes                | 295400   |
| lives                   | 295400   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2218067  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.08     |
| episodes                | 295500   |
| lives                   | 295500   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2218949  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.12     |
| episodes                | 295600   |
| lives                   | 295600   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2219847  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.12     |
| episodes                | 295700   |
| lives                   | 295700   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2220745  |
| value_loss              | 0.94     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.15     |
| episodes                | 295800   |
| lives                   | 295800   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2221644  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.13     |
| episodes                | 295900   |
| lives                   | 295900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2222554  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.14     |
| episodes                | 296000   |
| lives                   | 296000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2223473  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.13     |
| episodes                | 296100   |
| lives                   | 296100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2224388  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.08     |
| episodes                | 296200   |
| lives                   | 296200   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2225278  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.11     |
| episodes                | 296300   |
| lives                   | 296300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2226204  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0023  |
| entropy                 | 1.11     |
| episodes                | 296400   |
| lives                   | 296400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2227109  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.11     |
| episodes                | 296500   |
| lives                   | 296500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2228022  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.09     |
| episodes                | 296600   |
| lives                   | 296600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2228923  |
| value_loss              | 0.892    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.08     |
| episodes                | 296700   |
| lives                   | 296700   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2229814  |
| value_loss              | 0.933    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.06     |
| episodes                | 296800   |
| lives                   | 296800   |
| mean 100 episode length | 9.94     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2230708  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.09     |
| episodes                | 296900   |
| lives                   | 296900   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2231610  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.1      |
| episodes                | 297000   |
| lives                   | 297000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0031  |
| steps                   | 2232524  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.09     |
| episodes                | 297100   |
| lives                   | 297100   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2233426  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.16     |
| episodes                | 297200   |
| lives                   | 297200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2234373  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.08     |
| episodes                | 297300   |
| lives                   | 297300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2235282  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.09     |
| episodes                | 297400   |
| lives                   | 297400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2236201  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.13     |
| episodes                | 297500   |
| lives                   | 297500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2237146  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.12     |
| episodes                | 297600   |
| lives                   | 297600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2238065  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.08     |
| episodes                | 297700   |
| lives                   | 297700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2238972  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.1      |
| episodes                | 297800   |
| lives                   | 297800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2239890  |
| value_loss              | 1.2      |
--------------------------------------
Saving model due to mean reward increase: 11.4462 -> 11.5191
Saving model due to running mean reward increase: 11.2691 -> 11.5191
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.06     |
| episodes                | 297900   |
| lives                   | 297900   |
| mean 100 episode length | 9.87     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2240777  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.06     |
| episodes                | 298000   |
| lives                   | 298000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2241693  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 298100   |
| lives                   | 298100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2242610  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.09     |
| episodes                | 298200   |
| lives                   | 298200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2243521  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.04     |
| episodes                | 298300   |
| lives                   | 298300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2244422  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.1      |
| episodes                | 298400   |
| lives                   | 298400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2245340  |
| value_loss              | 0.984    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.05     |
| episodes                | 298500   |
| lives                   | 298500   |
| mean 100 episode length | 9.76     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2246216  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.03     |
| episodes                | 298600   |
| lives                   | 298600   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2247113  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.11     |
| episodes                | 298700   |
| lives                   | 298700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2248035  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.09     |
| episodes                | 298800   |
| lives                   | 298800   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2248934  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.07     |
| episodes                | 298900   |
| lives                   | 298900   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2249826  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.07     |
| episodes                | 299000   |
| lives                   | 299000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2250732  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 299100   |
| lives                   | 299100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2251638  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.1      |
| episodes                | 299200   |
| lives                   | 299200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2252589  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.06     |
| episodes                | 299300   |
| lives                   | 299300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2253496  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.09     |
| episodes                | 299400   |
| lives                   | 299400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2254408  |
| value_loss              | 0.933    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.09     |
| episodes                | 299500   |
| lives                   | 299500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2255329  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.06     |
| episodes                | 299600   |
| lives                   | 299600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2256236  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.08     |
| episodes                | 299700   |
| lives                   | 299700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2257163  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.07     |
| episodes                | 299800   |
| lives                   | 299800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2258087  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.06     |
| episodes                | 299900   |
| lives                   | 299900   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2258990  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.06     |
| episodes                | 300000   |
| lives                   | 300000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2259895  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 300100   |
| lives                   | 300100   |
| mean 100 episode length | 9.78     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2260773  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.08     |
| episodes                | 300200   |
| lives                   | 300200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2261703  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.09     |
| episodes                | 300300   |
| lives                   | 300300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2262613  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.05     |
| episodes                | 300400   |
| lives                   | 300400   |
| mean 100 episode length | 9.79     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2263492  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.05     |
| episodes                | 300500   |
| lives                   | 300500   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0032  |
| steps                   | 2264392  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.06     |
| episodes                | 300600   |
| lives                   | 300600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2265294  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.08     |
| episodes                | 300700   |
| lives                   | 300700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2266210  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.08     |
| episodes                | 300800   |
| lives                   | 300800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2267142  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.05     |
| episodes                | 300900   |
| lives                   | 300900   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2268041  |
| value_loss              | 1.52     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 301000   |
| lives                   | 301000   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2268943  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.04     |
| episodes                | 301100   |
| lives                   | 301100   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2269844  |
| value_loss              | 1.03     |
--------------------------------------
Saving model due to running mean reward increase: 11.2683 -> 11.3891
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.07     |
| episodes                | 301200   |
| lives                   | 301200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2270750  |
| value_loss              | 0.954    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.09     |
| episodes                | 301300   |
| lives                   | 301300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2271682  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 301400   |
| lives                   | 301400   |
| mean 100 episode length | 9.97     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2272579  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 301500   |
| lives                   | 301500   |
| mean 100 episode length | 9.69     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2273448  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.04     |
| episodes                | 301600   |
| lives                   | 301600   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2274346  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.05     |
| episodes                | 301700   |
| lives                   | 301700   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2275241  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.07     |
| episodes                | 301800   |
| lives                   | 301800   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2276136  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.07     |
| episodes                | 301900   |
| lives                   | 301900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2277064  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.11     |
| episodes                | 302000   |
| lives                   | 302000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2278002  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.12     |
| episodes                | 302100   |
| lives                   | 302100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2278940  |
| value_loss              | 0.938    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.07     |
| episodes                | 302200   |
| lives                   | 302200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2279874  |
| value_loss              | 0.972    |
--------------------------------------
Saving model due to mean reward increase: 11.5191 -> 11.8094
Saving model due to running mean reward increase: 11.5879 -> 11.8094
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 302300   |
| lives                   | 302300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2280780  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.07     |
| episodes                | 302400   |
| lives                   | 302400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2281704  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.07     |
| episodes                | 302500   |
| lives                   | 302500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2282629  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.04     |
| episodes                | 302600   |
| lives                   | 302600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2283550  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.11     |
| episodes                | 302700   |
| lives                   | 302700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2284484  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.08     |
| episodes                | 302800   |
| lives                   | 302800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2285405  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.03     |
| episodes                | 302900   |
| lives                   | 302900   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2286297  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.06     |
| episodes                | 303000   |
| lives                   | 303000   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2287227  |
| value_loss              | 0.967    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.07     |
| episodes                | 303100   |
| lives                   | 303100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2288134  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.04     |
| episodes                | 303200   |
| lives                   | 303200   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2289037  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.02     |
| episodes                | 303300   |
| lives                   | 303300   |
| mean 100 episode length | 9.82     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2289919  |
| value_loss              | 1.15     |
--------------------------------------
Saving model due to running mean reward increase: 11.1312 -> 11.3535
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 303400   |
| lives                   | 303400   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2290814  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.03     |
| episodes                | 303500   |
| lives                   | 303500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2291720  |
| value_loss              | 1.5      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 303600   |
| lives                   | 303600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2292630  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.09     |
| episodes                | 303700   |
| lives                   | 303700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2293552  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.03     |
| episodes                | 303800   |
| lives                   | 303800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2294457  |
| value_loss              | 0.989    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.09     |
| episodes                | 303900   |
| lives                   | 303900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2295387  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.06     |
| episodes                | 304000   |
| lives                   | 304000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2296302  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.08     |
| episodes                | 304100   |
| lives                   | 304100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2297211  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.03     |
| episodes                | 304200   |
| lives                   | 304200   |
| mean 100 episode length | 9.9      |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2298101  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.07     |
| episodes                | 304300   |
| lives                   | 304300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2299024  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.03     |
| episodes                | 304400   |
| lives                   | 304400   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2299919  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.07     |
| episodes                | 304500   |
| lives                   | 304500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2300831  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0027  |
| entropy                 | 1.05     |
| episodes                | 304600   |
| lives                   | 304600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0041  |
| steps                   | 2301739  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.08     |
| episodes                | 304700   |
| lives                   | 304700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2302664  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.14     |
| episodes                | 304800   |
| lives                   | 304800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2303637  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.04     |
| episodes                | 304900   |
| lives                   | 304900   |
| mean 100 episode length | 9.92     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2304529  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.1      |
| episodes                | 305000   |
| lives                   | 305000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2305466  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.11     |
| episodes                | 305100   |
| lives                   | 305100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2306399  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 305200   |
| lives                   | 305200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2307315  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.06     |
| episodes                | 305300   |
| lives                   | 305300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2308224  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.1      |
| episodes                | 305400   |
| lives                   | 305400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2309159  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.06     |
| episodes                | 305500   |
| lives                   | 305500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2310073  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.03     |
| episodes                | 305600   |
| lives                   | 305600   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2310964  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.09     |
| episodes                | 305700   |
| lives                   | 305700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2311900  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 305800   |
| lives                   | 305800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2312816  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 305900   |
| lives                   | 305900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2313722  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.05     |
| episodes                | 306000   |
| lives                   | 306000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2314630  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.07     |
| episodes                | 306100   |
| lives                   | 306100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2315546  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.09     |
| episodes                | 306200   |
| lives                   | 306200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2316473  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.04     |
| episodes                | 306300   |
| lives                   | 306300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2317380  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.09     |
| episodes                | 306400   |
| lives                   | 306400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2318321  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.11     |
| episodes                | 306500   |
| lives                   | 306500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2319273  |
| value_loss              | 0.934    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 306600   |
| lives                   | 306600   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2320176  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0024  |
| entropy                 | 1.06     |
| episodes                | 306700   |
| lives                   | 306700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2321087  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 306800   |
| lives                   | 306800   |
| mean 100 episode length | 9.98     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2321985  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.1      |
| episodes                | 306900   |
| lives                   | 306900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2322919  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.1      |
| episodes                | 307000   |
| lives                   | 307000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2323859  |
| value_loss              | 0.933    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 0.99     |
| episodes                | 307100   |
| lives                   | 307100   |
| mean 100 episode length | 9.71     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2324730  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.07     |
| episodes                | 307200   |
| lives                   | 307200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2325658  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.03     |
| episodes                | 307300   |
| lives                   | 307300   |
| mean 100 episode length | 9.74     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0027  |
| steps                   | 2326532  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.04     |
| episodes                | 307400   |
| lives                   | 307400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2327440  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.05     |
| episodes                | 307500   |
| lives                   | 307500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2328360  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.982    |
| episodes                | 307600   |
| lives                   | 307600   |
| mean 100 episode length | 9.85     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2329245  |
| value_loss              | 1.1      |
--------------------------------------
Saving model due to running mean reward increase: 11.3802 -> 11.541
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.09     |
| episodes                | 307700   |
| lives                   | 307700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2330167  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.01     |
| episodes                | 307800   |
| lives                   | 307800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2331078  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 0.976    |
| episodes                | 307900   |
| lives                   | 307900   |
| mean 100 episode length | 9.73     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2331951  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.991    |
| episodes                | 308000   |
| lives                   | 308000   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2332853  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.998    |
| episodes                | 308100   |
| lives                   | 308100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2333767  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.07     |
| episodes                | 308200   |
| lives                   | 308200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2334707  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.03     |
| episodes                | 308300   |
| lives                   | 308300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2335620  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.07     |
| episodes                | 308400   |
| lives                   | 308400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2336567  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 0.995    |
| episodes                | 308500   |
| lives                   | 308500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2337475  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.06     |
| episodes                | 308600   |
| lives                   | 308600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2338401  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.02     |
| episodes                | 308700   |
| lives                   | 308700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2339314  |
| value_loss              | 1.09     |
--------------------------------------
Saving model due to running mean reward increase: 11.2993 -> 11.6652
--------------------------------------
| approx_kl               | -0.0022  |
| entropy                 | 1.05     |
| episodes                | 308800   |
| lives                   | 308800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0036  |
| steps                   | 2340228  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.08     |
| episodes                | 308900   |
| lives                   | 308900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2341165  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 309000   |
| lives                   | 309000   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2342098  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.02     |
| episodes                | 309100   |
| lives                   | 309100   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2343001  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.995    |
| episodes                | 309200   |
| lives                   | 309200   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2343896  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.06     |
| episodes                | 309300   |
| lives                   | 309300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2344819  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.11     |
| episodes                | 309400   |
| lives                   | 309400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2345774  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 309500   |
| lives                   | 309500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2346698  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.998    |
| episodes                | 309600   |
| lives                   | 309600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2347612  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.06     |
| episodes                | 309700   |
| lives                   | 309700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2348535  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.1      |
| episodes                | 309800   |
| lives                   | 309800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2349504  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.02     |
| episodes                | 309900   |
| lives                   | 309900   |
| mean 100 episode length | 9.95     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2350399  |
| value_loss              | 1.65     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.04     |
| episodes                | 310000   |
| lives                   | 310000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2351320  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 310100   |
| lives                   | 310100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2352237  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.07     |
| episodes                | 310200   |
| lives                   | 310200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2353165  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 310300   |
| lives                   | 310300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2354075  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.06     |
| episodes                | 310400   |
| lives                   | 310400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2355014  |
| value_loss              | 0.896    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.07     |
| episodes                | 310500   |
| lives                   | 310500   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2355945  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.1      |
| episodes                | 310600   |
| lives                   | 310600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2356891  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.06     |
| episodes                | 310700   |
| lives                   | 310700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2357817  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.02     |
| episodes                | 310800   |
| lives                   | 310800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2358740  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.07     |
| episodes                | 310900   |
| lives                   | 310900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2359685  |
| value_loss              | 1.15     |
--------------------------------------
Saving model due to running mean reward increase: 11.6788 -> 11.7982
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 311000   |
| lives                   | 311000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2360610  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.05     |
| episodes                | 311100   |
| lives                   | 311100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2361536  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.09     |
| episodes                | 311200   |
| lives                   | 311200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2362484  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.03     |
| episodes                | 311300   |
| lives                   | 311300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2363405  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.04     |
| episodes                | 311400   |
| lives                   | 311400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2364320  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.08     |
| episodes                | 311500   |
| lives                   | 311500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2365272  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.07     |
| episodes                | 311600   |
| lives                   | 311600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2366200  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.06     |
| episodes                | 311700   |
| lives                   | 311700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2367123  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.07     |
| episodes                | 311800   |
| lives                   | 311800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2368052  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.02     |
| episodes                | 311900   |
| lives                   | 311900   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2368972  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.05     |
| episodes                | 312000   |
| lives                   | 312000   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2369901  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.04     |
| episodes                | 312100   |
| lives                   | 312100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2370813  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.04     |
| episodes                | 312200   |
| lives                   | 312200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2371734  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.05     |
| episodes                | 312300   |
| lives                   | 312300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2372675  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.01     |
| episodes                | 312400   |
| lives                   | 312400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2373587  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.11     |
| episodes                | 312500   |
| lives                   | 312500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2374557  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 312600   |
| lives                   | 312600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2375485  |
| value_loss              | 0.813    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.11     |
| episodes                | 312700   |
| lives                   | 312700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2376446  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.994    |
| episodes                | 312800   |
| lives                   | 312800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2377351  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 312900   |
| lives                   | 312900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2378294  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.06     |
| episodes                | 313000   |
| lives                   | 313000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2379242  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.05     |
| episodes                | 313100   |
| lives                   | 313100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2380177  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 313200   |
| lives                   | 313200   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2381091  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.06     |
| episodes                | 313300   |
| lives                   | 313300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2382034  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.05     |
| episodes                | 313400   |
| lives                   | 313400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2382963  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.01     |
| episodes                | 313500   |
| lives                   | 313500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2383870  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.08     |
| episodes                | 313600   |
| lives                   | 313600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2384821  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 313700   |
| lives                   | 313700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2385749  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.989    |
| episodes                | 313800   |
| lives                   | 313800   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2386642  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.01     |
| episodes                | 313900   |
| lives                   | 313900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2387555  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 0.993    |
| episodes                | 314000   |
| lives                   | 314000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2388467  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.03     |
| episodes                | 314100   |
| lives                   | 314100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2389383  |
| value_loss              | 1.2      |
--------------------------------------
Saving model due to mean reward increase: 11.8094 -> 11.9313
Saving model due to running mean reward increase: 11.5696 -> 11.9313
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.06     |
| episodes                | 314200   |
| lives                   | 314200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2390334  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.993    |
| episodes                | 314300   |
| lives                   | 314300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2391237  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 314400   |
| lives                   | 314400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2392148  |
| value_loss              | 0.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.07     |
| episodes                | 314500   |
| lives                   | 314500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2393093  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.993    |
| episodes                | 314600   |
| lives                   | 314600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2394003  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.07     |
| episodes                | 314700   |
| lives                   | 314700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2394937  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.09     |
| episodes                | 314800   |
| lives                   | 314800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2395871  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.06     |
| episodes                | 314900   |
| lives                   | 314900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2396820  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.08     |
| episodes                | 315000   |
| lives                   | 315000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2397740  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 0.992    |
| episodes                | 315100   |
| lives                   | 315100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2398658  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.07     |
| episodes                | 315200   |
| lives                   | 315200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2399604  |
| value_loss              | 1.24     |
--------------------------------------
Saving model due to running mean reward increase: 11.5474 -> 11.7814
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.01     |
| episodes                | 315300   |
| lives                   | 315300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2400521  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 315400   |
| lives                   | 315400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2401474  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.02     |
| episodes                | 315500   |
| lives                   | 315500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2402397  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 315600   |
| lives                   | 315600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2403319  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.03     |
| episodes                | 315700   |
| lives                   | 315700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2404239  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.01     |
| episodes                | 315800   |
| lives                   | 315800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2405157  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.04     |
| episodes                | 315900   |
| lives                   | 315900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2406085  |
| value_loss              | 0.798    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.01     |
| episodes                | 316000   |
| lives                   | 316000   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2406987  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 316100   |
| lives                   | 316100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2407930  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 316200   |
| lives                   | 316200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2408871  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.01     |
| episodes                | 316300   |
| lives                   | 316300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2409794  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 316400   |
| lives                   | 316400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2410717  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.07     |
| episodes                | 316500   |
| lives                   | 316500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2411676  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.03     |
| episodes                | 316600   |
| lives                   | 316600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2412614  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.09     |
| episodes                | 316700   |
| lives                   | 316700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2413574  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.01     |
| episodes                | 316800   |
| lives                   | 316800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2414501  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 316900   |
| lives                   | 316900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2415410  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.07     |
| episodes                | 317000   |
| lives                   | 317000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2416367  |
| value_loss              | 0.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.998    |
| episodes                | 317100   |
| lives                   | 317100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2417274  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.05     |
| episodes                | 317200   |
| lives                   | 317200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2418213  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.02     |
| episodes                | 317300   |
| lives                   | 317300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2419116  |
| value_loss              | 1.26     |
--------------------------------------
Saving model due to running mean reward increase: 11.5585 -> 11.8462
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 317400   |
| lives                   | 317400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2420071  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.05     |
| episodes                | 317500   |
| lives                   | 317500   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2420997  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.03     |
| episodes                | 317600   |
| lives                   | 317600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2421919  |
| value_loss              | 0.969    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 317700   |
| lives                   | 317700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2422828  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.996    |
| episodes                | 317800   |
| lives                   | 317800   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2423744  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 317900   |
| lives                   | 317900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2424650  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 318000   |
| lives                   | 318000   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2425582  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1        |
| episodes                | 318100   |
| lives                   | 318100   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2426486  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.06     |
| episodes                | 318200   |
| lives                   | 318200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2427424  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.04     |
| episodes                | 318300   |
| lives                   | 318300   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2428336  |
| value_loss              | 0.902    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.07     |
| episodes                | 318400   |
| lives                   | 318400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2429282  |
| value_loss              | 1.49     |
--------------------------------------
Saving model due to running mean reward increase: 11.575 -> 11.7005
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.06     |
| episodes                | 318500   |
| lives                   | 318500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2430222  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.03     |
| episodes                | 318600   |
| lives                   | 318600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2431140  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.02     |
| episodes                | 318700   |
| lives                   | 318700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2432050  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.09     |
| episodes                | 318800   |
| lives                   | 318800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2433023  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.04     |
| episodes                | 318900   |
| lives                   | 318900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2433954  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.03     |
| episodes                | 319000   |
| lives                   | 319000   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2434882  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.982    |
| episodes                | 319100   |
| lives                   | 319100   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2435784  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.08     |
| episodes                | 319200   |
| lives                   | 319200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2436733  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.04     |
| episodes                | 319300   |
| lives                   | 319300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2437673  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.996    |
| episodes                | 319400   |
| lives                   | 319400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2438593  |
| value_loss              | 0.929    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0021  |
| entropy                 | 1.1      |
| episodes                | 319500   |
| lives                   | 319500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2439578  |
| value_loss              | 1.34     |
--------------------------------------
Saving model due to running mean reward increase: 11.7654 -> 11.8842
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.11     |
| episodes                | 319600   |
| lives                   | 319600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2440539  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.04     |
| episodes                | 319700   |
| lives                   | 319700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2441476  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1        |
| episodes                | 319800   |
| lives                   | 319800   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2442385  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.03     |
| episodes                | 319900   |
| lives                   | 319900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2443318  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.05     |
| episodes                | 320000   |
| lives                   | 320000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2444262  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.04     |
| episodes                | 320100   |
| lives                   | 320100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2445198  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 320200   |
| lives                   | 320200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2446130  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.965    |
| episodes                | 320300   |
| lives                   | 320300   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2447031  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.01     |
| episodes                | 320400   |
| lives                   | 320400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2447972  |
| value_loss              | 0.912    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.04     |
| episodes                | 320500   |
| lives                   | 320500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2448912  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.01     |
| episodes                | 320600   |
| lives                   | 320600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2449841  |
| value_loss              | 1        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 320700   |
| lives                   | 320700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2450783  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.01     |
| episodes                | 320800   |
| lives                   | 320800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2451709  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.995    |
| episodes                | 320900   |
| lives                   | 320900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2452651  |
| value_loss              | 0.977    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.03     |
| episodes                | 321000   |
| lives                   | 321000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2453591  |
| value_loss              | 0.99     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.996    |
| episodes                | 321100   |
| lives                   | 321100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2454505  |
| value_loss              | 0.971    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.03     |
| episodes                | 321200   |
| lives                   | 321200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2455435  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.06     |
| episodes                | 321300   |
| lives                   | 321300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2456374  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.05     |
| episodes                | 321400   |
| lives                   | 321400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2457317  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.04     |
| episodes                | 321500   |
| lives                   | 321500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2458275  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.05     |
| episodes                | 321600   |
| lives                   | 321600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2459226  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 321700   |
| lives                   | 321700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2460164  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.05     |
| episodes                | 321800   |
| lives                   | 321800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2461112  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.01     |
| episodes                | 321900   |
| lives                   | 321900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2462040  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.948    |
| episodes                | 322000   |
| lives                   | 322000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2462955  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.04     |
| episodes                | 322100   |
| lives                   | 322100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2463919  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.06     |
| episodes                | 322200   |
| lives                   | 322200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2464868  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1        |
| episodes                | 322300   |
| lives                   | 322300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2465800  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0019  |
| entropy                 | 1.15     |
| episodes                | 322400   |
| lives                   | 322400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2466726  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.12     |
| episodes                | 322500   |
| lives                   | 322500   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 10.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0029  |
| steps                   | 2467633  |
| value_loss              | 1.97     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.06     |
| episodes                | 322600   |
| lives                   | 322600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0028  |
| steps                   | 2468545  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.09     |
| episodes                | 322700   |
| lives                   | 322700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2469471  |
| value_loss              | 1.39     |
--------------------------------------
Saving model due to running mean reward increase: 11.0748 -> 11.7875
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.11     |
| episodes                | 322800   |
| lives                   | 322800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2470412  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 1.06     |
| episodes                | 322900   |
| lives                   | 322900   |
| mean 100 episode length | 10       |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2471316  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.05     |
| episodes                | 323000   |
| lives                   | 323000   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.003   |
| steps                   | 2472209  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.11     |
| episodes                | 323100   |
| lives                   | 323100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2473163  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.12     |
| episodes                | 323200   |
| lives                   | 323200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2474117  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.09     |
| episodes                | 323300   |
| lives                   | 323300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2475068  |
| value_loss              | 1.87     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.03     |
| episodes                | 323400   |
| lives                   | 323400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2475976  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.09     |
| episodes                | 323500   |
| lives                   | 323500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2476926  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.1      |
| episodes                | 323600   |
| lives                   | 323600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2477893  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.07     |
| episodes                | 323700   |
| lives                   | 323700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2478844  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.1      |
| episodes                | 323800   |
| lives                   | 323800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2479791  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.04     |
| episodes                | 323900   |
| lives                   | 323900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2480724  |
| value_loss              | 1.75     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 324000   |
| lives                   | 324000   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2481635  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.05     |
| episodes                | 324100   |
| lives                   | 324100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0023  |
| steps                   | 2482566  |
| value_loss              | 1.68     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.01     |
| episodes                | 324200   |
| lives                   | 324200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2483485  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 324300   |
| lives                   | 324300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2484408  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.03     |
| episodes                | 324400   |
| lives                   | 324400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2485358  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 324500   |
| lives                   | 324500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2486297  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.991    |
| episodes                | 324600   |
| lives                   | 324600   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2487206  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 324700   |
| lives                   | 324700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2488142  |
| value_loss              | 0.882    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.03     |
| episodes                | 324800   |
| lives                   | 324800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0022  |
| steps                   | 2489070  |
| value_loss              | 1.32     |
--------------------------------------
Saving model due to mean reward increase: 11.9313 -> 11.9523
Saving model due to running mean reward increase: 11.3976 -> 11.9523
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.07     |
| episodes                | 324900   |
| lives                   | 324900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2490036  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.07     |
| episodes                | 325000   |
| lives                   | 325000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2490983  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.01     |
| episodes                | 325100   |
| lives                   | 325100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2491915  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.04     |
| episodes                | 325200   |
| lives                   | 325200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2492856  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.04     |
| episodes                | 325300   |
| lives                   | 325300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2493795  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.981    |
| episodes                | 325400   |
| lives                   | 325400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2494709  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.981    |
| episodes                | 325500   |
| lives                   | 325500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2495631  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.05     |
| episodes                | 325600   |
| lives                   | 325600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2496571  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.02     |
| episodes                | 325700   |
| lives                   | 325700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2497517  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 325800   |
| lives                   | 325800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.002   |
| steps                   | 2498444  |
| value_loss              | 1.81     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.987    |
| episodes                | 325900   |
| lives                   | 325900   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2499369  |
| value_loss              | 1        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.974    |
| episodes                | 326000   |
| lives                   | 326000   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2500289  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.01     |
| episodes                | 326100   |
| lives                   | 326100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2501223  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 326200   |
| lives                   | 326200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2502152  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0018  |
| entropy                 | 1.02     |
| episodes                | 326300   |
| lives                   | 326300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0025  |
| steps                   | 2503103  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 326400   |
| lives                   | 326400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2504046  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.945    |
| episodes                | 326500   |
| lives                   | 326500   |
| mean 100 episode length | 9.91     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2504937  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 326600   |
| lives                   | 326600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2505894  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 326700   |
| lives                   | 326700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2506848  |
| value_loss              | 0.946    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.956    |
| episodes                | 326800   |
| lives                   | 326800   |
| mean 100 episode length | 9.93     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2507741  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.02     |
| episodes                | 326900   |
| lives                   | 326900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2508704  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.05     |
| episodes                | 327000   |
| lives                   | 327000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2509649  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 327100   |
| lives                   | 327100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2510576  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.09     |
| episodes                | 327200   |
| lives                   | 327200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2511534  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.997    |
| episodes                | 327300   |
| lives                   | 327300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2512468  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.996    |
| episodes                | 327400   |
| lives                   | 327400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2513394  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.956    |
| episodes                | 327500   |
| lives                   | 327500   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2514293  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.992    |
| episodes                | 327600   |
| lives                   | 327600   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2515215  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.961    |
| episodes                | 327700   |
| lives                   | 327700   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2516129  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.05     |
| episodes                | 327800   |
| lives                   | 327800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2517077  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.994    |
| episodes                | 327900   |
| lives                   | 327900   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2517990  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.03     |
| episodes                | 328000   |
| lives                   | 328000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2518944  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.07     |
| episodes                | 328100   |
| lives                   | 328100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2519917  |
| value_loss              | 1.05     |
--------------------------------------
Saving model due to mean reward increase: 11.9523 -> 12.0926
Saving model due to running mean reward increase: 11.9765 -> 12.0926
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.05     |
| episodes                | 328200   |
| lives                   | 328200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2520888  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.968    |
| episodes                | 328300   |
| lives                   | 328300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2521809  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.06     |
| episodes                | 328400   |
| lives                   | 328400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2522775  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 328500   |
| lives                   | 328500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2523719  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.04     |
| episodes                | 328600   |
| lives                   | 328600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2524667  |
| value_loss              | 0.93     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.987    |
| episodes                | 328700   |
| lives                   | 328700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2525600  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.05     |
| episodes                | 328800   |
| lives                   | 328800   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2526556  |
| value_loss              | 0.974    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.04     |
| episodes                | 328900   |
| lives                   | 328900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2527494  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0016  |
| entropy                 | 1.01     |
| episodes                | 329000   |
| lives                   | 329000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2528432  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.983    |
| episodes                | 329100   |
| lives                   | 329100   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2529354  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 329200   |
| lives                   | 329200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2530273  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.97     |
| episodes                | 329300   |
| lives                   | 329300   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2531190  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.03     |
| episodes                | 329400   |
| lives                   | 329400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2532129  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.03     |
| episodes                | 329500   |
| lives                   | 329500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2533076  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.02     |
| episodes                | 329600   |
| lives                   | 329600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2534009  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 329700   |
| lives                   | 329700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2534937  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 329800   |
| lives                   | 329800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2535877  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 329900   |
| lives                   | 329900   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2536794  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.03     |
| episodes                | 330000   |
| lives                   | 330000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2537733  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.05     |
| episodes                | 330100   |
| lives                   | 330100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2538693  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.07     |
| episodes                | 330200   |
| lives                   | 330200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2539657  |
| value_loss              | 1.17     |
--------------------------------------
Saving model due to running mean reward increase: 11.79 -> 11.8947
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.06     |
| episodes                | 330300   |
| lives                   | 330300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2540618  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.04     |
| episodes                | 330400   |
| lives                   | 330400   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2541542  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.992    |
| episodes                | 330500   |
| lives                   | 330500   |
| mean 100 episode length | 9.99     |
| mean 100 episode reward | 11.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2542441  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.08     |
| episodes                | 330600   |
| lives                   | 330600   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2543435  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.002   |
| entropy                 | 1.02     |
| episodes                | 330700   |
| lives                   | 330700   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2544356  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.05     |
| episodes                | 330800   |
| lives                   | 330800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2545305  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.02     |
| episodes                | 330900   |
| lives                   | 330900   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2546236  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.03     |
| episodes                | 331000   |
| lives                   | 331000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2547177  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.03     |
| episodes                | 331100   |
| lives                   | 331100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2548110  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 331200   |
| lives                   | 331200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2549042  |
| value_loss              | 1.45     |
--------------------------------------
Saving model due to running mean reward increase: 11.4657 -> 11.9807
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.05     |
| episodes                | 331300   |
| lives                   | 331300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2550020  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.05     |
| episodes                | 331400   |
| lives                   | 331400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2550963  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 331500   |
| lives                   | 331500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2551914  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.03     |
| episodes                | 331600   |
| lives                   | 331600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0021  |
| steps                   | 2552860  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.07     |
| episodes                | 331700   |
| lives                   | 331700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2553837  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0017  |
| entropy                 | 1.01     |
| episodes                | 331800   |
| lives                   | 331800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2554770  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.03     |
| episodes                | 331900   |
| lives                   | 331900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2555727  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.04     |
| episodes                | 332000   |
| lives                   | 332000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2556695  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.02     |
| episodes                | 332100   |
| lives                   | 332100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2557662  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1        |
| episodes                | 332200   |
| lives                   | 332200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2558580  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.07     |
| episodes                | 332300   |
| lives                   | 332300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2559541  |
| value_loss              | 1.28     |
--------------------------------------
Saving model due to running mean reward increase: 11.6779 -> 11.7714
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 332400   |
| lives                   | 332400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2560511  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1        |
| episodes                | 332500   |
| lives                   | 332500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2561447  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.984    |
| episodes                | 332600   |
| lives                   | 332600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2562386  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 332700   |
| lives                   | 332700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2563347  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 332800   |
| lives                   | 332800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2564301  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1.03     |
| episodes                | 332900   |
| lives                   | 332900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2565264  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.04     |
| episodes                | 333000   |
| lives                   | 333000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2566214  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.994    |
| episodes                | 333100   |
| lives                   | 333100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2567144  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.984    |
| episodes                | 333200   |
| lives                   | 333200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2568084  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 333300   |
| lives                   | 333300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2569035  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.962    |
| episodes                | 333400   |
| lives                   | 333400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2569944  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.03     |
| episodes                | 333500   |
| lives                   | 333500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2570903  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.07     |
| episodes                | 333600   |
| lives                   | 333600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2571875  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.02     |
| episodes                | 333700   |
| lives                   | 333700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2572846  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 333800   |
| lives                   | 333800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2573791  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 333900   |
| lives                   | 333900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2574748  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 1.03     |
| episodes                | 334000   |
| lives                   | 334000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2575710  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.005   |
| entropy                 | 0.963    |
| episodes                | 334100   |
| lives                   | 334100   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 10.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2576622  |
| value_loss              | 2.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0014  |
| entropy                 | 1.01     |
| episodes                | 334200   |
| lives                   | 334200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0026  |
| steps                   | 2577550  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 1        |
| episodes                | 334300   |
| lives                   | 334300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2578490  |
| value_loss              | 1.76     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.01     |
| episodes                | 334400   |
| lives                   | 334400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2579433  |
| value_loss              | 1.23     |
--------------------------------------
Saving model due to running mean reward increase: 11.6158 -> 11.7133
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.11     |
| episodes                | 334500   |
| lives                   | 334500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2580417  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.07     |
| episodes                | 334600   |
| lives                   | 334600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2581382  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 334700   |
| lives                   | 334700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2582329  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 334800   |
| lives                   | 334800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0024  |
| steps                   | 2583262  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.09     |
| episodes                | 334900   |
| lives                   | 334900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2584246  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.09     |
| episodes                | 335000   |
| lives                   | 335000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2585226  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.08     |
| episodes                | 335100   |
| lives                   | 335100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2586184  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.05     |
| episodes                | 335200   |
| lives                   | 335200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2587137  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.05     |
| episodes                | 335300   |
| lives                   | 335300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2588088  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.04     |
| episodes                | 335400   |
| lives                   | 335400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2589050  |
| value_loss              | 1.65     |
--------------------------------------
Saving model due to running mean reward increase: 11.6845 -> 12.0334
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.07     |
| episodes                | 335500   |
| lives                   | 335500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2590019  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.06     |
| episodes                | 335600   |
| lives                   | 335600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2590987  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.04     |
| episodes                | 335700   |
| lives                   | 335700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2591953  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 335800   |
| lives                   | 335800   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2592915  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.06     |
| episodes                | 335900   |
| lives                   | 335900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2593869  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 336000   |
| lives                   | 336000   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2594857  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.05     |
| episodes                | 336100   |
| lives                   | 336100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2595809  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.01     |
| episodes                | 336200   |
| lives                   | 336200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2596754  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.04     |
| episodes                | 336300   |
| lives                   | 336300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2597711  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.08     |
| episodes                | 336400   |
| lives                   | 336400   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2598712  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.05     |
| episodes                | 336500   |
| lives                   | 336500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2599669  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.08     |
| episodes                | 336600   |
| lives                   | 336600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2600652  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.994    |
| episodes                | 336700   |
| lives                   | 336700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2601590  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 336800   |
| lives                   | 336800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2602536  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.05     |
| episodes                | 336900   |
| lives                   | 336900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2603517  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 337000   |
| lives                   | 337000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0018  |
| steps                   | 2604489  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.07     |
| episodes                | 337100   |
| lives                   | 337100   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2605475  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.982    |
| episodes                | 337200   |
| lives                   | 337200   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2606412  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.04     |
| episodes                | 337300   |
| lives                   | 337300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2607386  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.02     |
| episodes                | 337400   |
| lives                   | 337400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2608338  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.05     |
| episodes                | 337500   |
| lives                   | 337500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2609308  |
| value_loss              | 1.39     |
--------------------------------------
Saving model due to running mean reward increase: 11.8246 -> 11.8842
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.02     |
| episodes                | 337600   |
| lives                   | 337600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2610285  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1        |
| episodes                | 337700   |
| lives                   | 337700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2611233  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.02     |
| episodes                | 337800   |
| lives                   | 337800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2612179  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.01     |
| episodes                | 337900   |
| lives                   | 337900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2613162  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 338000   |
| lives                   | 338000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2614108  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.03     |
| episodes                | 338100   |
| lives                   | 338100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2615076  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.969    |
| episodes                | 338200   |
| lives                   | 338200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0016  |
| steps                   | 2616009  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.994    |
| episodes                | 338300   |
| lives                   | 338300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2616945  |
| value_loss              | 1.69     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.962    |
| episodes                | 338400   |
| lives                   | 338400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2617873  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.989    |
| episodes                | 338500   |
| lives                   | 338500   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2618808  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1        |
| episodes                | 338600   |
| lives                   | 338600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2619764  |
| value_loss              | 1.19     |
--------------------------------------
Saving model due to running mean reward increase: 11.7836 -> 11.969
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.08     |
| episodes                | 338700   |
| lives                   | 338700   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2620760  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1        |
| episodes                | 338800   |
| lives                   | 338800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2621697  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0012  |
| entropy                 | 0.973    |
| episodes                | 338900   |
| lives                   | 338900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0019  |
| steps                   | 2622633  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.967    |
| episodes                | 339000   |
| lives                   | 339000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2623579  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.997    |
| episodes                | 339100   |
| lives                   | 339100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2624543  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.06     |
| episodes                | 339200   |
| lives                   | 339200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2625509  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0013  |
| entropy                 | 0.967    |
| episodes                | 339300   |
| lives                   | 339300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2626461  |
| value_loss              | 1.64     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1        |
| episodes                | 339400   |
| lives                   | 339400   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2627438  |
| value_loss              | 1.46     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 1.05     |
| episodes                | 339500   |
| lives                   | 339500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2628413  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0015  |
| entropy                 | 0.998    |
| episodes                | 339600   |
| lives                   | 339600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2629358  |
| value_loss              | 1.17     |
--------------------------------------
Saving model due to running mean reward increase: 11.7753 -> 11.951
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.959    |
| episodes                | 339700   |
| lives                   | 339700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2630294  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.988    |
| episodes                | 339800   |
| lives                   | 339800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2631223  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.05     |
| episodes                | 339900   |
| lives                   | 339900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2632193  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.965    |
| episodes                | 340000   |
| lives                   | 340000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2633131  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 340100   |
| lives                   | 340100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2634089  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.03     |
| episodes                | 340200   |
| lives                   | 340200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2635057  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.986    |
| episodes                | 340300   |
| lives                   | 340300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2636008  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.934    |
| episodes                | 340400   |
| lives                   | 340400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2636918  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.981    |
| episodes                | 340500   |
| lives                   | 340500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2637867  |
| value_loss              | 0.901    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.01     |
| episodes                | 340600   |
| lives                   | 340600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2638833  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.995    |
| episodes                | 340700   |
| lives                   | 340700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2639790  |
| value_loss              | 1.5      |
--------------------------------------
Saving model due to running mean reward increase: 11.7185 -> 12.0304
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.06     |
| episodes                | 340800   |
| lives                   | 340800   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2640792  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.976    |
| episodes                | 340900   |
| lives                   | 340900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2641733  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.01     |
| episodes                | 341000   |
| lives                   | 341000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2642704  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1        |
| episodes                | 341100   |
| lives                   | 341100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2643663  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 341200   |
| lives                   | 341200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2644616  |
| value_loss              | 1        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.932    |
| episodes                | 341300   |
| lives                   | 341300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2645549  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.975    |
| episodes                | 341400   |
| lives                   | 341400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2646499  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.99     |
| episodes                | 341500   |
| lives                   | 341500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2647450  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1        |
| episodes                | 341600   |
| lives                   | 341600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2648419  |
| value_loss              | 1.56     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.97     |
| episodes                | 341700   |
| lives                   | 341700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2649361  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.958    |
| episodes                | 341800   |
| lives                   | 341800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2650310  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.973    |
| episodes                | 341900   |
| lives                   | 341900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2651265  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.98     |
| episodes                | 342000   |
| lives                   | 342000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2652233  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.983    |
| episodes                | 342100   |
| lives                   | 342100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2653185  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.965    |
| episodes                | 342200   |
| lives                   | 342200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2654140  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1        |
| episodes                | 342300   |
| lives                   | 342300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2655107  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.01     |
| episodes                | 342400   |
| lives                   | 342400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2656074  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.975    |
| episodes                | 342500   |
| lives                   | 342500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2657027  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.954    |
| episodes                | 342600   |
| lives                   | 342600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2657977  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 342700   |
| lives                   | 342700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2658949  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 342800   |
| lives                   | 342800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2659916  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1        |
| episodes                | 342900   |
| lives                   | 342900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2660887  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.06     |
| episodes                | 343000   |
| lives                   | 343000   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0014  |
| steps                   | 2661873  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.954    |
| episodes                | 343100   |
| lives                   | 343100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2662815  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.01     |
| episodes                | 343200   |
| lives                   | 343200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2663786  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.99     |
| episodes                | 343300   |
| lives                   | 343300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0017  |
| steps                   | 2664737  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 343400   |
| lives                   | 343400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2665703  |
| value_loss              | 0.888    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.976    |
| episodes                | 343500   |
| lives                   | 343500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2666657  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.03     |
| episodes                | 343600   |
| lives                   | 343600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2667640  |
| value_loss              | 1        |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.963    |
| episodes                | 343700   |
| lives                   | 343700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2668590  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.998    |
| episodes                | 343800   |
| lives                   | 343800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2669564  |
| value_loss              | 1.15     |
--------------------------------------
Saving model due to running mean reward increase: 11.7515 -> 11.7721
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.02     |
| episodes                | 343900   |
| lives                   | 343900   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2670560  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 344000   |
| lives                   | 344000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2671524  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 1.01     |
| episodes                | 344100   |
| lives                   | 344100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2672505  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.997    |
| episodes                | 344200   |
| lives                   | 344200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2673480  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.996    |
| episodes                | 344300   |
| lives                   | 344300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2674451  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 344400   |
| lives                   | 344400   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2675441  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.962    |
| episodes                | 344500   |
| lives                   | 344500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2676403  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.001   |
| entropy                 | 0.986    |
| episodes                | 344600   |
| lives                   | 344600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2677364  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.998    |
| episodes                | 344700   |
| lives                   | 344700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2678344  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1.01     |
| episodes                | 344800   |
| lives                   | 344800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2679295  |
| value_loss              | 1.36     |
--------------------------------------
Saving model due to running mean reward increase: 11.8047 -> 11.927
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.945    |
| episodes                | 344900   |
| lives                   | 344900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2680263  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 345000   |
| lives                   | 345000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2681242  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 345100   |
| lives                   | 345100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2682203  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.98     |
| episodes                | 345200   |
| lives                   | 345200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2683182  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.949    |
| episodes                | 345300   |
| lives                   | 345300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2684114  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.992    |
| episodes                | 345400   |
| lives                   | 345400   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2685089  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.01     |
| episodes                | 345500   |
| lives                   | 345500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2686065  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 345600   |
| lives                   | 345600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2687045  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 345700   |
| lives                   | 345700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2688022  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.96     |
| episodes                | 345800   |
| lives                   | 345800   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2688981  |
| value_loss              | 0.961    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.994    |
| episodes                | 345900   |
| lives                   | 345900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2689961  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.989    |
| episodes                | 346000   |
| lives                   | 346000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2690916  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.967    |
| episodes                | 346100   |
| lives                   | 346100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2691886  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.973    |
| episodes                | 346200   |
| lives                   | 346200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2692845  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.952    |
| episodes                | 346300   |
| lives                   | 346300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2693804  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.939    |
| episodes                | 346400   |
| lives                   | 346400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2694752  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0011  |
| entropy                 | 0.961    |
| episodes                | 346500   |
| lives                   | 346500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2695705  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.983    |
| episodes                | 346600   |
| lives                   | 346600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2696678  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.994    |
| episodes                | 346700   |
| lives                   | 346700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2697640  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.984    |
| episodes                | 346800   |
| lives                   | 346800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2698607  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.979    |
| episodes                | 346900   |
| lives                   | 346900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2699565  |
| value_loss              | 1.11     |
--------------------------------------
Saving model due to mean reward increase: 12.0926 -> 12.1942
Saving model due to running mean reward increase: 11.8225 -> 12.1942
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.03     |
| episodes                | 347000   |
| lives                   | 347000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2700549  |
| value_loss              | 0.972    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1        |
| episodes                | 347100   |
| lives                   | 347100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2701529  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.996    |
| episodes                | 347200   |
| lives                   | 347200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2702494  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 1.01     |
| episodes                | 347300   |
| lives                   | 347300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2703450  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.992    |
| episodes                | 347400   |
| lives                   | 347400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2704416  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.972    |
| episodes                | 347500   |
| lives                   | 347500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2705360  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.01     |
| episodes                | 347600   |
| lives                   | 347600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2706335  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.04     |
| episodes                | 347700   |
| lives                   | 347700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2707320  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1        |
| episodes                | 347800   |
| lives                   | 347800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2708296  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 1        |
| episodes                | 347900   |
| lives                   | 347900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2709272  |
| value_loss              | 1.53     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.983    |
| episodes                | 348000   |
| lives                   | 348000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2710225  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.994    |
| episodes                | 348100   |
| lives                   | 348100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2711190  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.987    |
| episodes                | 348200   |
| lives                   | 348200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2712147  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.999    |
| episodes                | 348300   |
| lives                   | 348300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2713117  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.994    |
| episodes                | 348400   |
| lives                   | 348400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2714085  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.01     |
| episodes                | 348500   |
| lives                   | 348500   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2715073  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.989    |
| episodes                | 348600   |
| lives                   | 348600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2716044  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.988    |
| episodes                | 348700   |
| lives                   | 348700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2717021  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.997    |
| episodes                | 348800   |
| lives                   | 348800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2717991  |
| value_loss              | 0.983    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.993    |
| episodes                | 348900   |
| lives                   | 348900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2718948  |
| value_loss              | 0.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1        |
| episodes                | 349000   |
| lives                   | 349000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2719925  |
| value_loss              | 1.09     |
--------------------------------------
Saving model due to running mean reward increase: 11.9932 -> 12.0238
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.969    |
| episodes                | 349100   |
| lives                   | 349100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0013  |
| steps                   | 2720866  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1        |
| episodes                | 349200   |
| lives                   | 349200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2721853  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.01     |
| episodes                | 349300   |
| lives                   | 349300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2722837  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.994    |
| episodes                | 349400   |
| lives                   | 349400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2723800  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.985    |
| episodes                | 349500   |
| lives                   | 349500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2724784  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.969    |
| episodes                | 349600   |
| lives                   | 349600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2725754  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1        |
| episodes                | 349700   |
| lives                   | 349700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2726734  |
| value_loss              | 0.986    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.969    |
| episodes                | 349800   |
| lives                   | 349800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2727684  |
| value_loss              | 0.989    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.997    |
| episodes                | 349900   |
| lives                   | 349900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2728659  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.993    |
| episodes                | 350000   |
| lives                   | 350000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2729639  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.961    |
| episodes                | 350100   |
| lives                   | 350100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2730604  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.01     |
| episodes                | 350200   |
| lives                   | 350200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2731587  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.986    |
| episodes                | 350300   |
| lives                   | 350300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2732562  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.01     |
| episodes                | 350400   |
| lives                   | 350400   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2733556  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.981    |
| episodes                | 350500   |
| lives                   | 350500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2734541  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.99     |
| episodes                | 350600   |
| lives                   | 350600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2735516  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.998    |
| episodes                | 350700   |
| lives                   | 350700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2736488  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.928    |
| episodes                | 350800   |
| lives                   | 350800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2737441  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.944    |
| episodes                | 350900   |
| lives                   | 350900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2738387  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.02     |
| episodes                | 351000   |
| lives                   | 351000   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2739377  |
| value_loss              | 0.991    |
--------------------------------------
Saving model due to running mean reward increase: 11.8261 -> 12.1825
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.975    |
| episodes                | 351100   |
| lives                   | 351100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2740360  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.988    |
| episodes                | 351200   |
| lives                   | 351200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2741330  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.947    |
| episodes                | 351300   |
| lives                   | 351300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2742273  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.922    |
| episodes                | 351400   |
| lives                   | 351400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2743209  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.92     |
| episodes                | 351500   |
| lives                   | 351500   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2744140  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.99     |
| episodes                | 351600   |
| lives                   | 351600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2745096  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.961    |
| episodes                | 351700   |
| lives                   | 351700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2746063  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.973    |
| episodes                | 351800   |
| lives                   | 351800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2747039  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.983    |
| episodes                | 351900   |
| lives                   | 351900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2747985  |
| value_loss              | 1.62     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.964    |
| episodes                | 352000   |
| lives                   | 352000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2748941  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.936    |
| episodes                | 352100   |
| lives                   | 352100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2749891  |
| value_loss              | 1.43     |
--------------------------------------
Saving model due to running mean reward increase: 11.6516 -> 11.8803
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.995    |
| episodes                | 352200   |
| lives                   | 352200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2750878  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.933    |
| episodes                | 352300   |
| lives                   | 352300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2751816  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.98     |
| episodes                | 352400   |
| lives                   | 352400   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2752796  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.932    |
| episodes                | 352500   |
| lives                   | 352500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2753734  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.977    |
| episodes                | 352600   |
| lives                   | 352600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2754697  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.954    |
| episodes                | 352700   |
| lives                   | 352700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2755651  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 352800   |
| lives                   | 352800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2756630  |
| value_loss              | 1.6      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.03     |
| episodes                | 352900   |
| lives                   | 352900   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2757623  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.956    |
| episodes                | 353000   |
| lives                   | 353000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2758571  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.978    |
| episodes                | 353100   |
| lives                   | 353100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2759536  |
| value_loss              | 1.26     |
--------------------------------------
Saving model due to running mean reward increase: 11.8592 -> 11.8667
--------------------------------------
| approx_kl               | -0.0009  |
| entropy                 | 0.974    |
| episodes                | 353200   |
| lives                   | 353200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2760505  |
| value_loss              | 0.899    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.917    |
| episodes                | 353300   |
| lives                   | 353300   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2761435  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 353400   |
| lives                   | 353400   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2762422  |
| value_loss              | 1.59     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.963    |
| episodes                | 353500   |
| lives                   | 353500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2763385  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.966    |
| episodes                | 353600   |
| lives                   | 353600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2764338  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.948    |
| episodes                | 353700   |
| lives                   | 353700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2765304  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.98     |
| episodes                | 353800   |
| lives                   | 353800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2766275  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.947    |
| episodes                | 353900   |
| lives                   | 353900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2767229  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.97     |
| episodes                | 354000   |
| lives                   | 354000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2768187  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.985    |
| episodes                | 354100   |
| lives                   | 354100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2769161  |
| value_loss              | 1.2      |
--------------------------------------
Saving model due to running mean reward increase: 11.9688 -> 11.9889
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.976    |
| episodes                | 354200   |
| lives                   | 354200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2770133  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.974    |
| episodes                | 354300   |
| lives                   | 354300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2771074  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.966    |
| episodes                | 354400   |
| lives                   | 354400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2772035  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.998    |
| episodes                | 354500   |
| lives                   | 354500   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2773022  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1        |
| episodes                | 354600   |
| lives                   | 354600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2774004  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0008  |
| entropy                 | 0.957    |
| episodes                | 354700   |
| lives                   | 354700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2774961  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.952    |
| episodes                | 354800   |
| lives                   | 354800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2775909  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.966    |
| episodes                | 354900   |
| lives                   | 354900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2776871  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.988    |
| episodes                | 355000   |
| lives                   | 355000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2777846  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.973    |
| episodes                | 355100   |
| lives                   | 355100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2778796  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 355200   |
| lives                   | 355200   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2779796  |
| value_loss              | 0.79     |
--------------------------------------
Saving model due to mean reward increase: 12.1942 -> 12.3118
Saving model due to running mean reward increase: 11.9858 -> 12.3118
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.02     |
| episodes                | 355300   |
| lives                   | 355300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2780774  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.961    |
| episodes                | 355400   |
| lives                   | 355400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2781728  |
| value_loss              | 0.949    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.995    |
| episodes                | 355500   |
| lives                   | 355500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2782695  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.973    |
| episodes                | 355600   |
| lives                   | 355600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2783663  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.978    |
| episodes                | 355700   |
| lives                   | 355700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2784625  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.99     |
| episodes                | 355800   |
| lives                   | 355800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0012  |
| steps                   | 2785603  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.03     |
| episodes                | 355900   |
| lives                   | 355900   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2786606  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.957    |
| episodes                | 356000   |
| lives                   | 356000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2787564  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.02     |
| episodes                | 356100   |
| lives                   | 356100   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2788559  |
| value_loss              | 1.66     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.966    |
| episodes                | 356200   |
| lives                   | 356200   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2789507  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.966    |
| episodes                | 356300   |
| lives                   | 356300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2790470  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 0.904    |
| episodes                | 356400   |
| lives                   | 356400   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2791404  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.02     |
| episodes                | 356500   |
| lives                   | 356500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2792377  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.944    |
| episodes                | 356600   |
| lives                   | 356600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2793324  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1        |
| episodes                | 356700   |
| lives                   | 356700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2794297  |
| value_loss              | 0.974    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.98     |
| episodes                | 356800   |
| lives                   | 356800   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2795243  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.991    |
| episodes                | 356900   |
| lives                   | 356900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2796222  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 357000   |
| lives                   | 357000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2797203  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.04     |
| episodes                | 357100   |
| lives                   | 357100   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2798201  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.982    |
| episodes                | 357200   |
| lives                   | 357200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2799174  |
| value_loss              | 1.32     |
--------------------------------------
Saving model due to running mean reward increase: 12.0508 -> 12.1465
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1        |
| episodes                | 357300   |
| lives                   | 357300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2800154  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 357400   |
| lives                   | 357400   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2801145  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.991    |
| episodes                | 357500   |
| lives                   | 357500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2802104  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.935    |
| episodes                | 357600   |
| lives                   | 357600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2803051  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.01     |
| episodes                | 357700   |
| lives                   | 357700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2804024  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.99     |
| episodes                | 357800   |
| lives                   | 357800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2805006  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.02     |
| episodes                | 357900   |
| lives                   | 357900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2805987  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0086  |
| entropy                 | 1.02     |
| episodes                | 358000   |
| lives                   | 358000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | 0.0035   |
| steps                   | 2806959  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.15     |
| episodes                | 358100   |
| lives                   | 358100   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 10.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0015  |
| steps                   | 2807893  |
| value_loss              | 1.91     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.08     |
| episodes                | 358200   |
| lives                   | 358200   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2808815  |
| value_loss              | 1.85     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.1      |
| episodes                | 358300   |
| lives                   | 358300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2809757  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.03     |
| episodes                | 358400   |
| lives                   | 358400   |
| mean 100 episode length | 10.1     |
| mean 100 episode reward | 11       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2810671  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.02     |
| episodes                | 358500   |
| lives                   | 358500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2811593  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.05     |
| episodes                | 358600   |
| lives                   | 358600   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2812530  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.03     |
| episodes                | 358700   |
| lives                   | 358700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2813475  |
| value_loss              | 1.73     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.01     |
| episodes                | 358800   |
| lives                   | 358800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.001   |
| steps                   | 2814408  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.04     |
| episodes                | 358900   |
| lives                   | 358900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2815384  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.06     |
| episodes                | 359000   |
| lives                   | 359000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2816343  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 1.04     |
| episodes                | 359100   |
| lives                   | 359100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0011  |
| steps                   | 2817296  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0006  |
| entropy                 | 1.03     |
| episodes                | 359200   |
| lives                   | 359200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2818257  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1.02     |
| episodes                | 359300   |
| lives                   | 359300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2819201  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0007  |
| entropy                 | 0.999    |
| episodes                | 359400   |
| lives                   | 359400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2820157  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.946    |
| episodes                | 359500   |
| lives                   | 359500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2821077  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.996    |
| episodes                | 359600   |
| lives                   | 359600   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0009  |
| steps                   | 2822011  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.985    |
| episodes                | 359700   |
| lives                   | 359700   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2822942  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.978    |
| episodes                | 359800   |
| lives                   | 359800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2823884  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.06     |
| episodes                | 359900   |
| lives                   | 359900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2824853  |
| value_loss              | 1.2      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.07     |
| episodes                | 360000   |
| lives                   | 360000   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2825845  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1        |
| episodes                | 360100   |
| lives                   | 360100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2826814  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.01     |
| episodes                | 360200   |
| lives                   | 360200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2827774  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.01     |
| episodes                | 360300   |
| lives                   | 360300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2828732  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.976    |
| episodes                | 360400   |
| lives                   | 360400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2829680  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.991    |
| episodes                | 360500   |
| lives                   | 360500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2830626  |
| value_loss              | 1.58     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.967    |
| episodes                | 360600   |
| lives                   | 360600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2831576  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.02     |
| episodes                | 360700   |
| lives                   | 360700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2832551  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 1        |
| episodes                | 360800   |
| lives                   | 360800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2833531  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.995    |
| episodes                | 360900   |
| lives                   | 360900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2834488  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.978    |
| episodes                | 361000   |
| lives                   | 361000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2835440  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.981    |
| episodes                | 361100   |
| lives                   | 361100   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2836396  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.978    |
| episodes                | 361200   |
| lives                   | 361200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0008  |
| steps                   | 2837329  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.968    |
| episodes                | 361300   |
| lives                   | 361300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2838273  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.01     |
| episodes                | 361400   |
| lives                   | 361400   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2839262  |
| value_loss              | 1.24     |
--------------------------------------
Saving model due to running mean reward increase: 12.0641 -> 12.2629
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.05     |
| episodes                | 361500   |
| lives                   | 361500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2840246  |
| value_loss              | 0.951    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.02     |
| episodes                | 361600   |
| lives                   | 361600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2841221  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.977    |
| episodes                | 361700   |
| lives                   | 361700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2842175  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.952    |
| episodes                | 361800   |
| lives                   | 361800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2843109  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 361900   |
| lives                   | 361900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2844082  |
| value_loss              | 1.15     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 362000   |
| lives                   | 362000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2845058  |
| value_loss              | 1.61     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.987    |
| episodes                | 362100   |
| lives                   | 362100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2846039  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.03     |
| episodes                | 362200   |
| lives                   | 362200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2847034  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.99     |
| episodes                | 362300   |
| lives                   | 362300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2848016  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1.03     |
| episodes                | 362400   |
| lives                   | 362400   |
| mean 100 episode length | 11.1     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2849024  |
| value_loss              | 1.03     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.912    |
| episodes                | 362500   |
| lives                   | 362500   |
| mean 100 episode length | 10.2     |
| mean 100 episode reward | 11.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0007  |
| steps                   | 2849943  |
| value_loss              | 1.84     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.925    |
| episodes                | 362600   |
| lives                   | 362600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2850892  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.97     |
| episodes                | 362700   |
| lives                   | 362700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2851859  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.01     |
| episodes                | 362800   |
| lives                   | 362800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2852838  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.97     |
| episodes                | 362900   |
| lives                   | 362900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2853801  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.991    |
| episodes                | 363000   |
| lives                   | 363000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2854767  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.991    |
| episodes                | 363100   |
| lives                   | 363100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2855745  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.01     |
| episodes                | 363200   |
| lives                   | 363200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2856726  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 1        |
| episodes                | 363300   |
| lives                   | 363300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2857696  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.978    |
| episodes                | 363400   |
| lives                   | 363400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2858667  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.992    |
| episodes                | 363500   |
| lives                   | 363500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2859635  |
| value_loss              | 1.41     |
--------------------------------------
Saving model due to running mean reward increase: 11.9904 -> 12.0951
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 363600   |
| lives                   | 363600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2860610  |
| value_loss              | 1.51     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.989    |
| episodes                | 363700   |
| lives                   | 363700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2861588  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.961    |
| episodes                | 363800   |
| lives                   | 363800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2862562  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.991    |
| episodes                | 363900   |
| lives                   | 363900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2863541  |
| value_loss              | 0.961    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.942    |
| episodes                | 364000   |
| lives                   | 364000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2864488  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.965    |
| episodes                | 364100   |
| lives                   | 364100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2865458  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0005  |
| entropy                 | 0.97     |
| episodes                | 364200   |
| lives                   | 364200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2866420  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.975    |
| episodes                | 364300   |
| lives                   | 364300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2867376  |
| value_loss              | 0.998    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.966    |
| episodes                | 364400   |
| lives                   | 364400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2868342  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.968    |
| episodes                | 364500   |
| lives                   | 364500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2869301  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.945    |
| episodes                | 364600   |
| lives                   | 364600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2870249  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.998    |
| episodes                | 364700   |
| lives                   | 364700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2871224  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.939    |
| episodes                | 364800   |
| lives                   | 364800   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2872158  |
| value_loss              | 1.57     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.927    |
| episodes                | 364900   |
| lives                   | 364900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2873105  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.945    |
| episodes                | 365000   |
| lives                   | 365000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2874051  |
| value_loss              | 1.29     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1        |
| episodes                | 365100   |
| lives                   | 365100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2875031  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.01     |
| episodes                | 365200   |
| lives                   | 365200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2876005  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.988    |
| episodes                | 365300   |
| lives                   | 365300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2876975  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.929    |
| episodes                | 365400   |
| lives                   | 365400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2877933  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.964    |
| episodes                | 365500   |
| lives                   | 365500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2878896  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.986    |
| episodes                | 365600   |
| lives                   | 365600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2879876  |
| value_loss              | 1.32     |
--------------------------------------
Saving model due to running mean reward increase: 11.9177 -> 12.1811
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.967    |
| episodes                | 365700   |
| lives                   | 365700   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2880834  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.978    |
| episodes                | 365800   |
| lives                   | 365800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2881810  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.975    |
| episodes                | 365900   |
| lives                   | 365900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2882795  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.961    |
| episodes                | 366000   |
| lives                   | 366000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0006  |
| steps                   | 2883745  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0004  |
| entropy                 | 0.935    |
| episodes                | 366100   |
| lives                   | 366100   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2884692  |
| value_loss              | 0.966    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.964    |
| episodes                | 366200   |
| lives                   | 366200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2885671  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.997    |
| episodes                | 366300   |
| lives                   | 366300   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2886663  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.01     |
| episodes                | 366400   |
| lives                   | 366400   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2887663  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1        |
| episodes                | 366500   |
| lives                   | 366500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2888643  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.988    |
| episodes                | 366600   |
| lives                   | 366600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2889603  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.899    |
| episodes                | 366700   |
| lives                   | 366700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2890540  |
| value_loss              | 1.47     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.987    |
| episodes                | 366800   |
| lives                   | 366800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2891510  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.998    |
| episodes                | 366900   |
| lives                   | 366900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2892492  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.02     |
| episodes                | 367000   |
| lives                   | 367000   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2893479  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 1.02     |
| episodes                | 367100   |
| lives                   | 367100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2894461  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.999    |
| episodes                | 367200   |
| lives                   | 367200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2895453  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.02     |
| episodes                | 367300   |
| lives                   | 367300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2896437  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.93     |
| episodes                | 367400   |
| lives                   | 367400   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2897377  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.01     |
| episodes                | 367500   |
| lives                   | 367500   |
| mean 100 episode length | 11.1     |
| mean 100 episode reward | 12.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2898384  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.974    |
| episodes                | 367600   |
| lives                   | 367600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2899363  |
| value_loss              | 1.44     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.03     |
| episodes                | 367700   |
| lives                   | 367700   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2900354  |
| value_loss              | 1.7      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.979    |
| episodes                | 367800   |
| lives                   | 367800   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2901342  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.972    |
| episodes                | 367900   |
| lives                   | 367900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2902312  |
| value_loss              | 1.54     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.982    |
| episodes                | 368000   |
| lives                   | 368000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0005  |
| steps                   | 2903284  |
| value_loss              | 1.43     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.989    |
| episodes                | 368100   |
| lives                   | 368100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2904256  |
| value_loss              | 1.21     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.02     |
| episodes                | 368200   |
| lives                   | 368200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2905246  |
| value_loss              | 1.48     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.01     |
| episodes                | 368300   |
| lives                   | 368300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2906226  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1        |
| episodes                | 368400   |
| lives                   | 368400   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2907204  |
| value_loss              | 0.8      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.991    |
| episodes                | 368500   |
| lives                   | 368500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2908185  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.974    |
| episodes                | 368600   |
| lives                   | 368600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2909141  |
| value_loss              | 1.56     |
--------------------------------------
Saving model due to running mean reward increase: 11.8288 -> 11.9913
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.979    |
| episodes                | 368700   |
| lives                   | 368700   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2910108  |
| value_loss              | 1.39     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.928    |
| episodes                | 368800   |
| lives                   | 368800   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0004  |
| steps                   | 2911053  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.968    |
| episodes                | 368900   |
| lives                   | 368900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2912022  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.946    |
| episodes                | 369000   |
| lives                   | 369000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2912982  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.939    |
| episodes                | 369100   |
| lives                   | 369100   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2913919  |
| value_loss              | 1.49     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.01     |
| episodes                | 369200   |
| lives                   | 369200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2914913  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.985    |
| episodes                | 369300   |
| lives                   | 369300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2915894  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0003  |
| entropy                 | 0.997    |
| episodes                | 369400   |
| lives                   | 369400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2916868  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 1.02     |
| episodes                | 369500   |
| lives                   | 369500   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.4     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2917872  |
| value_loss              | 1.36     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.989    |
| episodes                | 369600   |
| lives                   | 369600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2918853  |
| value_loss              | 1.07     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.957    |
| episodes                | 369700   |
| lives                   | 369700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2919801  |
| value_loss              | 0.968    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.968    |
| episodes                | 369800   |
| lives                   | 369800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2920781  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.963    |
| episodes                | 369900   |
| lives                   | 369900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2921734  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1        |
| episodes                | 370000   |
| lives                   | 370000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2922717  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.992    |
| episodes                | 370100   |
| lives                   | 370100   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2923708  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.977    |
| episodes                | 370200   |
| lives                   | 370200   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2924695  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.976    |
| episodes                | 370300   |
| lives                   | 370300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2925652  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.944    |
| episodes                | 370400   |
| lives                   | 370400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2926603  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.948    |
| episodes                | 370500   |
| lives                   | 370500   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2927571  |
| value_loss              | 1.16     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.998    |
| episodes                | 370600   |
| lives                   | 370600   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2928562  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.996    |
| episodes                | 370700   |
| lives                   | 370700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2929539  |
| value_loss              | 1.05     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.988    |
| episodes                | 370800   |
| lives                   | 370800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2930524  |
| value_loss              | 1.23     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.957    |
| episodes                | 370900   |
| lives                   | 370900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2931488  |
| value_loss              | 1.26     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.979    |
| episodes                | 371000   |
| lives                   | 371000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2932450  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.95     |
| episodes                | 371100   |
| lives                   | 371100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2933415  |
| value_loss              | 0.96     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.913    |
| episodes                | 371200   |
| lives                   | 371200   |
| mean 100 episode length | 10.3     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2934347  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.954    |
| episodes                | 371300   |
| lives                   | 371300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0003  |
| steps                   | 2935311  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.01     |
| episodes                | 371400   |
| lives                   | 371400   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2936295  |
| value_loss              | 0.937    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.935    |
| episodes                | 371500   |
| lives                   | 371500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2937232  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1        |
| episodes                | 371600   |
| lives                   | 371600   |
| mean 100 episode length | 11       |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2938234  |
| value_loss              | 1.42     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.949    |
| episodes                | 371700   |
| lives                   | 371700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2939181  |
| value_loss              | 1.31     |
--------------------------------------
Saving model due to running mean reward increase: 11.8535 -> 12.0742
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.01     |
| episodes                | 371800   |
| lives                   | 371800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2940161  |
| value_loss              | 1.38     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.935    |
| episodes                | 371900   |
| lives                   | 371900   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2941108  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.99     |
| episodes                | 372000   |
| lives                   | 372000   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2942090  |
| value_loss              | 1.45     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.969    |
| episodes                | 372100   |
| lives                   | 372100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2943059  |
| value_loss              | 1.27     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.987    |
| episodes                | 372200   |
| lives                   | 372200   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2944031  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.907    |
| episodes                | 372300   |
| lives                   | 372300   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2944969  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.947    |
| episodes                | 372400   |
| lives                   | 372400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2945915  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.959    |
| episodes                | 372500   |
| lives                   | 372500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2946895  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.961    |
| episodes                | 372600   |
| lives                   | 372600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2947848  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.996    |
| episodes                | 372700   |
| lives                   | 372700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2948832  |
| value_loss              | 1.4      |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.01     |
| episodes                | 372800   |
| lives                   | 372800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2949817  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.979    |
| episodes                | 372900   |
| lives                   | 372900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2950797  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.954    |
| episodes                | 373000   |
| lives                   | 373000   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2951757  |
| value_loss              | 1.63     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0002  |
| entropy                 | 0.999    |
| episodes                | 373100   |
| lives                   | 373100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2952738  |
| value_loss              | 1.12     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.943    |
| episodes                | 373200   |
| lives                   | 373200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2953693  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.982    |
| episodes                | 373300   |
| lives                   | 373300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2954660  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.977    |
| episodes                | 373400   |
| lives                   | 373400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2955633  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.946    |
| episodes                | 373500   |
| lives                   | 373500   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2956582  |
| value_loss              | 1.14     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.944    |
| episodes                | 373600   |
| lives                   | 373600   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.6     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2957531  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.98     |
| episodes                | 373700   |
| lives                   | 373700   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2958514  |
| value_loss              | 1.41     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.02     |
| episodes                | 373800   |
| lives                   | 373800   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2959489  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.937    |
| episodes                | 373900   |
| lives                   | 373900   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2960455  |
| value_loss              | 1.18     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.997    |
| episodes                | 374000   |
| lives                   | 374000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2961428  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.991    |
| episodes                | 374100   |
| lives                   | 374100   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2962414  |
| value_loss              | 0.971    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.993    |
| episodes                | 374200   |
| lives                   | 374200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2963393  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.952    |
| episodes                | 374300   |
| lives                   | 374300   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2964354  |
| value_loss              | 1.55     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.992    |
| episodes                | 374400   |
| lives                   | 374400   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2965342  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.983    |
| episodes                | 374500   |
| lives                   | 374500   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2966317  |
| value_loss              | 0.872    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.972    |
| episodes                | 374600   |
| lives                   | 374600   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2967290  |
| value_loss              | 1.33     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.934    |
| episodes                | 374700   |
| lives                   | 374700   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2968243  |
| value_loss              | 0.944    |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 0.968    |
| episodes                | 374800   |
| lives                   | 374800   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2969203  |
| value_loss              | 1.32     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.985    |
| episodes                | 374900   |
| lives                   | 374900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2970187  |
| value_loss              | 1.17     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.988    |
| episodes                | 375000   |
| lives                   | 375000   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2971152  |
| value_loss              | 1.3      |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 0.986    |
| episodes                | 375100   |
| lives                   | 375100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2972135  |
| value_loss              | 1.71     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.998    |
| episodes                | 375200   |
| lives                   | 375200   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2973110  |
| value_loss              | 1.22     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.989    |
| episodes                | 375300   |
| lives                   | 375300   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2974091  |
| value_loss              | 1.13     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.946    |
| episodes                | 375400   |
| lives                   | 375400   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2975047  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.938    |
| episodes                | 375500   |
| lives                   | 375500   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2975985  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.949    |
| episodes                | 375600   |
| lives                   | 375600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2976943  |
| value_loss              | 0.95     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.966    |
| episodes                | 375700   |
| lives                   | 375700   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0002  |
| steps                   | 2977886  |
| value_loss              | 1.06     |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 0.988    |
| episodes                | 375800   |
| lives                   | 375800   |
| mean 100 episode length | 11.1     |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2978892  |
| value_loss              | 1.04     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.96     |
| episodes                | 375900   |
| lives                   | 375900   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2979836  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.927    |
| episodes                | 376000   |
| lives                   | 376000   |
| mean 100 episode length | 10.4     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2980777  |
| value_loss              | 0.968    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1        |
| episodes                | 376100   |
| lives                   | 376100   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2981754  |
| value_loss              | 1.25     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.966    |
| episodes                | 376200   |
| lives                   | 376200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2982709  |
| value_loss              | 1.08     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.934    |
| episodes                | 376300   |
| lives                   | 376300   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2983660  |
| value_loss              | 0.959    |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.96     |
| episodes                | 376400   |
| lives                   | 376400   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2984627  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.956    |
| episodes                | 376500   |
| lives                   | 376500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2985585  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.986    |
| episodes                | 376600   |
| lives                   | 376600   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2986548  |
| value_loss              | 1.34     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 1.04     |
| episodes                | 376700   |
| lives                   | 376700   |
| mean 100 episode length | 11.2     |
| mean 100 episode reward | 12.5     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2987570  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.997    |
| episodes                | 376800   |
| lives                   | 376800   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.3     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2988564  |
| value_loss              | 1.19     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.956    |
| episodes                | 376900   |
| lives                   | 376900   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2989521  |
| value_loss              | 1.35     |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 0.945    |
| episodes                | 377000   |
| lives                   | 377000   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.7     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2990472  |
| value_loss              | 1.1      |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 0.972    |
| episodes                | 377100   |
| lives                   | 377100   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2991445  |
| value_loss              | 1.31     |
--------------------------------------
--------------------------------------
| approx_kl               | -0       |
| entropy                 | 0.952    |
| episodes                | 377200   |
| lives                   | 377200   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2992406  |
| value_loss              | 1.09     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.988    |
| episodes                | 377300   |
| lives                   | 377300   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2993377  |
| value_loss              | 1.11     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.933    |
| episodes                | 377400   |
| lives                   | 377400   |
| mean 100 episode length | 10.5     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2994329  |
| value_loss              | 1.28     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.962    |
| episodes                | 377500   |
| lives                   | 377500   |
| mean 100 episode length | 10.6     |
| mean 100 episode reward | 11.8     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2995291  |
| value_loss              | 1.37     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.992    |
| episodes                | 377600   |
| lives                   | 377600   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12.1     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2996276  |
| value_loss              | 1.02     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.995    |
| episodes                | 377700   |
| lives                   | 377700   |
| mean 100 episode length | 10.9     |
| mean 100 episode reward | 12.2     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2997267  |
| value_loss              | 1.01     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.991    |
| episodes                | 377800   |
| lives                   | 377800   |
| mean 100 episode length | 10.7     |
| mean 100 episode reward | 11.9     |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2998234  |
| value_loss              | 1.24     |
--------------------------------------
--------------------------------------
| approx_kl               | -0.0001  |
| entropy                 | 0.976    |
| episodes                | 377900   |
| lives                   | 377900   |
| mean 100 episode length | 10.8     |
| mean 100 episode reward | 12       |
| most_used_dataset       | 4        |
| number_of_used          | 21       |
| policy_loss             | -0.0001  |
| steps                   | 2999210  |
| value_loss              | 1.42     |
--------------------------------------
Restored model with mean reward: 12.3118
